{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libaries\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open abstracts and read file\n",
    "\n",
    "with open('../input/arvix_abstracts.txt') as sh:\n",
    "    abst = sh.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7540200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating set of all charcaters\n",
    "\n",
    "character_set = sorted(set(abst))\n",
    "\n",
    "#dictionary mapping characters to integers\n",
    "dict_int = {term :num for num, term in enumerate(character_set)}\n",
    "\n",
    "#create array of entire abstracts\n",
    "\n",
    "encoded = np.array([dict_int[word] for word in abst], dtype = np.int32)\n",
    "\n",
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In science and engineering, intelligent processing of complex signals such as images, sound or language is often performed by a parameterized hierarchy of nonlinear processing layers, sometimes biologically inspired. Hierarchical systems (or, more generally, nested systems) offer a way to generate complex mappings using simple stages. Each layer performs a different operation and achieves an ever \n",
      "[35 67  1 72 56 62 58 67 56 58  1 54 67 57  1 58 67 60 62 67 58 58 71 62\n",
      " 67 60  9  1 62 67 73 58 65 65 62 60 58 67 73  1 69 71 68 56 58 72 72 62\n",
      " 67 60  1 68 59  1 56 68 66 69 65 58 77  1 72 62 60 67 54 65 72  1 72 74\n",
      " 56 61  1 54 72  1 62 66 54 60 58 72  9  1 72 68 74 67 57  1 68 71  1 65\n",
      " 54 67 60 74 54 60 58  1 62 72  1 68 59 73 58 67  1 69 58 71 59 68 71 66\n",
      " 58 57  1 55 78  1 54  1 69 54 71 54 66 58 73 58 71 62 79 58 57  1 61 62\n",
      " 58 71 54 71 56 61 78  1 68 59  1 67 68 67 65 62 67 58 54 71  1 69 71 68\n",
      " 56 58 72 72 62 67 60  1 65 54 78 58 71 72  9  1 72 68 66 58 73 62 66 58\n",
      " 72  1 55 62 68 65 68 60 62 56 54 65 65 78  1 62 67 72 69 62 71 58 57 11\n",
      "  1 34 62 58 71 54 71 56 61 62 56 54 65  1 72 78 72 73 58 66 72  1  6 68\n",
      " 71  9  1 66 68 71 58  1 60 58 67 58 71 54 65 65 78  9  1 67 58 72 73 58\n",
      " 57  1 72 78 72 73 58 66 72  7  1 68 59 59 58 71  1 54  1 76 54 78  1 73\n",
      " 68  1 60 58 67 58 71 54 73 58  1 56 68 66 69 65 58 77  1 66 54 69 69 62\n",
      " 67 60 72  1 74 72 62 67 60  1 72 62 66 69 65 58  1 72 73 54 60 58 72 11\n",
      "  1 31 54 56 61  1 65 54 78 58 71  1 69 58 71 59 68 71 66 72  1 54  1 57\n",
      " 62 59 59 58 71 58 67 73  1 68 69 58 71 54 73 62 68 67  1 54 67 57  1 54\n",
      " 56 61 62 58 75 58 72  1 54 67  1 58 75 58 71  1]\n"
     ]
    }
   ],
   "source": [
    "#print raw text and array encoding\n",
    "len(character_set)\n",
    "\n",
    "print(abst[:400])\n",
    "\n",
    "print(encoded[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creatiung batches of x & y\n",
    "graph = tf.Graph()\n",
    "\n",
    "def create_batches(encoded, batch_size, character_length):\n",
    "    \n",
    "        character_per_batch = batch_size * character_length\n",
    "    \n",
    "        num_batches = len(encoded)//character_per_batch\n",
    "    \n",
    "    \n",
    "        encoded = encoded[: (num_batches * character_per_batch)]\n",
    "    \n",
    "        encoded = encoded.reshape(batch_size, -1)\n",
    "    \n",
    "    \n",
    "        #Split encoded into x & y set\n",
    "    \n",
    "        for step in range(0, encoded.shape[1], character_length):\n",
    "        \n",
    "            x = encoded[:, step : step + character_length]\n",
    "        \n",
    "            y = np.zeros(x.shape, dtype = x.dtype)\n",
    "        \n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        \n",
    "            yield x, y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = create_batches(encoded, batch_size = 5, character_length = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35 67  1 72 56 62 58 67]\n",
      " [ 1 73 68 68 65 72  1 74]\n",
      " [58 54 71 67 62 67 60  1]\n",
      " [69 73 62 66 62 79 54 73]\n",
      " [67 78  1 69 71 54 56 73]]\n",
      "[[67  1 72 56 62 58 67 35]\n",
      " [73 68 68 65 72  1 74  1]\n",
      " [54 71 67 62 67 60  1 58]\n",
      " [73 62 66 62 79 54 73 69]\n",
      " [78  1 69 71 54 56 73 67]]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(batches)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input placeholders\n",
    "\n",
    "def tensor_inputs(batch_size, character_length):\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, character_length])\n",
    "    \n",
    "    output = tf.placeholder(tf.int32, [batch_size, character_length])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    return inputs, output, keep_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building LSTM cells\n",
    "\n",
    "def LSTM_cell(lstmsize, batch_size, keep_prob, num_layers):\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "    \n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstmsize)\n",
    "    \n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = keep_prob)\n",
    "        \n",
    "        return drop\n",
    "\n",
    "    \n",
    "    multi_lstm = tf.contrib.rnn.MultiRNNCell([build_cell(lstmsize, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    initial_state = multi_lstm.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return multi_lstm, initial_state\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to calculate logits using inputs and lstm outputs\n",
    "\n",
    "\n",
    "def logits(lstmoutput, lstmsize, outclasses_size):\n",
    "    \n",
    "    lstm_batch_list = tf.concat(lstmoutput, axis = -1)\n",
    "    \n",
    "    lstm_output = tf.reshape(lstm_batch_list, [-1, lstmsize])\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        \n",
    "        softmax_w = tf.get_variable('softmax_w', [lstmsize, outclasses_size], \n",
    "                                    initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        \n",
    "        softmax_b = tf.get_variable('softmax_b', [outclasses_size], initializer = tf.zeros_initializer())\n",
    "            \n",
    "        logits = tf.add(tf.matmul(lstm_output, softmax_w), softmax_b)\n",
    "        \n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        return logits, predictions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax cross entropy loss \n",
    "\n",
    "def loss(logits, targets, classes):\n",
    "\n",
    "        \n",
    "    y_one_hot = tf.one_hot(targets, classes)\n",
    "        \n",
    "    y = tf.reshape(y_one_hot, logits.get_shape())\n",
    "        \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining optimizers for training\n",
    "\n",
    "def optimizer(learning_rate, grad_clip, loss):\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    \n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(character_set)\n",
    "\n",
    "lstmsize = 780\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "character_length = 50\n",
    "               \n",
    "keep_prob = 0.8\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "sampling = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class build_arvix:\n",
    "    \n",
    "    def __init__(self, num_classes = len(character_set), lstmsize = lstmsize, \n",
    "                 batch_size = batch_size, character_length = character_length, \n",
    "                 keep_prob = keep_prob, num_layers = num_layers,\n",
    "                 learning_rate = 0.1, sampling = False, grad_clip = 5):\n",
    "        \n",
    "        \n",
    "        if sampling == True:\n",
    "            \n",
    "            batch_size, character_length = 1, 1\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            batch_size, character_length = batch_size, character_length\n",
    "            \n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        #Build the input placeholders\n",
    "        \n",
    "        self.inputs, self.targets, self.keep_prob = tensor_inputs(batch_size, character_length)\n",
    "        \n",
    "        #Build the LSTM archictecture\n",
    "        \n",
    "        multi_lstm, self.initial_state = LSTM_cell(lstmsize, batch_size, self.keep_prob, num_layers)\n",
    "        \n",
    "        \n",
    "        #Run inputs through LSTM Cell\n",
    "        \n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        outputs, final_state = tf.nn.dynamic_rnn(multi_lstm, x_one_hot, initial_state = self.initial_state)\n",
    "        \n",
    "        self.final_state = final_state\n",
    "        \n",
    "        \n",
    "        #predictions using lstm run\n",
    "        \n",
    "        self.logits, self.predictions = logits(outputs, lstmsize, num_classes)\n",
    "            \n",
    "            \n",
    "        #loss function & optimizer\n",
    "        \n",
    "        self.loss = loss(self.logits, self.targets, num_classes)\n",
    "        \n",
    "        self.optimizer = optimizer(learning_rate, grad_clip, self.loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-9-cc4cab5c6772>:5: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-cc4cab5c6772>:12: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-14-9445152145e7>:34: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-11-4df05d15940f>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "7630823\n",
      "epoch: 0/50.... training_step: 50.... batch_loss: 3.049984.... 0.073720 sec/batch\n",
      "epoch: 0/50.... training_step: 100.... batch_loss: 2.861654.... 0.078946 sec/batch\n",
      "epoch: 0/50.... training_step: 150.... batch_loss: 2.507607.... 0.075006 sec/batch\n",
      "epoch: 0/50.... training_step: 200.... batch_loss: 2.276673.... 0.074080 sec/batch\n",
      "epoch: 0/50.... training_step: 250.... batch_loss: 2.059935.... 0.075517 sec/batch\n",
      "epoch: 0/50.... training_step: 300.... batch_loss: 1.830162.... 0.075942 sec/batch\n",
      "epoch: 0/50.... training_step: 350.... batch_loss: 1.580538.... 0.076162 sec/batch\n",
      "epoch: 0/50.... training_step: 400.... batch_loss: 1.494491.... 0.074160 sec/batch\n",
      "epoch: 0/50.... training_step: 450.... batch_loss: 1.476223.... 0.075879 sec/batch\n",
      "epoch: 0/50.... training_step: 500.... batch_loss: 1.379296.... 0.078664 sec/batch\n",
      "epoch: 0/50.... training_step: 550.... batch_loss: 1.288628.... 0.074424 sec/batch\n",
      "epoch: 0/50.... training_step: 600.... batch_loss: 1.210690.... 0.073551 sec/batch\n",
      "epoch: 0/50.... training_step: 650.... batch_loss: 1.166772.... 0.073848 sec/batch\n",
      "epoch: 0/50.... training_step: 700.... batch_loss: 1.044134.... 0.075341 sec/batch\n",
      "epoch: 0/50.... training_step: 750.... batch_loss: 1.064654.... 0.076905 sec/batch\n",
      "epoch: 0/50.... training_step: 800.... batch_loss: 1.010835.... 0.079963 sec/batch\n",
      "epoch: 0/50.... training_step: 850.... batch_loss: 1.037961.... 0.075404 sec/batch\n",
      "epoch: 0/50.... training_step: 900.... batch_loss: 0.865591.... 0.074872 sec/batch\n",
      "epoch: 0/50.... training_step: 950.... batch_loss: 0.810456.... 0.075845 sec/batch\n",
      "epoch: 0/50.... training_step: 1000.... batch_loss: 0.772333.... 0.073442 sec/batch\n",
      "epoch: 0/50.... training_step: 1050.... batch_loss: 0.774700.... 0.073336 sec/batch\n",
      "epoch: 0/50.... training_step: 1100.... batch_loss: 0.676493.... 0.075774 sec/batch\n",
      "epoch: 0/50.... training_step: 1150.... batch_loss: 0.695159.... 0.075686 sec/batch\n",
      "epoch: 0/50.... training_step: 1200.... batch_loss: 0.610083.... 0.073524 sec/batch\n",
      "epoch: 0/50.... training_step: 1250.... batch_loss: 0.549756.... 0.075262 sec/batch\n",
      "epoch: 0/50.... training_step: 1300.... batch_loss: 0.538533.... 0.073696 sec/batch\n",
      "epoch: 0/50.... training_step: 1350.... batch_loss: 0.518569.... 0.076415 sec/batch\n",
      "epoch: 0/50.... training_step: 1400.... batch_loss: 0.467134.... 0.076666 sec/batch\n",
      "epoch: 0/50.... training_step: 1450.... batch_loss: 0.440014.... 0.074133 sec/batch\n",
      "epoch: 0/50.... training_step: 1500.... batch_loss: 0.395564.... 0.077229 sec/batch\n",
      "epoch: 0/50.... training_step: 1550.... batch_loss: 0.395127.... 0.074067 sec/batch\n",
      "epoch: 0/50.... training_step: 1600.... batch_loss: 0.346135.... 0.074330 sec/batch\n",
      "epoch: 0/50.... training_step: 1650.... batch_loss: 0.331663.... 0.074452 sec/batch\n",
      "epoch: 0/50.... training_step: 1700.... batch_loss: 0.312230.... 0.075085 sec/batch\n",
      "epoch: 0/50.... training_step: 1750.... batch_loss: 0.287364.... 0.075413 sec/batch\n",
      "epoch: 0/50.... training_step: 1800.... batch_loss: 0.284799.... 0.075087 sec/batch\n",
      "epoch: 0/50.... training_step: 1850.... batch_loss: 0.280628.... 0.074734 sec/batch\n",
      "epoch: 0/50.... training_step: 1900.... batch_loss: 0.264463.... 0.078790 sec/batch\n",
      "epoch: 0/50.... training_step: 1950.... batch_loss: 0.236828.... 0.073862 sec/batch\n",
      "epoch: 0/50.... training_step: 2000.... batch_loss: 0.210853.... 0.077638 sec/batch\n",
      "epoch: 0/50.... training_step: 2050.... batch_loss: 0.219559.... 0.074272 sec/batch\n",
      "epoch: 0/50.... training_step: 2100.... batch_loss: 0.229101.... 0.073635 sec/batch\n",
      "epoch: 0/50.... training_step: 2150.... batch_loss: 0.216399.... 0.078630 sec/batch\n",
      "epoch: 0/50.... training_step: 2200.... batch_loss: 0.192127.... 0.078959 sec/batch\n",
      "epoch: 0/50.... training_step: 2250.... batch_loss: 0.183228.... 0.074107 sec/batch\n",
      "epoch: 0/50.... training_step: 2300.... batch_loss: 0.196214.... 0.075271 sec/batch\n",
      "epoch: 0/50.... training_step: 2350.... batch_loss: 0.189509.... 0.080466 sec/batch\n",
      "epoch: 1/50.... training_step: 2400.... batch_loss: 0.168889.... 0.080479 sec/batch\n",
      "epoch: 1/50.... training_step: 2450.... batch_loss: 0.167619.... 0.076141 sec/batch\n",
      "epoch: 1/50.... training_step: 2500.... batch_loss: 0.156257.... 0.076163 sec/batch\n",
      "epoch: 1/50.... training_step: 2550.... batch_loss: 0.151191.... 0.074424 sec/batch\n",
      "epoch: 1/50.... training_step: 2600.... batch_loss: 0.154723.... 0.073452 sec/batch\n",
      "epoch: 1/50.... training_step: 2650.... batch_loss: 0.148710.... 0.076107 sec/batch\n",
      "epoch: 1/50.... training_step: 2700.... batch_loss: 0.139939.... 0.077749 sec/batch\n",
      "epoch: 1/50.... training_step: 2750.... batch_loss: 0.132233.... 0.075413 sec/batch\n",
      "epoch: 1/50.... training_step: 2800.... batch_loss: 0.140437.... 0.075218 sec/batch\n",
      "epoch: 1/50.... training_step: 2850.... batch_loss: 0.132996.... 0.077469 sec/batch\n",
      "epoch: 1/50.... training_step: 2900.... batch_loss: 0.130718.... 0.078454 sec/batch\n",
      "epoch: 1/50.... training_step: 2950.... batch_loss: 0.130471.... 0.080864 sec/batch\n",
      "epoch: 1/50.... training_step: 3000.... batch_loss: 0.128868.... 0.078838 sec/batch\n",
      "epoch: 1/50.... training_step: 3050.... batch_loss: 0.143426.... 0.076269 sec/batch\n",
      "epoch: 1/50.... training_step: 3100.... batch_loss: 0.116816.... 0.074816 sec/batch\n",
      "epoch: 1/50.... training_step: 3150.... batch_loss: 0.129467.... 0.074723 sec/batch\n",
      "epoch: 1/50.... training_step: 3200.... batch_loss: 0.104534.... 0.082503 sec/batch\n",
      "epoch: 1/50.... training_step: 3250.... batch_loss: 0.109285.... 0.074774 sec/batch\n",
      "epoch: 1/50.... training_step: 3300.... batch_loss: 0.118565.... 0.075410 sec/batch\n",
      "epoch: 1/50.... training_step: 3350.... batch_loss: 0.115529.... 0.076449 sec/batch\n",
      "epoch: 1/50.... training_step: 3400.... batch_loss: 0.111575.... 0.076481 sec/batch\n",
      "epoch: 1/50.... training_step: 3450.... batch_loss: 0.105711.... 0.080388 sec/batch\n",
      "epoch: 1/50.... training_step: 3500.... batch_loss: 0.106251.... 0.073870 sec/batch\n",
      "epoch: 1/50.... training_step: 3550.... batch_loss: 0.118938.... 0.076013 sec/batch\n",
      "epoch: 1/50.... training_step: 3600.... batch_loss: 0.107015.... 0.083163 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/50.... training_step: 3650.... batch_loss: 0.102355.... 0.075258 sec/batch\n",
      "epoch: 1/50.... training_step: 3700.... batch_loss: 0.103925.... 0.075031 sec/batch\n",
      "epoch: 1/50.... training_step: 3750.... batch_loss: 0.111810.... 0.076310 sec/batch\n",
      "epoch: 1/50.... training_step: 3800.... batch_loss: 0.100358.... 0.075227 sec/batch\n",
      "epoch: 1/50.... training_step: 3850.... batch_loss: 0.096712.... 0.078383 sec/batch\n",
      "epoch: 1/50.... training_step: 3900.... batch_loss: 0.100135.... 0.074508 sec/batch\n",
      "epoch: 1/50.... training_step: 3950.... batch_loss: 0.096455.... 0.074539 sec/batch\n",
      "epoch: 1/50.... training_step: 4000.... batch_loss: 0.102573.... 0.074327 sec/batch\n",
      "epoch: 1/50.... training_step: 4050.... batch_loss: 0.099794.... 0.074052 sec/batch\n",
      "epoch: 1/50.... training_step: 4100.... batch_loss: 0.089429.... 0.075745 sec/batch\n",
      "epoch: 1/50.... training_step: 4150.... batch_loss: 0.094096.... 0.075433 sec/batch\n",
      "epoch: 1/50.... training_step: 4200.... batch_loss: 0.080668.... 0.074409 sec/batch\n",
      "epoch: 1/50.... training_step: 4250.... batch_loss: 0.098178.... 0.075652 sec/batch\n",
      "epoch: 1/50.... training_step: 4300.... batch_loss: 0.109502.... 0.075521 sec/batch\n",
      "epoch: 1/50.... training_step: 4350.... batch_loss: 0.090290.... 0.074883 sec/batch\n",
      "epoch: 1/50.... training_step: 4400.... batch_loss: 0.083487.... 0.073349 sec/batch\n",
      "epoch: 1/50.... training_step: 4450.... batch_loss: 0.092343.... 0.077107 sec/batch\n",
      "epoch: 1/50.... training_step: 4500.... batch_loss: 0.096953.... 0.076531 sec/batch\n",
      "epoch: 1/50.... training_step: 4550.... batch_loss: 0.081407.... 0.076407 sec/batch\n",
      "epoch: 1/50.... training_step: 4600.... batch_loss: 0.091953.... 0.074976 sec/batch\n",
      "epoch: 1/50.... training_step: 4650.... batch_loss: 0.093763.... 0.074382 sec/batch\n",
      "epoch: 1/50.... training_step: 4700.... batch_loss: 0.092657.... 0.075593 sec/batch\n",
      "epoch: 2/50.... training_step: 4750.... batch_loss: 0.087645.... 0.076867 sec/batch\n",
      "epoch: 2/50.... training_step: 4800.... batch_loss: 0.077835.... 0.075234 sec/batch\n",
      "epoch: 2/50.... training_step: 4850.... batch_loss: 0.098597.... 0.073889 sec/batch\n",
      "epoch: 2/50.... training_step: 4900.... batch_loss: 0.087192.... 0.075001 sec/batch\n",
      "epoch: 2/50.... training_step: 4950.... batch_loss: 0.087493.... 0.074760 sec/batch\n",
      "epoch: 2/50.... training_step: 5000.... batch_loss: 0.084340.... 0.074586 sec/batch\n",
      "epoch: 2/50.... training_step: 5050.... batch_loss: 0.088758.... 0.076324 sec/batch\n",
      "epoch: 2/50.... training_step: 5100.... batch_loss: 0.085170.... 0.075789 sec/batch\n",
      "epoch: 2/50.... training_step: 5150.... batch_loss: 0.080519.... 0.074240 sec/batch\n",
      "epoch: 2/50.... training_step: 5200.... batch_loss: 0.081064.... 0.073857 sec/batch\n",
      "epoch: 2/50.... training_step: 5250.... batch_loss: 0.073543.... 0.074892 sec/batch\n",
      "epoch: 2/50.... training_step: 5300.... batch_loss: 0.084428.... 0.075969 sec/batch\n",
      "epoch: 2/50.... training_step: 5350.... batch_loss: 0.090097.... 0.075024 sec/batch\n",
      "epoch: 2/50.... training_step: 5400.... batch_loss: 0.075663.... 0.073650 sec/batch\n",
      "epoch: 2/50.... training_step: 5450.... batch_loss: 0.082659.... 0.077504 sec/batch\n",
      "epoch: 2/50.... training_step: 5500.... batch_loss: 0.089832.... 0.074638 sec/batch\n",
      "epoch: 2/50.... training_step: 5550.... batch_loss: 0.090627.... 0.078014 sec/batch\n",
      "epoch: 2/50.... training_step: 5600.... batch_loss: 0.084756.... 0.077941 sec/batch\n",
      "epoch: 2/50.... training_step: 5650.... batch_loss: 0.086122.... 0.079211 sec/batch\n",
      "epoch: 2/50.... training_step: 5700.... batch_loss: 0.083346.... 0.078800 sec/batch\n",
      "epoch: 2/50.... training_step: 5750.... batch_loss: 0.075696.... 0.074776 sec/batch\n",
      "epoch: 2/50.... training_step: 5800.... batch_loss: 0.089231.... 0.076909 sec/batch\n",
      "epoch: 2/50.... training_step: 5850.... batch_loss: 0.073719.... 0.076588 sec/batch\n",
      "epoch: 2/50.... training_step: 5900.... batch_loss: 0.079254.... 0.073742 sec/batch\n",
      "epoch: 2/50.... training_step: 5950.... batch_loss: 0.084864.... 0.074393 sec/batch\n",
      "epoch: 2/50.... training_step: 6000.... batch_loss: 0.084446.... 0.075634 sec/batch\n",
      "epoch: 2/50.... training_step: 6050.... batch_loss: 0.078945.... 0.073405 sec/batch\n",
      "epoch: 2/50.... training_step: 6100.... batch_loss: 0.070544.... 0.077672 sec/batch\n",
      "epoch: 2/50.... training_step: 6150.... batch_loss: 0.073162.... 0.076408 sec/batch\n",
      "epoch: 2/50.... training_step: 6200.... batch_loss: 0.078333.... 0.074328 sec/batch\n",
      "epoch: 2/50.... training_step: 6250.... batch_loss: 0.074559.... 0.075335 sec/batch\n",
      "epoch: 2/50.... training_step: 6300.... batch_loss: 0.077171.... 0.073148 sec/batch\n",
      "epoch: 2/50.... training_step: 6350.... batch_loss: 0.082485.... 0.075355 sec/batch\n",
      "epoch: 2/50.... training_step: 6400.... batch_loss: 0.075407.... 0.072781 sec/batch\n",
      "epoch: 2/50.... training_step: 6450.... batch_loss: 0.083106.... 0.074367 sec/batch\n",
      "epoch: 2/50.... training_step: 6500.... batch_loss: 0.067341.... 0.076786 sec/batch\n",
      "epoch: 2/50.... training_step: 6550.... batch_loss: 0.078807.... 0.073991 sec/batch\n",
      "epoch: 2/50.... training_step: 6600.... batch_loss: 0.081342.... 0.076676 sec/batch\n",
      "epoch: 2/50.... training_step: 6650.... batch_loss: 0.080216.... 0.074257 sec/batch\n",
      "epoch: 2/50.... training_step: 6700.... batch_loss: 0.069735.... 0.075538 sec/batch\n",
      "epoch: 2/50.... training_step: 6750.... batch_loss: 0.075106.... 0.074594 sec/batch\n",
      "epoch: 2/50.... training_step: 6800.... batch_loss: 0.067736.... 0.074141 sec/batch\n",
      "epoch: 2/50.... training_step: 6850.... batch_loss: 0.070486.... 0.074090 sec/batch\n",
      "epoch: 2/50.... training_step: 6900.... batch_loss: 0.077250.... 0.073970 sec/batch\n",
      "epoch: 2/50.... training_step: 6950.... batch_loss: 0.076692.... 0.073150 sec/batch\n",
      "epoch: 2/50.... training_step: 7000.... batch_loss: 0.078584.... 0.077825 sec/batch\n",
      "epoch: 2/50.... training_step: 7050.... batch_loss: 0.071002.... 0.076434 sec/batch\n",
      "epoch: 3/50.... training_step: 7100.... batch_loss: 0.067822.... 0.077061 sec/batch\n",
      "epoch: 3/50.... training_step: 7150.... batch_loss: 0.068161.... 0.073853 sec/batch\n",
      "epoch: 3/50.... training_step: 7200.... batch_loss: 0.071573.... 0.073194 sec/batch\n",
      "epoch: 3/50.... training_step: 7250.... batch_loss: 0.067559.... 0.076825 sec/batch\n",
      "epoch: 3/50.... training_step: 7300.... batch_loss: 0.071740.... 0.077326 sec/batch\n",
      "epoch: 3/50.... training_step: 7350.... batch_loss: 0.072261.... 0.073298 sec/batch\n",
      "epoch: 3/50.... training_step: 7400.... batch_loss: 0.066023.... 0.074322 sec/batch\n",
      "epoch: 3/50.... training_step: 7450.... batch_loss: 0.080531.... 0.077365 sec/batch\n",
      "epoch: 3/50.... training_step: 7500.... batch_loss: 0.076347.... 0.077379 sec/batch\n",
      "epoch: 3/50.... training_step: 7550.... batch_loss: 0.061998.... 0.074151 sec/batch\n",
      "epoch: 3/50.... training_step: 7600.... batch_loss: 0.062538.... 0.076151 sec/batch\n",
      "epoch: 3/50.... training_step: 7650.... batch_loss: 0.067018.... 0.074669 sec/batch\n",
      "epoch: 3/50.... training_step: 7700.... batch_loss: 0.075957.... 0.077190 sec/batch\n",
      "epoch: 3/50.... training_step: 7750.... batch_loss: 0.074520.... 0.073853 sec/batch\n",
      "epoch: 3/50.... training_step: 7800.... batch_loss: 0.074207.... 0.074486 sec/batch\n",
      "epoch: 3/50.... training_step: 7850.... batch_loss: 0.067890.... 0.074626 sec/batch\n",
      "epoch: 3/50.... training_step: 7900.... batch_loss: 0.068826.... 0.077514 sec/batch\n",
      "epoch: 3/50.... training_step: 7950.... batch_loss: 0.077649.... 0.078393 sec/batch\n",
      "epoch: 3/50.... training_step: 8000.... batch_loss: 0.075018.... 0.073947 sec/batch\n",
      "epoch: 3/50.... training_step: 8050.... batch_loss: 0.068579.... 0.077697 sec/batch\n",
      "epoch: 3/50.... training_step: 8100.... batch_loss: 0.070043.... 0.077150 sec/batch\n",
      "epoch: 3/50.... training_step: 8150.... batch_loss: 0.064710.... 0.074724 sec/batch\n",
      "epoch: 3/50.... training_step: 8200.... batch_loss: 0.077353.... 0.076884 sec/batch\n",
      "epoch: 3/50.... training_step: 8250.... batch_loss: 0.082579.... 0.073741 sec/batch\n",
      "epoch: 3/50.... training_step: 8300.... batch_loss: 0.065168.... 0.078329 sec/batch\n",
      "epoch: 3/50.... training_step: 8350.... batch_loss: 0.073986.... 0.073616 sec/batch\n",
      "epoch: 3/50.... training_step: 8400.... batch_loss: 0.073786.... 0.075851 sec/batch\n",
      "epoch: 3/50.... training_step: 8450.... batch_loss: 0.070266.... 0.074161 sec/batch\n",
      "epoch: 3/50.... training_step: 8500.... batch_loss: 0.054997.... 0.073416 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3/50.... training_step: 8550.... batch_loss: 0.067493.... 0.074084 sec/batch\n",
      "epoch: 3/50.... training_step: 8600.... batch_loss: 0.069444.... 0.075144 sec/batch\n",
      "epoch: 3/50.... training_step: 8650.... batch_loss: 0.069114.... 0.076025 sec/batch\n",
      "epoch: 3/50.... training_step: 8700.... batch_loss: 0.070371.... 0.078218 sec/batch\n",
      "epoch: 3/50.... training_step: 8750.... batch_loss: 0.072462.... 0.076491 sec/batch\n",
      "epoch: 3/50.... training_step: 8800.... batch_loss: 0.074632.... 0.074667 sec/batch\n",
      "epoch: 3/50.... training_step: 8850.... batch_loss: 0.053172.... 0.076638 sec/batch\n",
      "epoch: 3/50.... training_step: 8900.... batch_loss: 0.076338.... 0.073676 sec/batch\n",
      "epoch: 3/50.... training_step: 8950.... batch_loss: 0.077867.... 0.077133 sec/batch\n",
      "epoch: 3/50.... training_step: 9000.... batch_loss: 0.076778.... 0.073642 sec/batch\n",
      "epoch: 3/50.... training_step: 9050.... batch_loss: 0.073768.... 0.075918 sec/batch\n",
      "epoch: 3/50.... training_step: 9100.... batch_loss: 0.070379.... 0.076270 sec/batch\n",
      "epoch: 3/50.... training_step: 9150.... batch_loss: 0.070299.... 0.076457 sec/batch\n",
      "epoch: 3/50.... training_step: 9200.... batch_loss: 0.075682.... 0.073849 sec/batch\n",
      "epoch: 3/50.... training_step: 9250.... batch_loss: 0.065616.... 0.075141 sec/batch\n",
      "epoch: 3/50.... training_step: 9300.... batch_loss: 0.069126.... 0.074013 sec/batch\n",
      "epoch: 3/50.... training_step: 9350.... batch_loss: 0.064308.... 0.074470 sec/batch\n",
      "epoch: 3/50.... training_step: 9400.... batch_loss: 0.064968.... 0.074524 sec/batch\n",
      "epoch: 4/50.... training_step: 9450.... batch_loss: 0.056575.... 0.074815 sec/batch\n",
      "epoch: 4/50.... training_step: 9500.... batch_loss: 0.069078.... 0.074699 sec/batch\n",
      "epoch: 4/50.... training_step: 9550.... batch_loss: 0.063987.... 0.076642 sec/batch\n",
      "epoch: 4/50.... training_step: 9600.... batch_loss: 0.064683.... 0.075724 sec/batch\n",
      "epoch: 4/50.... training_step: 9650.... batch_loss: 0.066250.... 0.079545 sec/batch\n",
      "epoch: 4/50.... training_step: 9700.... batch_loss: 0.067219.... 0.075972 sec/batch\n",
      "epoch: 4/50.... training_step: 9750.... batch_loss: 0.072850.... 0.076228 sec/batch\n",
      "epoch: 4/50.... training_step: 9800.... batch_loss: 0.072451.... 0.077294 sec/batch\n",
      "epoch: 4/50.... training_step: 9850.... batch_loss: 0.078230.... 0.073578 sec/batch\n",
      "epoch: 4/50.... training_step: 9900.... batch_loss: 0.065799.... 0.076740 sec/batch\n",
      "epoch: 4/50.... training_step: 9950.... batch_loss: 0.061992.... 0.077125 sec/batch\n",
      "epoch: 4/50.... training_step: 10000.... batch_loss: 0.065733.... 0.075372 sec/batch\n",
      "epoch: 4/50.... training_step: 10050.... batch_loss: 0.059162.... 0.081895 sec/batch\n",
      "epoch: 4/50.... training_step: 10100.... batch_loss: 0.072015.... 0.075782 sec/batch\n",
      "epoch: 4/50.... training_step: 10150.... batch_loss: 0.068663.... 0.077410 sec/batch\n",
      "epoch: 4/50.... training_step: 10200.... batch_loss: 0.063826.... 0.075236 sec/batch\n",
      "epoch: 4/50.... training_step: 10250.... batch_loss: 0.069408.... 0.074988 sec/batch\n",
      "epoch: 4/50.... training_step: 10300.... batch_loss: 0.066698.... 0.073928 sec/batch\n",
      "epoch: 4/50.... training_step: 10350.... batch_loss: 0.068982.... 0.075568 sec/batch\n",
      "epoch: 4/50.... training_step: 10400.... batch_loss: 0.068031.... 0.074164 sec/batch\n",
      "epoch: 4/50.... training_step: 10450.... batch_loss: 0.064425.... 0.073662 sec/batch\n",
      "epoch: 4/50.... training_step: 10500.... batch_loss: 0.057691.... 0.073521 sec/batch\n",
      "epoch: 4/50.... training_step: 10550.... batch_loss: 0.070328.... 0.073261 sec/batch\n",
      "epoch: 4/50.... training_step: 10600.... batch_loss: 0.061999.... 0.080733 sec/batch\n",
      "epoch: 4/50.... training_step: 10650.... batch_loss: 0.057923.... 0.073076 sec/batch\n",
      "epoch: 4/50.... training_step: 10700.... batch_loss: 0.065226.... 0.075964 sec/batch\n",
      "epoch: 4/50.... training_step: 10750.... batch_loss: 0.067338.... 0.073991 sec/batch\n",
      "epoch: 4/50.... training_step: 10800.... batch_loss: 0.073085.... 0.078556 sec/batch\n",
      "epoch: 4/50.... training_step: 10850.... batch_loss: 0.067580.... 0.072982 sec/batch\n",
      "epoch: 4/50.... training_step: 10900.... batch_loss: 0.060334.... 0.074555 sec/batch\n",
      "epoch: 4/50.... training_step: 10950.... batch_loss: 0.065137.... 0.076174 sec/batch\n",
      "epoch: 4/50.... training_step: 11000.... batch_loss: 0.071743.... 0.075575 sec/batch\n",
      "epoch: 4/50.... training_step: 11050.... batch_loss: 0.060300.... 0.077046 sec/batch\n",
      "epoch: 4/50.... training_step: 11100.... batch_loss: 0.064669.... 0.076632 sec/batch\n",
      "epoch: 4/50.... training_step: 11150.... batch_loss: 0.068416.... 0.076094 sec/batch\n",
      "epoch: 4/50.... training_step: 11200.... batch_loss: 0.074121.... 0.077498 sec/batch\n",
      "epoch: 4/50.... training_step: 11250.... batch_loss: 0.062893.... 0.075785 sec/batch\n",
      "epoch: 4/50.... training_step: 11300.... batch_loss: 0.071464.... 0.076341 sec/batch\n",
      "epoch: 4/50.... training_step: 11350.... batch_loss: 0.068494.... 0.076162 sec/batch\n",
      "epoch: 4/50.... training_step: 11400.... batch_loss: 0.062247.... 0.073884 sec/batch\n",
      "epoch: 4/50.... training_step: 11450.... batch_loss: 0.059170.... 0.074181 sec/batch\n",
      "epoch: 4/50.... training_step: 11500.... batch_loss: 0.065309.... 0.077691 sec/batch\n",
      "epoch: 4/50.... training_step: 11550.... batch_loss: 0.058137.... 0.076588 sec/batch\n",
      "epoch: 4/50.... training_step: 11600.... batch_loss: 0.068130.... 0.073352 sec/batch\n",
      "epoch: 4/50.... training_step: 11650.... batch_loss: 0.060011.... 0.074001 sec/batch\n",
      "epoch: 4/50.... training_step: 11700.... batch_loss: 0.062354.... 0.076641 sec/batch\n",
      "epoch: 4/50.... training_step: 11750.... batch_loss: 0.068428.... 0.073436 sec/batch\n",
      "epoch: 5/50.... training_step: 11800.... batch_loss: 0.060494.... 0.077178 sec/batch\n",
      "epoch: 5/50.... training_step: 11850.... batch_loss: 0.054999.... 0.073364 sec/batch\n",
      "epoch: 5/50.... training_step: 11900.... batch_loss: 0.061072.... 0.076198 sec/batch\n",
      "epoch: 5/50.... training_step: 11950.... batch_loss: 0.065468.... 0.074945 sec/batch\n",
      "epoch: 5/50.... training_step: 12000.... batch_loss: 0.055766.... 0.078716 sec/batch\n",
      "epoch: 5/50.... training_step: 12050.... batch_loss: 0.061810.... 0.075619 sec/batch\n",
      "epoch: 5/50.... training_step: 12100.... batch_loss: 0.066569.... 0.074050 sec/batch\n",
      "epoch: 5/50.... training_step: 12150.... batch_loss: 0.067586.... 0.075376 sec/batch\n",
      "epoch: 5/50.... training_step: 12200.... batch_loss: 0.072468.... 0.076967 sec/batch\n",
      "epoch: 5/50.... training_step: 12250.... batch_loss: 0.058728.... 0.073560 sec/batch\n",
      "epoch: 5/50.... training_step: 12300.... batch_loss: 0.067729.... 0.076869 sec/batch\n",
      "epoch: 5/50.... training_step: 12350.... batch_loss: 0.065521.... 0.073552 sec/batch\n",
      "epoch: 5/50.... training_step: 12400.... batch_loss: 0.060575.... 0.075636 sec/batch\n",
      "epoch: 5/50.... training_step: 12450.... batch_loss: 0.063499.... 0.073635 sec/batch\n",
      "epoch: 5/50.... training_step: 12500.... batch_loss: 0.059017.... 0.074739 sec/batch\n",
      "epoch: 5/50.... training_step: 12550.... batch_loss: 0.073585.... 0.074081 sec/batch\n",
      "epoch: 5/50.... training_step: 12600.... batch_loss: 0.069297.... 0.076627 sec/batch\n",
      "epoch: 5/50.... training_step: 12650.... batch_loss: 0.064570.... 0.072959 sec/batch\n",
      "epoch: 5/50.... training_step: 12700.... batch_loss: 0.066959.... 0.079669 sec/batch\n",
      "epoch: 5/50.... training_step: 12750.... batch_loss: 0.062372.... 0.073812 sec/batch\n",
      "epoch: 5/50.... training_step: 12800.... batch_loss: 0.063429.... 0.078948 sec/batch\n",
      "epoch: 5/50.... training_step: 12850.... batch_loss: 0.054597.... 0.073334 sec/batch\n",
      "epoch: 5/50.... training_step: 12900.... batch_loss: 0.060174.... 0.075222 sec/batch\n",
      "epoch: 5/50.... training_step: 12950.... batch_loss: 0.060508.... 0.074176 sec/batch\n",
      "epoch: 5/50.... training_step: 13000.... batch_loss: 0.058893.... 0.077508 sec/batch\n",
      "epoch: 5/50.... training_step: 13050.... batch_loss: 0.062706.... 0.073116 sec/batch\n",
      "epoch: 5/50.... training_step: 13100.... batch_loss: 0.065866.... 0.076040 sec/batch\n",
      "epoch: 5/50.... training_step: 13150.... batch_loss: 0.056170.... 0.075143 sec/batch\n",
      "epoch: 5/50.... training_step: 13200.... batch_loss: 0.064137.... 0.077341 sec/batch\n",
      "epoch: 5/50.... training_step: 13250.... batch_loss: 0.065317.... 0.083905 sec/batch\n",
      "epoch: 5/50.... training_step: 13300.... batch_loss: 0.060825.... 0.073210 sec/batch\n",
      "epoch: 5/50.... training_step: 13350.... batch_loss: 0.063863.... 0.074067 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5/50.... training_step: 13400.... batch_loss: 0.064993.... 0.073269 sec/batch\n",
      "epoch: 5/50.... training_step: 13450.... batch_loss: 0.062541.... 0.076193 sec/batch\n",
      "epoch: 5/50.... training_step: 13500.... batch_loss: 0.065304.... 0.074013 sec/batch\n",
      "epoch: 5/50.... training_step: 13550.... batch_loss: 0.063007.... 0.076067 sec/batch\n",
      "epoch: 5/50.... training_step: 13600.... batch_loss: 0.054053.... 0.073713 sec/batch\n",
      "epoch: 5/50.... training_step: 13650.... batch_loss: 0.059402.... 0.076068 sec/batch\n",
      "epoch: 5/50.... training_step: 13700.... batch_loss: 0.055921.... 0.075742 sec/batch\n",
      "epoch: 5/50.... training_step: 13750.... batch_loss: 0.058658.... 0.073449 sec/batch\n",
      "epoch: 5/50.... training_step: 13800.... batch_loss: 0.062864.... 0.074556 sec/batch\n",
      "epoch: 5/50.... training_step: 13850.... batch_loss: 0.059313.... 0.073543 sec/batch\n",
      "epoch: 5/50.... training_step: 13900.... batch_loss: 0.062429.... 0.077976 sec/batch\n",
      "epoch: 5/50.... training_step: 13950.... batch_loss: 0.053735.... 0.072797 sec/batch\n",
      "epoch: 5/50.... training_step: 14000.... batch_loss: 0.067453.... 0.074654 sec/batch\n",
      "epoch: 5/50.... training_step: 14050.... batch_loss: 0.067784.... 0.076823 sec/batch\n",
      "epoch: 5/50.... training_step: 14100.... batch_loss: 0.055319.... 0.075133 sec/batch\n",
      "epoch: 6/50.... training_step: 14150.... batch_loss: 0.061072.... 0.073666 sec/batch\n",
      "epoch: 6/50.... training_step: 14200.... batch_loss: 0.060265.... 0.074716 sec/batch\n",
      "epoch: 6/50.... training_step: 14250.... batch_loss: 0.066873.... 0.075287 sec/batch\n",
      "epoch: 6/50.... training_step: 14300.... batch_loss: 0.064373.... 0.073634 sec/batch\n",
      "epoch: 6/50.... training_step: 14350.... batch_loss: 0.055035.... 0.073980 sec/batch\n",
      "epoch: 6/50.... training_step: 14400.... batch_loss: 0.060845.... 0.073282 sec/batch\n",
      "epoch: 6/50.... training_step: 14450.... batch_loss: 0.053566.... 0.073142 sec/batch\n",
      "epoch: 6/50.... training_step: 14500.... batch_loss: 0.070260.... 0.073261 sec/batch\n",
      "epoch: 6/50.... training_step: 14550.... batch_loss: 0.068016.... 0.074233 sec/batch\n",
      "epoch: 6/50.... training_step: 14600.... batch_loss: 0.057925.... 0.074346 sec/batch\n",
      "epoch: 6/50.... training_step: 14650.... batch_loss: 0.056812.... 0.076412 sec/batch\n",
      "epoch: 6/50.... training_step: 14700.... batch_loss: 0.061805.... 0.074654 sec/batch\n",
      "epoch: 6/50.... training_step: 14750.... batch_loss: 0.068198.... 0.076156 sec/batch\n",
      "epoch: 6/50.... training_step: 14800.... batch_loss: 0.062437.... 0.075099 sec/batch\n",
      "epoch: 6/50.... training_step: 14850.... batch_loss: 0.054380.... 0.073953 sec/batch\n",
      "epoch: 6/50.... training_step: 14900.... batch_loss: 0.064966.... 0.076009 sec/batch\n",
      "epoch: 6/50.... training_step: 14950.... batch_loss: 0.062677.... 0.076056 sec/batch\n",
      "epoch: 6/50.... training_step: 15000.... batch_loss: 0.054273.... 0.077045 sec/batch\n",
      "epoch: 6/50.... training_step: 15050.... batch_loss: 0.066321.... 0.075519 sec/batch\n",
      "epoch: 6/50.... training_step: 15100.... batch_loss: 0.066022.... 0.076753 sec/batch\n",
      "epoch: 6/50.... training_step: 15150.... batch_loss: 0.063331.... 0.076382 sec/batch\n",
      "epoch: 6/50.... training_step: 15200.... batch_loss: 0.060085.... 0.074830 sec/batch\n",
      "epoch: 6/50.... training_step: 15250.... batch_loss: 0.057184.... 0.074054 sec/batch\n",
      "epoch: 6/50.... training_step: 15300.... batch_loss: 0.055934.... 0.074322 sec/batch\n",
      "epoch: 6/50.... training_step: 15350.... batch_loss: 0.061451.... 0.076433 sec/batch\n",
      "epoch: 6/50.... training_step: 15400.... batch_loss: 0.062359.... 0.077889 sec/batch\n",
      "epoch: 6/50.... training_step: 15450.... batch_loss: 0.061074.... 0.074085 sec/batch\n",
      "epoch: 6/50.... training_step: 15500.... batch_loss: 0.056316.... 0.073911 sec/batch\n",
      "epoch: 6/50.... training_step: 15550.... batch_loss: 0.072505.... 0.075371 sec/batch\n",
      "epoch: 6/50.... training_step: 15600.... batch_loss: 0.066474.... 0.076955 sec/batch\n",
      "epoch: 6/50.... training_step: 15650.... batch_loss: 0.062160.... 0.074276 sec/batch\n",
      "epoch: 6/50.... training_step: 15700.... batch_loss: 0.058615.... 0.073701 sec/batch\n",
      "epoch: 6/50.... training_step: 15750.... batch_loss: 0.063158.... 0.076015 sec/batch\n",
      "epoch: 6/50.... training_step: 15800.... batch_loss: 0.059616.... 0.073278 sec/batch\n",
      "epoch: 6/50.... training_step: 15850.... batch_loss: 0.063224.... 0.077629 sec/batch\n",
      "epoch: 6/50.... training_step: 15900.... batch_loss: 0.059555.... 0.073112 sec/batch\n",
      "epoch: 6/50.... training_step: 15950.... batch_loss: 0.063106.... 0.076305 sec/batch\n",
      "epoch: 6/50.... training_step: 16000.... batch_loss: 0.060606.... 0.073169 sec/batch\n",
      "epoch: 6/50.... training_step: 16050.... batch_loss: 0.056923.... 0.074523 sec/batch\n",
      "epoch: 6/50.... training_step: 16100.... batch_loss: 0.064331.... 0.076457 sec/batch\n",
      "epoch: 6/50.... training_step: 16150.... batch_loss: 0.061616.... 0.073705 sec/batch\n",
      "epoch: 6/50.... training_step: 16200.... batch_loss: 0.069147.... 0.074748 sec/batch\n",
      "epoch: 6/50.... training_step: 16250.... batch_loss: 0.058763.... 0.075964 sec/batch\n",
      "epoch: 6/50.... training_step: 16300.... batch_loss: 0.060202.... 0.074939 sec/batch\n",
      "epoch: 6/50.... training_step: 16350.... batch_loss: 0.068592.... 0.073926 sec/batch\n",
      "epoch: 6/50.... training_step: 16400.... batch_loss: 0.054160.... 0.074078 sec/batch\n",
      "epoch: 6/50.... training_step: 16450.... batch_loss: 0.061844.... 0.074227 sec/batch\n",
      "epoch: 7/50.... training_step: 16500.... batch_loss: 0.056400.... 0.073606 sec/batch\n",
      "epoch: 7/50.... training_step: 16550.... batch_loss: 0.052122.... 0.074942 sec/batch\n",
      "epoch: 7/50.... training_step: 16600.... batch_loss: 0.061771.... 0.074141 sec/batch\n",
      "epoch: 7/50.... training_step: 16650.... batch_loss: 0.062801.... 0.076970 sec/batch\n",
      "epoch: 7/50.... training_step: 16700.... batch_loss: 0.058386.... 0.074481 sec/batch\n",
      "epoch: 7/50.... training_step: 16750.... batch_loss: 0.063637.... 0.077520 sec/batch\n",
      "epoch: 7/50.... training_step: 16800.... batch_loss: 0.057352.... 0.074279 sec/batch\n",
      "epoch: 7/50.... training_step: 16850.... batch_loss: 0.073941.... 0.075679 sec/batch\n",
      "epoch: 7/50.... training_step: 16900.... batch_loss: 0.057381.... 0.073391 sec/batch\n",
      "epoch: 7/50.... training_step: 16950.... batch_loss: 0.047039.... 0.077854 sec/batch\n",
      "epoch: 7/50.... training_step: 17000.... batch_loss: 0.052989.... 0.074572 sec/batch\n",
      "epoch: 7/50.... training_step: 17050.... batch_loss: 0.057048.... 0.076679 sec/batch\n",
      "epoch: 7/50.... training_step: 17100.... batch_loss: 0.055592.... 0.076048 sec/batch\n",
      "epoch: 7/50.... training_step: 17150.... batch_loss: 0.053862.... 0.074681 sec/batch\n",
      "epoch: 7/50.... training_step: 17200.... batch_loss: 0.053990.... 0.073658 sec/batch\n",
      "epoch: 7/50.... training_step: 17250.... batch_loss: 0.053724.... 0.074738 sec/batch\n",
      "epoch: 7/50.... training_step: 17300.... batch_loss: 0.063923.... 0.073915 sec/batch\n",
      "epoch: 7/50.... training_step: 17350.... batch_loss: 0.059775.... 0.073988 sec/batch\n",
      "epoch: 7/50.... training_step: 17400.... batch_loss: 0.058690.... 0.073725 sec/batch\n",
      "epoch: 7/50.... training_step: 17450.... batch_loss: 0.052802.... 0.074720 sec/batch\n",
      "epoch: 7/50.... training_step: 17500.... batch_loss: 0.060640.... 0.074220 sec/batch\n",
      "epoch: 7/50.... training_step: 17550.... batch_loss: 0.064112.... 0.076391 sec/batch\n",
      "epoch: 7/50.... training_step: 17600.... batch_loss: 0.060434.... 0.076436 sec/batch\n",
      "epoch: 7/50.... training_step: 17650.... batch_loss: 0.062454.... 0.073644 sec/batch\n",
      "epoch: 7/50.... training_step: 17700.... batch_loss: 0.062142.... 0.076597 sec/batch\n",
      "epoch: 7/50.... training_step: 17750.... batch_loss: 0.056984.... 0.073220 sec/batch\n",
      "epoch: 7/50.... training_step: 17800.... batch_loss: 0.056650.... 0.074506 sec/batch\n",
      "epoch: 7/50.... training_step: 17850.... batch_loss: 0.053406.... 0.077827 sec/batch\n",
      "epoch: 7/50.... training_step: 17900.... batch_loss: 0.058723.... 0.075104 sec/batch\n",
      "epoch: 7/50.... training_step: 17950.... batch_loss: 0.053696.... 0.074131 sec/batch\n",
      "epoch: 7/50.... training_step: 18000.... batch_loss: 0.059709.... 0.075558 sec/batch\n",
      "epoch: 7/50.... training_step: 18050.... batch_loss: 0.060649.... 0.074926 sec/batch\n",
      "epoch: 7/50.... training_step: 18100.... batch_loss: 0.053122.... 0.075495 sec/batch\n",
      "epoch: 7/50.... training_step: 18150.... batch_loss: 0.052407.... 0.076606 sec/batch\n",
      "epoch: 7/50.... training_step: 18200.... batch_loss: 0.057617.... 0.074429 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7/50.... training_step: 18250.... batch_loss: 0.055748.... 0.077394 sec/batch\n",
      "epoch: 7/50.... training_step: 18300.... batch_loss: 0.064110.... 0.075281 sec/batch\n",
      "epoch: 7/50.... training_step: 18350.... batch_loss: 0.059088.... 0.077505 sec/batch\n",
      "epoch: 7/50.... training_step: 18400.... batch_loss: 0.055927.... 0.073638 sec/batch\n",
      "epoch: 7/50.... training_step: 18450.... batch_loss: 0.057657.... 0.076480 sec/batch\n",
      "epoch: 7/50.... training_step: 18500.... batch_loss: 0.057326.... 0.073614 sec/batch\n",
      "epoch: 7/50.... training_step: 18550.... batch_loss: 0.059904.... 0.075296 sec/batch\n",
      "epoch: 7/50.... training_step: 18600.... batch_loss: 0.055112.... 0.074013 sec/batch\n",
      "epoch: 7/50.... training_step: 18650.... batch_loss: 0.061056.... 0.074326 sec/batch\n",
      "epoch: 7/50.... training_step: 18700.... batch_loss: 0.063263.... 0.075399 sec/batch\n",
      "epoch: 7/50.... training_step: 18750.... batch_loss: 0.065232.... 0.073840 sec/batch\n",
      "epoch: 7/50.... training_step: 18800.... batch_loss: 0.061652.... 0.073842 sec/batch\n",
      "epoch: 8/50.... training_step: 18850.... batch_loss: 0.058943.... 0.074651 sec/batch\n",
      "epoch: 8/50.... training_step: 18900.... batch_loss: 0.055859.... 0.075054 sec/batch\n",
      "epoch: 8/50.... training_step: 18950.... batch_loss: 0.060736.... 0.073795 sec/batch\n",
      "epoch: 8/50.... training_step: 19000.... batch_loss: 0.054386.... 0.076179 sec/batch\n",
      "epoch: 8/50.... training_step: 19050.... batch_loss: 0.058532.... 0.073840 sec/batch\n",
      "epoch: 8/50.... training_step: 19100.... batch_loss: 0.057372.... 0.077556 sec/batch\n",
      "epoch: 8/50.... training_step: 19150.... batch_loss: 0.067526.... 0.075631 sec/batch\n",
      "epoch: 8/50.... training_step: 19200.... batch_loss: 0.056098.... 0.076352 sec/batch\n",
      "epoch: 8/50.... training_step: 19250.... batch_loss: 0.056072.... 0.073961 sec/batch\n",
      "epoch: 8/50.... training_step: 19300.... batch_loss: 0.056639.... 0.076625 sec/batch\n",
      "epoch: 8/50.... training_step: 19350.... batch_loss: 0.058372.... 0.073512 sec/batch\n",
      "epoch: 8/50.... training_step: 19400.... batch_loss: 0.056350.... 0.073339 sec/batch\n",
      "epoch: 8/50.... training_step: 19450.... batch_loss: 0.054249.... 0.073584 sec/batch\n",
      "epoch: 8/50.... training_step: 19500.... batch_loss: 0.059314.... 0.075565 sec/batch\n",
      "epoch: 8/50.... training_step: 19550.... batch_loss: 0.052287.... 0.076267 sec/batch\n",
      "epoch: 8/50.... training_step: 19600.... batch_loss: 0.056390.... 0.074096 sec/batch\n",
      "epoch: 8/50.... training_step: 19650.... batch_loss: 0.055332.... 0.076062 sec/batch\n",
      "epoch: 8/50.... training_step: 19700.... batch_loss: 0.055147.... 0.077127 sec/batch\n",
      "epoch: 8/50.... training_step: 19750.... batch_loss: 0.056773.... 0.075113 sec/batch\n",
      "epoch: 8/50.... training_step: 19800.... batch_loss: 0.055296.... 0.076655 sec/batch\n",
      "epoch: 8/50.... training_step: 19850.... batch_loss: 0.055094.... 0.072776 sec/batch\n",
      "epoch: 8/50.... training_step: 19900.... batch_loss: 0.052965.... 0.076399 sec/batch\n",
      "epoch: 8/50.... training_step: 19950.... batch_loss: 0.061989.... 0.075663 sec/batch\n",
      "epoch: 8/50.... training_step: 20000.... batch_loss: 0.055644.... 0.075790 sec/batch\n",
      "epoch: 8/50.... training_step: 20050.... batch_loss: 0.053677.... 0.078287 sec/batch\n",
      "epoch: 8/50.... training_step: 20100.... batch_loss: 0.054161.... 0.073909 sec/batch\n",
      "epoch: 8/50.... training_step: 20150.... batch_loss: 0.056236.... 0.079900 sec/batch\n",
      "epoch: 8/50.... training_step: 20200.... batch_loss: 0.048184.... 0.075343 sec/batch\n",
      "epoch: 8/50.... training_step: 20250.... batch_loss: 0.062182.... 0.078738 sec/batch\n",
      "epoch: 8/50.... training_step: 20300.... batch_loss: 0.048547.... 0.076860 sec/batch\n",
      "epoch: 8/50.... training_step: 20350.... batch_loss: 0.053254.... 0.073704 sec/batch\n",
      "epoch: 8/50.... training_step: 20400.... batch_loss: 0.055096.... 0.074809 sec/batch\n",
      "epoch: 8/50.... training_step: 20450.... batch_loss: 0.065551.... 0.075333 sec/batch\n",
      "epoch: 8/50.... training_step: 20500.... batch_loss: 0.057077.... 0.075384 sec/batch\n",
      "epoch: 8/50.... training_step: 20550.... batch_loss: 0.061263.... 0.076482 sec/batch\n",
      "epoch: 8/50.... training_step: 20600.... batch_loss: 0.052405.... 0.074058 sec/batch\n",
      "epoch: 8/50.... training_step: 20650.... batch_loss: 0.051510.... 0.075244 sec/batch\n",
      "epoch: 8/50.... training_step: 20700.... batch_loss: 0.052906.... 0.074364 sec/batch\n",
      "epoch: 8/50.... training_step: 20750.... batch_loss: 0.052407.... 0.072946 sec/batch\n",
      "epoch: 8/50.... training_step: 20800.... batch_loss: 0.049315.... 0.075545 sec/batch\n",
      "epoch: 8/50.... training_step: 20850.... batch_loss: 0.052575.... 0.074347 sec/batch\n",
      "epoch: 8/50.... training_step: 20900.... batch_loss: 0.054189.... 0.073774 sec/batch\n",
      "epoch: 8/50.... training_step: 20950.... batch_loss: 0.057197.... 0.073439 sec/batch\n",
      "epoch: 8/50.... training_step: 21000.... batch_loss: 0.047452.... 0.076689 sec/batch\n",
      "epoch: 8/50.... training_step: 21050.... batch_loss: 0.063872.... 0.073900 sec/batch\n",
      "epoch: 8/50.... training_step: 21100.... batch_loss: 0.069083.... 0.076343 sec/batch\n",
      "epoch: 8/50.... training_step: 21150.... batch_loss: 0.050630.... 0.076315 sec/batch\n",
      "epoch: 8/50.... training_step: 21200.... batch_loss: 0.058484.... 0.074209 sec/batch\n",
      "epoch: 9/50.... training_step: 21250.... batch_loss: 0.064161.... 0.073416 sec/batch\n",
      "epoch: 9/50.... training_step: 21300.... batch_loss: 0.063947.... 0.076712 sec/batch\n",
      "epoch: 9/50.... training_step: 21350.... batch_loss: 0.061816.... 0.075252 sec/batch\n",
      "epoch: 9/50.... training_step: 21400.... batch_loss: 0.063825.... 0.074552 sec/batch\n",
      "epoch: 9/50.... training_step: 21450.... batch_loss: 0.057833.... 0.075423 sec/batch\n",
      "epoch: 9/50.... training_step: 21500.... batch_loss: 0.053481.... 0.076500 sec/batch\n",
      "epoch: 9/50.... training_step: 21550.... batch_loss: 0.049729.... 0.074255 sec/batch\n",
      "epoch: 9/50.... training_step: 21600.... batch_loss: 0.051755.... 0.073889 sec/batch\n",
      "epoch: 9/50.... training_step: 21650.... batch_loss: 0.056131.... 0.074355 sec/batch\n",
      "epoch: 9/50.... training_step: 21700.... batch_loss: 0.064980.... 0.075503 sec/batch\n",
      "epoch: 9/50.... training_step: 21750.... batch_loss: 0.059407.... 0.075269 sec/batch\n",
      "epoch: 9/50.... training_step: 21800.... batch_loss: 0.051930.... 0.073764 sec/batch\n",
      "epoch: 9/50.... training_step: 21850.... batch_loss: 0.060630.... 0.079692 sec/batch\n",
      "epoch: 9/50.... training_step: 21900.... batch_loss: 0.053922.... 0.074152 sec/batch\n",
      "epoch: 9/50.... training_step: 21950.... batch_loss: 0.053105.... 0.075421 sec/batch\n",
      "epoch: 9/50.... training_step: 22000.... batch_loss: 0.060379.... 0.073522 sec/batch\n",
      "epoch: 9/50.... training_step: 22050.... batch_loss: 0.060444.... 0.076574 sec/batch\n",
      "epoch: 9/50.... training_step: 22100.... batch_loss: 0.050937.... 0.075723 sec/batch\n",
      "epoch: 9/50.... training_step: 22150.... batch_loss: 0.060970.... 0.074239 sec/batch\n",
      "epoch: 9/50.... training_step: 22200.... batch_loss: 0.054037.... 0.074019 sec/batch\n",
      "epoch: 9/50.... training_step: 22250.... batch_loss: 0.050628.... 0.073036 sec/batch\n",
      "epoch: 9/50.... training_step: 22300.... batch_loss: 0.056163.... 0.075130 sec/batch\n",
      "epoch: 9/50.... training_step: 22350.... batch_loss: 0.062581.... 0.078224 sec/batch\n",
      "epoch: 9/50.... training_step: 22400.... batch_loss: 0.055876.... 0.077390 sec/batch\n",
      "epoch: 9/50.... training_step: 22450.... batch_loss: 0.057345.... 0.074230 sec/batch\n",
      "epoch: 9/50.... training_step: 22500.... batch_loss: 0.053823.... 0.076640 sec/batch\n",
      "epoch: 9/50.... training_step: 22550.... batch_loss: 0.055401.... 0.077306 sec/batch\n",
      "epoch: 9/50.... training_step: 22600.... batch_loss: 0.046469.... 0.074208 sec/batch\n",
      "epoch: 9/50.... training_step: 22650.... batch_loss: 0.057265.... 0.075572 sec/batch\n",
      "epoch: 9/50.... training_step: 22700.... batch_loss: 0.051548.... 0.073906 sec/batch\n",
      "epoch: 9/50.... training_step: 22750.... batch_loss: 0.052279.... 0.075208 sec/batch\n",
      "epoch: 9/50.... training_step: 22800.... batch_loss: 0.049508.... 0.075353 sec/batch\n",
      "epoch: 9/50.... training_step: 22850.... batch_loss: 0.055333.... 0.077342 sec/batch\n",
      "epoch: 9/50.... training_step: 22900.... batch_loss: 0.049143.... 0.073875 sec/batch\n",
      "epoch: 9/50.... training_step: 22950.... batch_loss: 0.059632.... 0.077988 sec/batch\n",
      "epoch: 9/50.... training_step: 23000.... batch_loss: 0.050326.... 0.074341 sec/batch\n",
      "epoch: 9/50.... training_step: 23050.... batch_loss: 0.051264.... 0.076499 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9/50.... training_step: 23100.... batch_loss: 0.053131.... 0.076799 sec/batch\n",
      "epoch: 9/50.... training_step: 23150.... batch_loss: 0.059367.... 0.074184 sec/batch\n",
      "epoch: 9/50.... training_step: 23200.... batch_loss: 0.063061.... 0.073602 sec/batch\n",
      "epoch: 9/50.... training_step: 23250.... batch_loss: 0.054974.... 0.075846 sec/batch\n",
      "epoch: 9/50.... training_step: 23300.... batch_loss: 0.057833.... 0.075423 sec/batch\n",
      "epoch: 9/50.... training_step: 23350.... batch_loss: 0.053991.... 0.077295 sec/batch\n",
      "epoch: 9/50.... training_step: 23400.... batch_loss: 0.057663.... 0.078708 sec/batch\n",
      "epoch: 9/50.... training_step: 23450.... batch_loss: 0.058089.... 0.074901 sec/batch\n",
      "epoch: 9/50.... training_step: 23500.... batch_loss: 0.057647.... 0.075113 sec/batch\n",
      "epoch: 9/50.... training_step: 23550.... batch_loss: 0.051856.... 0.073139 sec/batch\n",
      "epoch: 10/50.... training_step: 23600.... batch_loss: 0.056726.... 0.076422 sec/batch\n",
      "epoch: 10/50.... training_step: 23650.... batch_loss: 0.052559.... 0.075851 sec/batch\n",
      "epoch: 10/50.... training_step: 23700.... batch_loss: 0.050008.... 0.075984 sec/batch\n",
      "epoch: 10/50.... training_step: 23750.... batch_loss: 0.045939.... 0.076169 sec/batch\n",
      "epoch: 10/50.... training_step: 23800.... batch_loss: 0.052230.... 0.077065 sec/batch\n",
      "epoch: 10/50.... training_step: 23850.... batch_loss: 0.051311.... 0.074622 sec/batch\n",
      "epoch: 10/50.... training_step: 23900.... batch_loss: 0.055469.... 0.073836 sec/batch\n",
      "epoch: 10/50.... training_step: 23950.... batch_loss: 0.056910.... 0.073773 sec/batch\n",
      "epoch: 10/50.... training_step: 24000.... batch_loss: 0.052131.... 0.075795 sec/batch\n",
      "epoch: 10/50.... training_step: 24050.... batch_loss: 0.058535.... 0.073599 sec/batch\n",
      "epoch: 10/50.... training_step: 24100.... batch_loss: 0.052436.... 0.075837 sec/batch\n",
      "epoch: 10/50.... training_step: 24150.... batch_loss: 0.056481.... 0.077637 sec/batch\n",
      "epoch: 10/50.... training_step: 24200.... batch_loss: 0.055321.... 0.073510 sec/batch\n",
      "epoch: 10/50.... training_step: 24250.... batch_loss: 0.052856.... 0.077904 sec/batch\n",
      "epoch: 10/50.... training_step: 24300.... batch_loss: 0.049796.... 0.075168 sec/batch\n",
      "epoch: 10/50.... training_step: 24350.... batch_loss: 0.053038.... 0.076465 sec/batch\n",
      "epoch: 10/50.... training_step: 24400.... batch_loss: 0.057896.... 0.074872 sec/batch\n",
      "epoch: 10/50.... training_step: 24450.... batch_loss: 0.049452.... 0.073858 sec/batch\n",
      "epoch: 10/50.... training_step: 24500.... batch_loss: 0.053436.... 0.075720 sec/batch\n",
      "epoch: 10/50.... training_step: 24550.... batch_loss: 0.057942.... 0.073614 sec/batch\n",
      "epoch: 10/50.... training_step: 24600.... batch_loss: 0.057186.... 0.076950 sec/batch\n",
      "epoch: 10/50.... training_step: 24650.... batch_loss: 0.052163.... 0.074158 sec/batch\n",
      "epoch: 10/50.... training_step: 24700.... batch_loss: 0.053319.... 0.074054 sec/batch\n",
      "epoch: 10/50.... training_step: 24750.... batch_loss: 0.048697.... 0.073479 sec/batch\n",
      "epoch: 10/50.... training_step: 24800.... batch_loss: 0.058857.... 0.077221 sec/batch\n",
      "epoch: 10/50.... training_step: 24850.... batch_loss: 0.055315.... 0.075768 sec/batch\n",
      "epoch: 10/50.... training_step: 24900.... batch_loss: 0.053075.... 0.074241 sec/batch\n",
      "epoch: 10/50.... training_step: 24950.... batch_loss: 0.053917.... 0.077395 sec/batch\n",
      "epoch: 10/50.... training_step: 25000.... batch_loss: 0.052012.... 0.076595 sec/batch\n",
      "epoch: 10/50.... training_step: 25050.... batch_loss: 0.049020.... 0.075728 sec/batch\n",
      "epoch: 10/50.... training_step: 25100.... batch_loss: 0.055583.... 0.073675 sec/batch\n",
      "epoch: 10/50.... training_step: 25150.... batch_loss: 0.054108.... 0.078314 sec/batch\n",
      "epoch: 10/50.... training_step: 25200.... batch_loss: 0.052949.... 0.073868 sec/batch\n",
      "epoch: 10/50.... training_step: 25250.... batch_loss: 0.055760.... 0.076919 sec/batch\n",
      "epoch: 10/50.... training_step: 25300.... batch_loss: 0.061774.... 0.073449 sec/batch\n",
      "epoch: 10/50.... training_step: 25350.... batch_loss: 0.053487.... 0.073617 sec/batch\n",
      "epoch: 10/50.... training_step: 25400.... batch_loss: 0.060559.... 0.076388 sec/batch\n",
      "epoch: 10/50.... training_step: 25450.... batch_loss: 0.052677.... 0.076136 sec/batch\n",
      "epoch: 10/50.... training_step: 25500.... batch_loss: 0.052111.... 0.075201 sec/batch\n",
      "epoch: 10/50.... training_step: 25550.... batch_loss: 0.061548.... 0.075825 sec/batch\n",
      "epoch: 10/50.... training_step: 25600.... batch_loss: 0.055770.... 0.076874 sec/batch\n",
      "epoch: 10/50.... training_step: 25650.... batch_loss: 0.058303.... 0.073871 sec/batch\n",
      "epoch: 10/50.... training_step: 25700.... batch_loss: 0.052786.... 0.077327 sec/batch\n",
      "epoch: 10/50.... training_step: 25750.... batch_loss: 0.053778.... 0.075547 sec/batch\n",
      "epoch: 10/50.... training_step: 25800.... batch_loss: 0.057894.... 0.075236 sec/batch\n",
      "epoch: 10/50.... training_step: 25850.... batch_loss: 0.054831.... 0.073252 sec/batch\n",
      "epoch: 10/50.... training_step: 25900.... batch_loss: 0.059191.... 0.075586 sec/batch\n",
      "epoch: 11/50.... training_step: 25950.... batch_loss: 0.054178.... 0.075105 sec/batch\n",
      "epoch: 11/50.... training_step: 26000.... batch_loss: 0.049962.... 0.076044 sec/batch\n",
      "epoch: 11/50.... training_step: 26050.... batch_loss: 0.052781.... 0.073221 sec/batch\n",
      "epoch: 11/50.... training_step: 26100.... batch_loss: 0.048401.... 0.073299 sec/batch\n",
      "epoch: 11/50.... training_step: 26150.... batch_loss: 0.061204.... 0.074919 sec/batch\n",
      "epoch: 11/50.... training_step: 26200.... batch_loss: 0.060478.... 0.074632 sec/batch\n",
      "epoch: 11/50.... training_step: 26250.... batch_loss: 0.054428.... 0.076639 sec/batch\n",
      "epoch: 11/50.... training_step: 26300.... batch_loss: 0.054439.... 0.073514 sec/batch\n",
      "epoch: 11/50.... training_step: 26350.... batch_loss: 0.056883.... 0.078458 sec/batch\n",
      "epoch: 11/50.... training_step: 26400.... batch_loss: 0.047045.... 0.073750 sec/batch\n",
      "epoch: 11/50.... training_step: 26450.... batch_loss: 0.050737.... 0.074597 sec/batch\n",
      "epoch: 11/50.... training_step: 26500.... batch_loss: 0.054943.... 0.074621 sec/batch\n",
      "epoch: 11/50.... training_step: 26550.... batch_loss: 0.059000.... 0.077068 sec/batch\n",
      "epoch: 11/50.... training_step: 26600.... batch_loss: 0.061472.... 0.074914 sec/batch\n",
      "epoch: 11/50.... training_step: 26650.... batch_loss: 0.057556.... 0.077409 sec/batch\n",
      "epoch: 11/50.... training_step: 26700.... batch_loss: 0.051482.... 0.073851 sec/batch\n",
      "epoch: 11/50.... training_step: 26750.... batch_loss: 0.054301.... 0.074181 sec/batch\n",
      "epoch: 11/50.... training_step: 26800.... batch_loss: 0.055533.... 0.074860 sec/batch\n",
      "epoch: 11/50.... training_step: 26850.... batch_loss: 0.059479.... 0.073745 sec/batch\n",
      "epoch: 11/50.... training_step: 26900.... batch_loss: 0.046226.... 0.073770 sec/batch\n",
      "epoch: 11/50.... training_step: 26950.... batch_loss: 0.052373.... 0.074101 sec/batch\n",
      "epoch: 11/50.... training_step: 27000.... batch_loss: 0.048025.... 0.076034 sec/batch\n",
      "epoch: 11/50.... training_step: 27050.... batch_loss: 0.048385.... 0.073507 sec/batch\n",
      "epoch: 11/50.... training_step: 27100.... batch_loss: 0.055381.... 0.076850 sec/batch\n",
      "epoch: 11/50.... training_step: 27150.... batch_loss: 0.055478.... 0.074674 sec/batch\n",
      "epoch: 11/50.... training_step: 27200.... batch_loss: 0.050519.... 0.077668 sec/batch\n",
      "epoch: 11/50.... training_step: 27250.... batch_loss: 0.051444.... 0.073647 sec/batch\n",
      "epoch: 11/50.... training_step: 27300.... batch_loss: 0.049700.... 0.077067 sec/batch\n",
      "epoch: 11/50.... training_step: 27350.... batch_loss: 0.060483.... 0.073484 sec/batch\n",
      "epoch: 11/50.... training_step: 27400.... batch_loss: 0.050112.... 0.074542 sec/batch\n",
      "epoch: 11/50.... training_step: 27450.... batch_loss: 0.051249.... 0.075985 sec/batch\n",
      "epoch: 11/50.... training_step: 27500.... batch_loss: 0.053039.... 0.073824 sec/batch\n",
      "epoch: 11/50.... training_step: 27550.... batch_loss: 0.049490.... 0.075837 sec/batch\n",
      "epoch: 11/50.... training_step: 27600.... batch_loss: 0.055371.... 0.073114 sec/batch\n",
      "epoch: 11/50.... training_step: 27650.... batch_loss: 0.059249.... 0.074424 sec/batch\n",
      "epoch: 11/50.... training_step: 27700.... batch_loss: 0.046294.... 0.074567 sec/batch\n",
      "epoch: 11/50.... training_step: 27750.... batch_loss: 0.055021.... 0.074824 sec/batch\n",
      "epoch: 11/50.... training_step: 27800.... batch_loss: 0.053651.... 0.073564 sec/batch\n",
      "epoch: 11/50.... training_step: 27850.... batch_loss: 0.053093.... 0.075430 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11/50.... training_step: 27900.... batch_loss: 0.053072.... 0.076343 sec/batch\n",
      "epoch: 11/50.... training_step: 27950.... batch_loss: 0.052832.... 0.073531 sec/batch\n",
      "epoch: 11/50.... training_step: 28000.... batch_loss: 0.051780.... 0.073854 sec/batch\n",
      "epoch: 11/50.... training_step: 28050.... batch_loss: 0.056813.... 0.074475 sec/batch\n",
      "epoch: 11/50.... training_step: 28100.... batch_loss: 0.054009.... 0.074986 sec/batch\n",
      "epoch: 11/50.... training_step: 28150.... batch_loss: 0.054478.... 0.076021 sec/batch\n",
      "epoch: 11/50.... training_step: 28200.... batch_loss: 0.052585.... 0.077026 sec/batch\n",
      "epoch: 11/50.... training_step: 28250.... batch_loss: 0.052516.... 0.078745 sec/batch\n",
      "epoch: 12/50.... training_step: 28300.... batch_loss: 0.054446.... 0.076972 sec/batch\n",
      "epoch: 12/50.... training_step: 28350.... batch_loss: 0.056771.... 0.076355 sec/batch\n",
      "epoch: 12/50.... training_step: 28400.... batch_loss: 0.058541.... 0.077018 sec/batch\n",
      "epoch: 12/50.... training_step: 28450.... batch_loss: 0.050540.... 0.075297 sec/batch\n",
      "epoch: 12/50.... training_step: 28500.... batch_loss: 0.052109.... 0.073524 sec/batch\n",
      "epoch: 12/50.... training_step: 28550.... batch_loss: 0.045124.... 0.076459 sec/batch\n",
      "epoch: 12/50.... training_step: 28600.... batch_loss: 0.064323.... 0.074112 sec/batch\n",
      "epoch: 12/50.... training_step: 28650.... batch_loss: 0.058329.... 0.076994 sec/batch\n",
      "epoch: 12/50.... training_step: 28700.... batch_loss: 0.054228.... 0.073887 sec/batch\n",
      "epoch: 12/50.... training_step: 28750.... batch_loss: 0.052307.... 0.076582 sec/batch\n",
      "epoch: 12/50.... training_step: 28800.... batch_loss: 0.051201.... 0.073837 sec/batch\n",
      "epoch: 12/50.... training_step: 28850.... batch_loss: 0.048506.... 0.075202 sec/batch\n",
      "epoch: 12/50.... training_step: 28900.... batch_loss: 0.066367.... 0.075629 sec/batch\n",
      "epoch: 12/50.... training_step: 28950.... batch_loss: 0.044006.... 0.078380 sec/batch\n",
      "epoch: 12/50.... training_step: 29000.... batch_loss: 0.050741.... 0.073946 sec/batch\n",
      "epoch: 12/50.... training_step: 29050.... batch_loss: 0.051246.... 0.089884 sec/batch\n",
      "epoch: 12/50.... training_step: 29100.... batch_loss: 0.053342.... 0.074690 sec/batch\n",
      "epoch: 12/50.... training_step: 29150.... batch_loss: 0.056797.... 0.074845 sec/batch\n",
      "epoch: 12/50.... training_step: 29200.... batch_loss: 0.054914.... 0.076606 sec/batch\n",
      "epoch: 12/50.... training_step: 29250.... batch_loss: 0.053938.... 0.073447 sec/batch\n",
      "epoch: 12/50.... training_step: 29300.... batch_loss: 0.050163.... 0.073802 sec/batch\n",
      "epoch: 12/50.... training_step: 29350.... batch_loss: 0.048350.... 0.074323 sec/batch\n",
      "epoch: 12/50.... training_step: 29400.... batch_loss: 0.049001.... 0.079521 sec/batch\n",
      "epoch: 12/50.... training_step: 29450.... batch_loss: 0.056517.... 0.073917 sec/batch\n",
      "epoch: 12/50.... training_step: 29500.... batch_loss: 0.052086.... 0.075323 sec/batch\n",
      "epoch: 12/50.... training_step: 29550.... batch_loss: 0.048852.... 0.073103 sec/batch\n",
      "epoch: 12/50.... training_step: 29600.... batch_loss: 0.047540.... 0.076861 sec/batch\n",
      "epoch: 12/50.... training_step: 29650.... batch_loss: 0.048969.... 0.073066 sec/batch\n",
      "epoch: 12/50.... training_step: 29700.... batch_loss: 0.051843.... 0.077178 sec/batch\n",
      "epoch: 12/50.... training_step: 29750.... batch_loss: 0.059663.... 0.074134 sec/batch\n",
      "epoch: 12/50.... training_step: 29800.... batch_loss: 0.043768.... 0.078236 sec/batch\n",
      "epoch: 12/50.... training_step: 29850.... batch_loss: 0.046673.... 0.075439 sec/batch\n",
      "epoch: 12/50.... training_step: 29900.... batch_loss: 0.051750.... 0.074167 sec/batch\n",
      "epoch: 12/50.... training_step: 29950.... batch_loss: 0.052436.... 0.073484 sec/batch\n",
      "epoch: 12/50.... training_step: 30000.... batch_loss: 0.056991.... 0.076700 sec/batch\n",
      "epoch: 12/50.... training_step: 30050.... batch_loss: 0.052871.... 0.073876 sec/batch\n",
      "epoch: 12/50.... training_step: 30100.... batch_loss: 0.054869.... 0.077526 sec/batch\n",
      "epoch: 12/50.... training_step: 30150.... batch_loss: 0.052839.... 0.076721 sec/batch\n",
      "epoch: 12/50.... training_step: 30200.... batch_loss: 0.052160.... 0.077291 sec/batch\n",
      "epoch: 12/50.... training_step: 30250.... batch_loss: 0.048517.... 0.073268 sec/batch\n",
      "epoch: 12/50.... training_step: 30300.... batch_loss: 0.049538.... 0.075335 sec/batch\n",
      "epoch: 12/50.... training_step: 30350.... batch_loss: 0.050075.... 0.074157 sec/batch\n",
      "epoch: 12/50.... training_step: 30400.... batch_loss: 0.046056.... 0.074003 sec/batch\n",
      "epoch: 12/50.... training_step: 30450.... batch_loss: 0.046250.... 0.075740 sec/batch\n",
      "epoch: 12/50.... training_step: 30500.... batch_loss: 0.056560.... 0.076246 sec/batch\n",
      "epoch: 12/50.... training_step: 30550.... batch_loss: 0.054393.... 0.074562 sec/batch\n",
      "epoch: 12/50.... training_step: 30600.... batch_loss: 0.053282.... 0.077156 sec/batch\n",
      "epoch: 13/50.... training_step: 30650.... batch_loss: 0.055046.... 0.075555 sec/batch\n",
      "epoch: 13/50.... training_step: 30700.... batch_loss: 0.055376.... 0.073429 sec/batch\n",
      "epoch: 13/50.... training_step: 30750.... batch_loss: 0.049107.... 0.074417 sec/batch\n",
      "epoch: 13/50.... training_step: 30800.... batch_loss: 0.055091.... 0.074785 sec/batch\n",
      "epoch: 13/50.... training_step: 30850.... batch_loss: 0.053718.... 0.074957 sec/batch\n",
      "epoch: 13/50.... training_step: 30900.... batch_loss: 0.049055.... 0.074473 sec/batch\n",
      "epoch: 13/50.... training_step: 30950.... batch_loss: 0.050218.... 0.075855 sec/batch\n",
      "epoch: 13/50.... training_step: 31000.... batch_loss: 0.051276.... 0.074801 sec/batch\n",
      "epoch: 13/50.... training_step: 31050.... batch_loss: 0.057642.... 0.074274 sec/batch\n",
      "epoch: 13/50.... training_step: 31100.... batch_loss: 0.055655.... 0.073783 sec/batch\n",
      "epoch: 13/50.... training_step: 31150.... batch_loss: 0.052984.... 0.073769 sec/batch\n",
      "epoch: 13/50.... training_step: 31200.... batch_loss: 0.050769.... 0.074510 sec/batch\n",
      "epoch: 13/50.... training_step: 31250.... batch_loss: 0.057283.... 0.076392 sec/batch\n",
      "epoch: 13/50.... training_step: 31300.... batch_loss: 0.061809.... 0.077882 sec/batch\n",
      "epoch: 13/50.... training_step: 31350.... batch_loss: 0.054606.... 0.074150 sec/batch\n",
      "epoch: 13/50.... training_step: 31400.... batch_loss: 0.050647.... 0.081832 sec/batch\n",
      "epoch: 13/50.... training_step: 31450.... batch_loss: 0.048584.... 0.074618 sec/batch\n",
      "epoch: 13/50.... training_step: 31500.... batch_loss: 0.046530.... 0.076290 sec/batch\n",
      "epoch: 13/50.... training_step: 31550.... batch_loss: 0.055516.... 0.076686 sec/batch\n",
      "epoch: 13/50.... training_step: 31600.... batch_loss: 0.047737.... 0.075066 sec/batch\n",
      "epoch: 13/50.... training_step: 31650.... batch_loss: 0.049455.... 0.077847 sec/batch\n",
      "epoch: 13/50.... training_step: 31700.... batch_loss: 0.046159.... 0.073531 sec/batch\n",
      "epoch: 13/50.... training_step: 31750.... batch_loss: 0.050695.... 0.078651 sec/batch\n",
      "epoch: 13/50.... training_step: 31800.... batch_loss: 0.053344.... 0.074519 sec/batch\n",
      "epoch: 13/50.... training_step: 31850.... batch_loss: 0.051863.... 0.075418 sec/batch\n",
      "epoch: 13/50.... training_step: 31900.... batch_loss: 0.042493.... 0.078894 sec/batch\n",
      "epoch: 13/50.... training_step: 31950.... batch_loss: 0.052556.... 0.074223 sec/batch\n",
      "epoch: 13/50.... training_step: 32000.... batch_loss: 0.051985.... 0.074603 sec/batch\n",
      "epoch: 13/50.... training_step: 32050.... batch_loss: 0.058501.... 0.074152 sec/batch\n",
      "epoch: 13/50.... training_step: 32100.... batch_loss: 0.050247.... 0.077215 sec/batch\n",
      "epoch: 13/50.... training_step: 32150.... batch_loss: 0.053561.... 0.073668 sec/batch\n",
      "epoch: 13/50.... training_step: 32200.... batch_loss: 0.052387.... 0.074513 sec/batch\n",
      "epoch: 13/50.... training_step: 32250.... batch_loss: 0.049307.... 0.074308 sec/batch\n",
      "epoch: 13/50.... training_step: 32300.... batch_loss: 0.050767.... 0.077699 sec/batch\n",
      "epoch: 13/50.... training_step: 32350.... batch_loss: 0.051071.... 0.074836 sec/batch\n",
      "epoch: 13/50.... training_step: 32400.... batch_loss: 0.048049.... 0.076104 sec/batch\n",
      "epoch: 13/50.... training_step: 32450.... batch_loss: 0.060291.... 0.074355 sec/batch\n",
      "epoch: 13/50.... training_step: 32500.... batch_loss: 0.054942.... 0.074966 sec/batch\n",
      "epoch: 13/50.... training_step: 32550.... batch_loss: 0.051995.... 0.077073 sec/batch\n",
      "epoch: 13/50.... training_step: 32600.... batch_loss: 0.053049.... 0.077599 sec/batch\n",
      "epoch: 13/50.... training_step: 32650.... batch_loss: 0.051559.... 0.078396 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13/50.... training_step: 32700.... batch_loss: 0.055825.... 0.075843 sec/batch\n",
      "epoch: 13/50.... training_step: 32750.... batch_loss: 0.056830.... 0.075541 sec/batch\n",
      "epoch: 13/50.... training_step: 32800.... batch_loss: 0.051382.... 0.073945 sec/batch\n",
      "epoch: 13/50.... training_step: 32850.... batch_loss: 0.051508.... 0.076325 sec/batch\n",
      "epoch: 13/50.... training_step: 32900.... batch_loss: 0.051778.... 0.075202 sec/batch\n",
      "epoch: 13/50.... training_step: 32950.... batch_loss: 0.051079.... 0.076000 sec/batch\n",
      "epoch: 14/50.... training_step: 33000.... batch_loss: 0.052981.... 0.075644 sec/batch\n",
      "epoch: 14/50.... training_step: 33050.... batch_loss: 0.051944.... 0.076714 sec/batch\n",
      "epoch: 14/50.... training_step: 33100.... batch_loss: 0.045841.... 0.076217 sec/batch\n",
      "epoch: 14/50.... training_step: 33150.... batch_loss: 0.051449.... 0.074241 sec/batch\n",
      "epoch: 14/50.... training_step: 33200.... batch_loss: 0.054431.... 0.073940 sec/batch\n",
      "epoch: 14/50.... training_step: 33250.... batch_loss: 0.053733.... 0.074116 sec/batch\n",
      "epoch: 14/50.... training_step: 33300.... batch_loss: 0.052615.... 0.073885 sec/batch\n",
      "epoch: 14/50.... training_step: 33350.... batch_loss: 0.048658.... 0.074714 sec/batch\n",
      "epoch: 14/50.... training_step: 33400.... batch_loss: 0.050211.... 0.073943 sec/batch\n",
      "epoch: 14/50.... training_step: 33450.... batch_loss: 0.050399.... 0.073821 sec/batch\n",
      "epoch: 14/50.... training_step: 33500.... batch_loss: 0.051311.... 0.078088 sec/batch\n",
      "epoch: 14/50.... training_step: 33550.... batch_loss: 0.056136.... 0.073330 sec/batch\n",
      "epoch: 14/50.... training_step: 33600.... batch_loss: 0.055941.... 0.074205 sec/batch\n",
      "epoch: 14/50.... training_step: 33650.... batch_loss: 0.052732.... 0.074179 sec/batch\n",
      "epoch: 14/50.... training_step: 33700.... batch_loss: 0.048316.... 0.076775 sec/batch\n",
      "epoch: 14/50.... training_step: 33750.... batch_loss: 0.049951.... 0.075017 sec/batch\n",
      "epoch: 14/50.... training_step: 33800.... batch_loss: 0.049791.... 0.077324 sec/batch\n",
      "epoch: 14/50.... training_step: 33850.... batch_loss: 0.048695.... 0.072590 sec/batch\n",
      "epoch: 14/50.... training_step: 33900.... batch_loss: 0.054808.... 0.074108 sec/batch\n",
      "epoch: 14/50.... training_step: 33950.... batch_loss: 0.052957.... 0.075989 sec/batch\n",
      "epoch: 14/50.... training_step: 34000.... batch_loss: 0.047603.... 0.077243 sec/batch\n",
      "epoch: 14/50.... training_step: 34050.... batch_loss: 0.053607.... 0.073850 sec/batch\n",
      "epoch: 14/50.... training_step: 34100.... batch_loss: 0.049300.... 0.074704 sec/batch\n",
      "epoch: 14/50.... training_step: 34150.... batch_loss: 0.048394.... 0.074783 sec/batch\n",
      "epoch: 14/50.... training_step: 34200.... batch_loss: 0.048320.... 0.073905 sec/batch\n",
      "epoch: 14/50.... training_step: 34250.... batch_loss: 0.049535.... 0.074327 sec/batch\n",
      "epoch: 14/50.... training_step: 34300.... batch_loss: 0.049874.... 0.075554 sec/batch\n",
      "epoch: 14/50.... training_step: 34350.... batch_loss: 0.048277.... 0.074754 sec/batch\n",
      "epoch: 14/50.... training_step: 34400.... batch_loss: 0.053214.... 0.073191 sec/batch\n",
      "epoch: 14/50.... training_step: 34450.... batch_loss: 0.056065.... 0.079748 sec/batch\n",
      "epoch: 14/50.... training_step: 34500.... batch_loss: 0.063416.... 0.078064 sec/batch\n",
      "epoch: 14/50.... training_step: 34550.... batch_loss: 0.050522.... 0.075440 sec/batch\n",
      "epoch: 14/50.... training_step: 34600.... batch_loss: 0.049804.... 0.076566 sec/batch\n",
      "epoch: 14/50.... training_step: 34650.... batch_loss: 0.043702.... 0.074051 sec/batch\n",
      "epoch: 14/50.... training_step: 34700.... batch_loss: 0.054967.... 0.073715 sec/batch\n",
      "epoch: 14/50.... training_step: 34750.... batch_loss: 0.053419.... 0.080657 sec/batch\n",
      "epoch: 14/50.... training_step: 34800.... batch_loss: 0.050419.... 0.072434 sec/batch\n",
      "epoch: 14/50.... training_step: 34850.... batch_loss: 0.051421.... 0.076965 sec/batch\n",
      "epoch: 14/50.... training_step: 34900.... batch_loss: 0.048932.... 0.074076 sec/batch\n",
      "epoch: 14/50.... training_step: 34950.... batch_loss: 0.059304.... 0.073045 sec/batch\n",
      "epoch: 14/50.... training_step: 35000.... batch_loss: 0.047690.... 0.077890 sec/batch\n",
      "epoch: 14/50.... training_step: 35050.... batch_loss: 0.051462.... 0.073920 sec/batch\n",
      "epoch: 14/50.... training_step: 35100.... batch_loss: 0.046395.... 0.074005 sec/batch\n",
      "epoch: 14/50.... training_step: 35150.... batch_loss: 0.047057.... 0.077229 sec/batch\n",
      "epoch: 14/50.... training_step: 35200.... batch_loss: 0.050044.... 0.079822 sec/batch\n",
      "epoch: 14/50.... training_step: 35250.... batch_loss: 0.053011.... 0.075013 sec/batch\n",
      "epoch: 14/50.... training_step: 35300.... batch_loss: 0.049900.... 0.076575 sec/batch\n",
      "epoch: 15/50.... training_step: 35350.... batch_loss: 0.054011.... 0.074482 sec/batch\n",
      "epoch: 15/50.... training_step: 35400.... batch_loss: 0.045045.... 0.079579 sec/batch\n",
      "epoch: 15/50.... training_step: 35450.... batch_loss: 0.050460.... 0.073214 sec/batch\n",
      "epoch: 15/50.... training_step: 35500.... batch_loss: 0.044708.... 0.078041 sec/batch\n",
      "epoch: 15/50.... training_step: 35550.... batch_loss: 0.042866.... 0.076922 sec/batch\n",
      "epoch: 15/50.... training_step: 35600.... batch_loss: 0.049332.... 0.074161 sec/batch\n",
      "epoch: 15/50.... training_step: 35650.... batch_loss: 0.045685.... 0.079041 sec/batch\n",
      "epoch: 15/50.... training_step: 35700.... batch_loss: 0.051251.... 0.073641 sec/batch\n",
      "epoch: 15/50.... training_step: 35750.... batch_loss: 0.053541.... 0.078687 sec/batch\n",
      "epoch: 15/50.... training_step: 35800.... batch_loss: 0.054650.... 0.073545 sec/batch\n",
      "epoch: 15/50.... training_step: 35850.... batch_loss: 0.053215.... 0.076182 sec/batch\n",
      "epoch: 15/50.... training_step: 35900.... batch_loss: 0.054638.... 0.073874 sec/batch\n",
      "epoch: 15/50.... training_step: 35950.... batch_loss: 0.047084.... 0.073638 sec/batch\n",
      "epoch: 15/50.... training_step: 36000.... batch_loss: 0.049744.... 0.073729 sec/batch\n",
      "epoch: 15/50.... training_step: 36050.... batch_loss: 0.042206.... 0.078666 sec/batch\n",
      "epoch: 15/50.... training_step: 36100.... batch_loss: 0.049837.... 0.074566 sec/batch\n",
      "epoch: 15/50.... training_step: 36150.... batch_loss: 0.047845.... 0.074596 sec/batch\n",
      "epoch: 15/50.... training_step: 36200.... batch_loss: 0.055604.... 0.073868 sec/batch\n",
      "epoch: 15/50.... training_step: 36250.... batch_loss: 0.047188.... 0.078107 sec/batch\n",
      "epoch: 15/50.... training_step: 36300.... batch_loss: 0.049412.... 0.074727 sec/batch\n",
      "epoch: 15/50.... training_step: 36350.... batch_loss: 0.048245.... 0.073599 sec/batch\n",
      "epoch: 15/50.... training_step: 36400.... batch_loss: 0.047448.... 0.073612 sec/batch\n",
      "epoch: 15/50.... training_step: 36450.... batch_loss: 0.046276.... 0.076586 sec/batch\n",
      "epoch: 15/50.... training_step: 36500.... batch_loss: 0.049801.... 0.073537 sec/batch\n",
      "epoch: 15/50.... training_step: 36550.... batch_loss: 0.056768.... 0.075804 sec/batch\n",
      "epoch: 15/50.... training_step: 36600.... batch_loss: 0.040069.... 0.077139 sec/batch\n",
      "epoch: 15/50.... training_step: 36650.... batch_loss: 0.044080.... 0.075221 sec/batch\n",
      "epoch: 15/50.... training_step: 36700.... batch_loss: 0.042742.... 0.076454 sec/batch\n",
      "epoch: 15/50.... training_step: 36750.... batch_loss: 0.055520.... 0.075134 sec/batch\n",
      "epoch: 15/50.... training_step: 36800.... batch_loss: 0.045067.... 0.074315 sec/batch\n",
      "epoch: 15/50.... training_step: 36850.... batch_loss: 0.045350.... 0.073809 sec/batch\n",
      "epoch: 15/50.... training_step: 36900.... batch_loss: 0.054641.... 0.073532 sec/batch\n",
      "epoch: 15/50.... training_step: 36950.... batch_loss: 0.043275.... 0.073322 sec/batch\n",
      "epoch: 15/50.... training_step: 37000.... batch_loss: 0.047906.... 0.074839 sec/batch\n",
      "epoch: 15/50.... training_step: 37050.... batch_loss: 0.050071.... 0.076438 sec/batch\n",
      "epoch: 15/50.... training_step: 37100.... batch_loss: 0.045729.... 0.074734 sec/batch\n",
      "epoch: 15/50.... training_step: 37150.... batch_loss: 0.042149.... 0.077035 sec/batch\n",
      "epoch: 15/50.... training_step: 37200.... batch_loss: 0.051716.... 0.074666 sec/batch\n",
      "epoch: 15/50.... training_step: 37250.... batch_loss: 0.049157.... 0.077072 sec/batch\n",
      "epoch: 15/50.... training_step: 37300.... batch_loss: 0.043571.... 0.077123 sec/batch\n",
      "epoch: 15/50.... training_step: 37350.... batch_loss: 0.050243.... 0.075948 sec/batch\n",
      "epoch: 15/50.... training_step: 37400.... batch_loss: 0.052098.... 0.074426 sec/batch\n",
      "epoch: 15/50.... training_step: 37450.... batch_loss: 0.038684.... 0.074218 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15/50.... training_step: 37500.... batch_loss: 0.045695.... 0.075579 sec/batch\n",
      "epoch: 15/50.... training_step: 37550.... batch_loss: 0.050769.... 0.073716 sec/batch\n",
      "epoch: 15/50.... training_step: 37600.... batch_loss: 0.054598.... 0.076545 sec/batch\n",
      "epoch: 15/50.... training_step: 37650.... batch_loss: 0.045414.... 0.075801 sec/batch\n",
      "epoch: 16/50.... training_step: 37700.... batch_loss: 0.047895.... 0.076913 sec/batch\n",
      "epoch: 16/50.... training_step: 37750.... batch_loss: 0.050188.... 0.078056 sec/batch\n",
      "epoch: 16/50.... training_step: 37800.... batch_loss: 0.046622.... 0.074990 sec/batch\n",
      "epoch: 16/50.... training_step: 37850.... batch_loss: 0.047774.... 0.077009 sec/batch\n",
      "epoch: 16/50.... training_step: 37900.... batch_loss: 0.053543.... 0.075295 sec/batch\n",
      "epoch: 16/50.... training_step: 37950.... batch_loss: 0.053468.... 0.078966 sec/batch\n",
      "epoch: 16/50.... training_step: 38000.... batch_loss: 0.053995.... 0.079854 sec/batch\n",
      "epoch: 16/50.... training_step: 38050.... batch_loss: 0.047412.... 0.073854 sec/batch\n",
      "epoch: 16/50.... training_step: 38100.... batch_loss: 0.046603.... 0.078509 sec/batch\n",
      "epoch: 16/50.... training_step: 38150.... batch_loss: 0.045775.... 0.075693 sec/batch\n",
      "epoch: 16/50.... training_step: 38200.... batch_loss: 0.055945.... 0.077986 sec/batch\n",
      "epoch: 16/50.... training_step: 38250.... batch_loss: 0.049063.... 0.073914 sec/batch\n",
      "epoch: 16/50.... training_step: 38300.... batch_loss: 0.053465.... 0.076068 sec/batch\n",
      "epoch: 16/50.... training_step: 38350.... batch_loss: 0.053186.... 0.076651 sec/batch\n",
      "epoch: 16/50.... training_step: 38400.... batch_loss: 0.046772.... 0.074418 sec/batch\n",
      "epoch: 16/50.... training_step: 38450.... batch_loss: 0.048867.... 0.077976 sec/batch\n",
      "epoch: 16/50.... training_step: 38500.... batch_loss: 0.047200.... 0.074942 sec/batch\n",
      "epoch: 16/50.... training_step: 38550.... batch_loss: 0.045975.... 0.078422 sec/batch\n",
      "epoch: 16/50.... training_step: 38600.... batch_loss: 0.048202.... 0.075025 sec/batch\n",
      "epoch: 16/50.... training_step: 38650.... batch_loss: 0.050680.... 0.076287 sec/batch\n",
      "epoch: 16/50.... training_step: 38700.... batch_loss: 0.051659.... 0.076795 sec/batch\n",
      "epoch: 16/50.... training_step: 38750.... batch_loss: 0.049832.... 0.073935 sec/batch\n",
      "epoch: 16/50.... training_step: 38800.... batch_loss: 0.046654.... 0.075488 sec/batch\n",
      "epoch: 16/50.... training_step: 38850.... batch_loss: 0.053675.... 0.080133 sec/batch\n",
      "epoch: 16/50.... training_step: 38900.... batch_loss: 0.050587.... 0.078298 sec/batch\n",
      "epoch: 16/50.... training_step: 38950.... batch_loss: 0.043940.... 0.076688 sec/batch\n",
      "epoch: 16/50.... training_step: 39000.... batch_loss: 0.046637.... 0.075407 sec/batch\n",
      "epoch: 16/50.... training_step: 39050.... batch_loss: 0.045608.... 0.076249 sec/batch\n",
      "epoch: 16/50.... training_step: 39100.... batch_loss: 0.052998.... 0.073841 sec/batch\n",
      "epoch: 16/50.... training_step: 39150.... batch_loss: 0.049345.... 0.074492 sec/batch\n",
      "epoch: 16/50.... training_step: 39200.... batch_loss: 0.050461.... 0.073036 sec/batch\n",
      "epoch: 16/50.... training_step: 39250.... batch_loss: 0.054936.... 0.074416 sec/batch\n",
      "epoch: 16/50.... training_step: 39300.... batch_loss: 0.051269.... 0.074181 sec/batch\n",
      "epoch: 16/50.... training_step: 39350.... batch_loss: 0.050405.... 0.074662 sec/batch\n",
      "epoch: 16/50.... training_step: 39400.... batch_loss: 0.061262.... 0.074032 sec/batch\n",
      "epoch: 16/50.... training_step: 39450.... batch_loss: 0.051813.... 0.074084 sec/batch\n",
      "epoch: 16/50.... training_step: 39500.... batch_loss: 0.057443.... 0.076061 sec/batch\n",
      "epoch: 16/50.... training_step: 39550.... batch_loss: 0.048608.... 0.075153 sec/batch\n",
      "epoch: 16/50.... training_step: 39600.... batch_loss: 0.054793.... 0.072996 sec/batch\n",
      "epoch: 16/50.... training_step: 39650.... batch_loss: 0.046935.... 0.073905 sec/batch\n",
      "epoch: 16/50.... training_step: 39700.... batch_loss: 0.046722.... 0.075428 sec/batch\n",
      "epoch: 16/50.... training_step: 39750.... batch_loss: 0.044447.... 0.083914 sec/batch\n",
      "epoch: 16/50.... training_step: 39800.... batch_loss: 0.052579.... 0.074620 sec/batch\n",
      "epoch: 16/50.... training_step: 39850.... batch_loss: 0.047966.... 0.075456 sec/batch\n",
      "epoch: 16/50.... training_step: 39900.... batch_loss: 0.049181.... 0.074951 sec/batch\n",
      "epoch: 16/50.... training_step: 39950.... batch_loss: 0.052086.... 0.076035 sec/batch\n",
      "epoch: 16/50.... training_step: 40000.... batch_loss: 0.050271.... 0.077522 sec/batch\n",
      "epoch: 16/50.... training_step: 40050.... batch_loss: 0.050465.... 0.074503 sec/batch\n",
      "epoch: 17/50.... training_step: 40100.... batch_loss: 0.051504.... 0.075533 sec/batch\n",
      "epoch: 17/50.... training_step: 40150.... batch_loss: 0.050992.... 0.074378 sec/batch\n",
      "epoch: 17/50.... training_step: 40200.... batch_loss: 0.043499.... 0.075056 sec/batch\n",
      "epoch: 17/50.... training_step: 40250.... batch_loss: 0.052747.... 0.077749 sec/batch\n",
      "epoch: 17/50.... training_step: 40300.... batch_loss: 0.042627.... 0.073896 sec/batch\n",
      "epoch: 17/50.... training_step: 40350.... batch_loss: 0.045986.... 0.075182 sec/batch\n",
      "epoch: 17/50.... training_step: 40400.... batch_loss: 0.048394.... 0.074482 sec/batch\n",
      "epoch: 17/50.... training_step: 40450.... batch_loss: 0.041917.... 0.075399 sec/batch\n",
      "epoch: 17/50.... training_step: 40500.... batch_loss: 0.048815.... 0.079006 sec/batch\n",
      "epoch: 17/50.... training_step: 40550.... batch_loss: 0.051920.... 0.074418 sec/batch\n",
      "epoch: 17/50.... training_step: 40600.... batch_loss: 0.048168.... 0.074726 sec/batch\n",
      "epoch: 17/50.... training_step: 40650.... batch_loss: 0.051336.... 0.073502 sec/batch\n",
      "epoch: 17/50.... training_step: 40700.... batch_loss: 0.049286.... 0.075789 sec/batch\n",
      "epoch: 17/50.... training_step: 40750.... batch_loss: 0.047089.... 0.077703 sec/batch\n",
      "epoch: 17/50.... training_step: 40800.... batch_loss: 0.053863.... 0.074758 sec/batch\n",
      "epoch: 17/50.... training_step: 40850.... batch_loss: 0.042139.... 0.075821 sec/batch\n",
      "epoch: 17/50.... training_step: 40900.... batch_loss: 0.046865.... 0.074216 sec/batch\n",
      "epoch: 17/50.... training_step: 40950.... batch_loss: 0.048516.... 0.074241 sec/batch\n",
      "epoch: 17/50.... training_step: 41000.... batch_loss: 0.049056.... 0.076157 sec/batch\n",
      "epoch: 17/50.... training_step: 41050.... batch_loss: 0.051979.... 0.073992 sec/batch\n",
      "epoch: 17/50.... training_step: 41100.... batch_loss: 0.053483.... 0.076577 sec/batch\n",
      "epoch: 17/50.... training_step: 41150.... batch_loss: 0.051523.... 0.074403 sec/batch\n",
      "epoch: 17/50.... training_step: 41200.... batch_loss: 0.051800.... 0.075580 sec/batch\n",
      "epoch: 17/50.... training_step: 41250.... batch_loss: 0.054368.... 0.075667 sec/batch\n",
      "epoch: 17/50.... training_step: 41300.... batch_loss: 0.054170.... 0.076231 sec/batch\n",
      "epoch: 17/50.... training_step: 41350.... batch_loss: 0.048988.... 0.075334 sec/batch\n",
      "epoch: 17/50.... training_step: 41400.... batch_loss: 0.046336.... 0.074522 sec/batch\n",
      "epoch: 17/50.... training_step: 41450.... batch_loss: 0.045362.... 0.076661 sec/batch\n",
      "epoch: 17/50.... training_step: 41500.... batch_loss: 0.047657.... 0.074808 sec/batch\n",
      "epoch: 17/50.... training_step: 41550.... batch_loss: 0.045160.... 0.074413 sec/batch\n",
      "epoch: 17/50.... training_step: 41600.... batch_loss: 0.045718.... 0.076021 sec/batch\n",
      "epoch: 17/50.... training_step: 41650.... batch_loss: 0.045233.... 0.074237 sec/batch\n",
      "epoch: 17/50.... training_step: 41700.... batch_loss: 0.056205.... 0.076113 sec/batch\n",
      "epoch: 17/50.... training_step: 41750.... batch_loss: 0.050590.... 0.074414 sec/batch\n",
      "epoch: 17/50.... training_step: 41800.... batch_loss: 0.053333.... 0.075116 sec/batch\n",
      "epoch: 17/50.... training_step: 41850.... batch_loss: 0.046675.... 0.075957 sec/batch\n",
      "epoch: 17/50.... training_step: 41900.... batch_loss: 0.045415.... 0.074964 sec/batch\n",
      "epoch: 17/50.... training_step: 41950.... batch_loss: 0.049928.... 0.077476 sec/batch\n",
      "epoch: 17/50.... training_step: 42000.... batch_loss: 0.051868.... 0.075675 sec/batch\n",
      "epoch: 17/50.... training_step: 42050.... batch_loss: 0.045592.... 0.073754 sec/batch\n",
      "epoch: 17/50.... training_step: 42100.... batch_loss: 0.056105.... 0.075325 sec/batch\n",
      "epoch: 17/50.... training_step: 42150.... batch_loss: 0.047776.... 0.074475 sec/batch\n",
      "epoch: 17/50.... training_step: 42200.... batch_loss: 0.041382.... 0.076132 sec/batch\n",
      "epoch: 17/50.... training_step: 42250.... batch_loss: 0.045552.... 0.073487 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17/50.... training_step: 42300.... batch_loss: 0.052972.... 0.074223 sec/batch\n",
      "epoch: 17/50.... training_step: 42350.... batch_loss: 0.044324.... 0.073476 sec/batch\n",
      "epoch: 17/50.... training_step: 42400.... batch_loss: 0.048669.... 0.073989 sec/batch\n",
      "epoch: 18/50.... training_step: 42450.... batch_loss: 0.050336.... 0.075603 sec/batch\n",
      "epoch: 18/50.... training_step: 42500.... batch_loss: 0.047560.... 0.075120 sec/batch\n",
      "epoch: 18/50.... training_step: 42550.... batch_loss: 0.044757.... 0.075173 sec/batch\n",
      "epoch: 18/50.... training_step: 42600.... batch_loss: 0.049029.... 0.075952 sec/batch\n",
      "epoch: 18/50.... training_step: 42650.... batch_loss: 0.053826.... 0.074615 sec/batch\n",
      "epoch: 18/50.... training_step: 42700.... batch_loss: 0.050320.... 0.074536 sec/batch\n",
      "epoch: 18/50.... training_step: 42750.... batch_loss: 0.044905.... 0.072947 sec/batch\n",
      "epoch: 18/50.... training_step: 42800.... batch_loss: 0.046445.... 0.073407 sec/batch\n",
      "epoch: 18/50.... training_step: 42850.... batch_loss: 0.045254.... 0.074562 sec/batch\n",
      "epoch: 18/50.... training_step: 42900.... batch_loss: 0.049179.... 0.074783 sec/batch\n",
      "epoch: 18/50.... training_step: 42950.... batch_loss: 0.048161.... 0.075048 sec/batch\n",
      "epoch: 18/50.... training_step: 43000.... batch_loss: 0.052410.... 0.074523 sec/batch\n",
      "epoch: 18/50.... training_step: 43050.... batch_loss: 0.055349.... 0.073665 sec/batch\n",
      "epoch: 18/50.... training_step: 43100.... batch_loss: 0.053079.... 0.075446 sec/batch\n",
      "epoch: 18/50.... training_step: 43150.... batch_loss: 0.050422.... 0.076173 sec/batch\n",
      "epoch: 18/50.... training_step: 43200.... batch_loss: 0.048466.... 0.074182 sec/batch\n",
      "epoch: 18/50.... training_step: 43250.... batch_loss: 0.047074.... 0.074718 sec/batch\n",
      "epoch: 18/50.... training_step: 43300.... batch_loss: 0.049815.... 0.074696 sec/batch\n",
      "epoch: 18/50.... training_step: 43350.... batch_loss: 0.045017.... 0.075902 sec/batch\n",
      "epoch: 18/50.... training_step: 43400.... batch_loss: 0.045365.... 0.077915 sec/batch\n",
      "epoch: 18/50.... training_step: 43450.... batch_loss: 0.046931.... 0.073906 sec/batch\n",
      "epoch: 18/50.... training_step: 43500.... batch_loss: 0.049180.... 0.076178 sec/batch\n",
      "epoch: 18/50.... training_step: 43550.... batch_loss: 0.042878.... 0.073611 sec/batch\n",
      "epoch: 18/50.... training_step: 43600.... batch_loss: 0.052046.... 0.075438 sec/batch\n",
      "epoch: 18/50.... training_step: 43650.... batch_loss: 0.048906.... 0.076730 sec/batch\n",
      "epoch: 18/50.... training_step: 43700.... batch_loss: 0.046884.... 0.075710 sec/batch\n",
      "epoch: 18/50.... training_step: 43750.... batch_loss: 0.048351.... 0.073846 sec/batch\n",
      "epoch: 18/50.... training_step: 43800.... batch_loss: 0.050382.... 0.078568 sec/batch\n",
      "epoch: 18/50.... training_step: 43850.... batch_loss: 0.043821.... 0.075580 sec/batch\n",
      "epoch: 18/50.... training_step: 43900.... batch_loss: 0.042718.... 0.077509 sec/batch\n",
      "epoch: 18/50.... training_step: 43950.... batch_loss: 0.046609.... 0.075160 sec/batch\n",
      "epoch: 18/50.... training_step: 44000.... batch_loss: 0.049034.... 0.076027 sec/batch\n",
      "epoch: 18/50.... training_step: 44050.... batch_loss: 0.046030.... 0.074231 sec/batch\n",
      "epoch: 18/50.... training_step: 44100.... batch_loss: 0.045925.... 0.074493 sec/batch\n",
      "epoch: 18/50.... training_step: 44150.... batch_loss: 0.055348.... 0.077442 sec/batch\n",
      "epoch: 18/50.... training_step: 44200.... batch_loss: 0.040724.... 0.076538 sec/batch\n",
      "epoch: 18/50.... training_step: 44250.... batch_loss: 0.041889.... 0.075291 sec/batch\n",
      "epoch: 18/50.... training_step: 44300.... batch_loss: 0.042838.... 0.073624 sec/batch\n",
      "epoch: 18/50.... training_step: 44350.... batch_loss: 0.044330.... 0.076175 sec/batch\n",
      "epoch: 18/50.... training_step: 44400.... batch_loss: 0.049034.... 0.074865 sec/batch\n",
      "epoch: 18/50.... training_step: 44450.... batch_loss: 0.046327.... 0.081386 sec/batch\n",
      "epoch: 18/50.... training_step: 44500.... batch_loss: 0.042900.... 0.074737 sec/batch\n",
      "epoch: 18/50.... training_step: 44550.... batch_loss: 0.045430.... 0.078197 sec/batch\n",
      "epoch: 18/50.... training_step: 44600.... batch_loss: 0.043718.... 0.075199 sec/batch\n",
      "epoch: 18/50.... training_step: 44650.... batch_loss: 0.046827.... 0.075967 sec/batch\n",
      "epoch: 18/50.... training_step: 44700.... batch_loss: 0.043152.... 0.078590 sec/batch\n",
      "epoch: 18/50.... training_step: 44750.... batch_loss: 0.053142.... 0.077468 sec/batch\n",
      "epoch: 19/50.... training_step: 44800.... batch_loss: 0.041130.... 0.078030 sec/batch\n",
      "epoch: 19/50.... training_step: 44850.... batch_loss: 0.051323.... 0.077254 sec/batch\n",
      "epoch: 19/50.... training_step: 44900.... batch_loss: 0.042978.... 0.075057 sec/batch\n",
      "epoch: 19/50.... training_step: 44950.... batch_loss: 0.048617.... 0.074248 sec/batch\n",
      "epoch: 19/50.... training_step: 45000.... batch_loss: 0.039164.... 0.075337 sec/batch\n",
      "epoch: 19/50.... training_step: 45050.... batch_loss: 0.051625.... 0.074546 sec/batch\n",
      "epoch: 19/50.... training_step: 45100.... batch_loss: 0.049682.... 0.079127 sec/batch\n",
      "epoch: 19/50.... training_step: 45150.... batch_loss: 0.042379.... 0.076248 sec/batch\n",
      "epoch: 19/50.... training_step: 45200.... batch_loss: 0.041918.... 0.073840 sec/batch\n",
      "epoch: 19/50.... training_step: 45250.... batch_loss: 0.049835.... 0.074277 sec/batch\n",
      "epoch: 19/50.... training_step: 45300.... batch_loss: 0.050386.... 0.073387 sec/batch\n",
      "epoch: 19/50.... training_step: 45350.... batch_loss: 0.044022.... 0.076134 sec/batch\n",
      "epoch: 19/50.... training_step: 45400.... batch_loss: 0.040481.... 0.075892 sec/batch\n",
      "epoch: 19/50.... training_step: 45450.... batch_loss: 0.051190.... 0.074930 sec/batch\n",
      "epoch: 19/50.... training_step: 45500.... batch_loss: 0.048299.... 0.073692 sec/batch\n",
      "epoch: 19/50.... training_step: 45550.... batch_loss: 0.048777.... 0.074844 sec/batch\n",
      "epoch: 19/50.... training_step: 45600.... batch_loss: 0.048420.... 0.073710 sec/batch\n",
      "epoch: 19/50.... training_step: 45650.... batch_loss: 0.049708.... 0.074580 sec/batch\n",
      "epoch: 19/50.... training_step: 45700.... batch_loss: 0.052998.... 0.074694 sec/batch\n",
      "epoch: 19/50.... training_step: 45750.... batch_loss: 0.043895.... 0.076167 sec/batch\n",
      "epoch: 19/50.... training_step: 45800.... batch_loss: 0.049360.... 0.075299 sec/batch\n",
      "epoch: 19/50.... training_step: 45850.... batch_loss: 0.048384.... 0.073885 sec/batch\n",
      "epoch: 19/50.... training_step: 45900.... batch_loss: 0.045892.... 0.073475 sec/batch\n",
      "epoch: 19/50.... training_step: 45950.... batch_loss: 0.043072.... 0.077160 sec/batch\n",
      "epoch: 19/50.... training_step: 46000.... batch_loss: 0.044914.... 0.075800 sec/batch\n",
      "epoch: 19/50.... training_step: 46050.... batch_loss: 0.044785.... 0.077247 sec/batch\n",
      "epoch: 19/50.... training_step: 46100.... batch_loss: 0.050050.... 0.076378 sec/batch\n",
      "epoch: 19/50.... training_step: 46150.... batch_loss: 0.043504.... 0.075728 sec/batch\n",
      "epoch: 19/50.... training_step: 46200.... batch_loss: 0.045785.... 0.073972 sec/batch\n",
      "epoch: 19/50.... training_step: 46250.... batch_loss: 0.039364.... 0.074745 sec/batch\n",
      "epoch: 19/50.... training_step: 46300.... batch_loss: 0.047733.... 0.074408 sec/batch\n",
      "epoch: 19/50.... training_step: 46350.... batch_loss: 0.048227.... 0.074897 sec/batch\n",
      "epoch: 19/50.... training_step: 46400.... batch_loss: 0.047386.... 0.079580 sec/batch\n",
      "epoch: 19/50.... training_step: 46450.... batch_loss: 0.043958.... 0.074072 sec/batch\n",
      "epoch: 19/50.... training_step: 46500.... batch_loss: 0.050623.... 0.075958 sec/batch\n",
      "epoch: 19/50.... training_step: 46550.... batch_loss: 0.049269.... 0.074077 sec/batch\n",
      "epoch: 19/50.... training_step: 46600.... batch_loss: 0.047874.... 0.076949 sec/batch\n",
      "epoch: 19/50.... training_step: 46650.... batch_loss: 0.043458.... 0.074356 sec/batch\n",
      "epoch: 19/50.... training_step: 46700.... batch_loss: 0.047905.... 0.074420 sec/batch\n",
      "epoch: 19/50.... training_step: 46750.... batch_loss: 0.041594.... 0.075099 sec/batch\n",
      "epoch: 19/50.... training_step: 46800.... batch_loss: 0.046137.... 0.074587 sec/batch\n",
      "epoch: 19/50.... training_step: 46850.... batch_loss: 0.047395.... 0.078505 sec/batch\n",
      "epoch: 19/50.... training_step: 46900.... batch_loss: 0.047420.... 0.074874 sec/batch\n",
      "epoch: 19/50.... training_step: 46950.... batch_loss: 0.048380.... 0.074908 sec/batch\n",
      "epoch: 19/50.... training_step: 47000.... batch_loss: 0.045811.... 0.074976 sec/batch\n",
      "epoch: 19/50.... training_step: 47050.... batch_loss: 0.044967.... 0.074301 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19/50.... training_step: 47100.... batch_loss: 0.046311.... 0.074548 sec/batch\n",
      "epoch: 20/50.... training_step: 47150.... batch_loss: 0.046820.... 0.074199 sec/batch\n",
      "epoch: 20/50.... training_step: 47200.... batch_loss: 0.043049.... 0.080657 sec/batch\n",
      "epoch: 20/50.... training_step: 47250.... batch_loss: 0.044619.... 0.077947 sec/batch\n",
      "epoch: 20/50.... training_step: 47300.... batch_loss: 0.044932.... 0.075123 sec/batch\n",
      "epoch: 20/50.... training_step: 47350.... batch_loss: 0.046284.... 0.074120 sec/batch\n",
      "epoch: 20/50.... training_step: 47400.... batch_loss: 0.042424.... 0.075172 sec/batch\n",
      "epoch: 20/50.... training_step: 47450.... batch_loss: 0.048102.... 0.076579 sec/batch\n",
      "epoch: 20/50.... training_step: 47500.... batch_loss: 0.050768.... 0.075687 sec/batch\n",
      "epoch: 20/50.... training_step: 47550.... batch_loss: 0.037825.... 0.074509 sec/batch\n",
      "epoch: 20/50.... training_step: 47600.... batch_loss: 0.040560.... 0.075793 sec/batch\n",
      "epoch: 20/50.... training_step: 47650.... batch_loss: 0.047539.... 0.077023 sec/batch\n",
      "epoch: 20/50.... training_step: 47700.... batch_loss: 0.039618.... 0.076604 sec/batch\n",
      "epoch: 20/50.... training_step: 47750.... batch_loss: 0.040375.... 0.074275 sec/batch\n",
      "epoch: 20/50.... training_step: 47800.... batch_loss: 0.045903.... 0.074581 sec/batch\n",
      "epoch: 20/50.... training_step: 47850.... batch_loss: 0.043958.... 0.077249 sec/batch\n",
      "epoch: 20/50.... training_step: 47900.... batch_loss: 0.041742.... 0.074531 sec/batch\n",
      "epoch: 20/50.... training_step: 47950.... batch_loss: 0.048201.... 0.073670 sec/batch\n",
      "epoch: 20/50.... training_step: 48000.... batch_loss: 0.046243.... 0.074352 sec/batch\n",
      "epoch: 20/50.... training_step: 48050.... batch_loss: 0.043361.... 0.074811 sec/batch\n",
      "epoch: 20/50.... training_step: 48100.... batch_loss: 0.045896.... 0.075540 sec/batch\n",
      "epoch: 20/50.... training_step: 48150.... batch_loss: 0.041978.... 0.076745 sec/batch\n",
      "epoch: 20/50.... training_step: 48200.... batch_loss: 0.042292.... 0.075646 sec/batch\n",
      "epoch: 20/50.... training_step: 48250.... batch_loss: 0.051792.... 0.075431 sec/batch\n",
      "epoch: 20/50.... training_step: 48300.... batch_loss: 0.044228.... 0.075220 sec/batch\n",
      "epoch: 20/50.... training_step: 48350.... batch_loss: 0.046239.... 0.080817 sec/batch\n",
      "epoch: 20/50.... training_step: 48400.... batch_loss: 0.047138.... 0.075308 sec/batch\n",
      "epoch: 20/50.... training_step: 48450.... batch_loss: 0.051576.... 0.073904 sec/batch\n",
      "epoch: 20/50.... training_step: 48500.... batch_loss: 0.040583.... 0.076680 sec/batch\n",
      "epoch: 20/50.... training_step: 48550.... batch_loss: 0.042754.... 0.074063 sec/batch\n",
      "epoch: 20/50.... training_step: 48600.... batch_loss: 0.045368.... 0.075121 sec/batch\n",
      "epoch: 20/50.... training_step: 48650.... batch_loss: 0.039093.... 0.076378 sec/batch\n",
      "epoch: 20/50.... training_step: 48700.... batch_loss: 0.039100.... 0.075806 sec/batch\n",
      "epoch: 20/50.... training_step: 48750.... batch_loss: 0.045368.... 0.074239 sec/batch\n",
      "epoch: 20/50.... training_step: 48800.... batch_loss: 0.045573.... 0.074931 sec/batch\n",
      "epoch: 20/50.... training_step: 48850.... batch_loss: 0.045964.... 0.074333 sec/batch\n",
      "epoch: 20/50.... training_step: 48900.... batch_loss: 0.040918.... 0.075667 sec/batch\n",
      "epoch: 20/50.... training_step: 48950.... batch_loss: 0.043033.... 0.073716 sec/batch\n",
      "epoch: 20/50.... training_step: 49000.... batch_loss: 0.049420.... 0.074319 sec/batch\n",
      "epoch: 20/50.... training_step: 49050.... batch_loss: 0.045810.... 0.074311 sec/batch\n",
      "epoch: 20/50.... training_step: 49100.... batch_loss: 0.050401.... 0.073206 sec/batch\n",
      "epoch: 20/50.... training_step: 49150.... batch_loss: 0.044375.... 0.077342 sec/batch\n",
      "epoch: 20/50.... training_step: 49200.... batch_loss: 0.041075.... 0.073824 sec/batch\n",
      "epoch: 20/50.... training_step: 49250.... batch_loss: 0.045834.... 0.076259 sec/batch\n",
      "epoch: 20/50.... training_step: 49300.... batch_loss: 0.047568.... 0.074248 sec/batch\n",
      "epoch: 20/50.... training_step: 49350.... batch_loss: 0.047577.... 0.073946 sec/batch\n",
      "epoch: 20/50.... training_step: 49400.... batch_loss: 0.046714.... 0.075886 sec/batch\n",
      "epoch: 20/50.... training_step: 49450.... batch_loss: 0.042498.... 0.074315 sec/batch\n",
      "epoch: 21/50.... training_step: 49500.... batch_loss: 0.048429.... 0.074138 sec/batch\n",
      "epoch: 21/50.... training_step: 49550.... batch_loss: 0.040170.... 0.073986 sec/batch\n",
      "epoch: 21/50.... training_step: 49600.... batch_loss: 0.047037.... 0.075008 sec/batch\n",
      "epoch: 21/50.... training_step: 49650.... batch_loss: 0.045572.... 0.075708 sec/batch\n",
      "epoch: 21/50.... training_step: 49700.... batch_loss: 0.042145.... 0.076530 sec/batch\n",
      "epoch: 21/50.... training_step: 49750.... batch_loss: 0.047373.... 0.073584 sec/batch\n",
      "epoch: 21/50.... training_step: 49800.... batch_loss: 0.045932.... 0.076711 sec/batch\n",
      "epoch: 21/50.... training_step: 49850.... batch_loss: 0.045930.... 0.075792 sec/batch\n",
      "epoch: 21/50.... training_step: 49900.... batch_loss: 0.046013.... 0.076208 sec/batch\n",
      "epoch: 21/50.... training_step: 49950.... batch_loss: 0.047772.... 0.075026 sec/batch\n",
      "epoch: 21/50.... training_step: 50000.... batch_loss: 0.042593.... 0.074597 sec/batch\n",
      "epoch: 21/50.... training_step: 50050.... batch_loss: 0.047857.... 0.074795 sec/batch\n",
      "epoch: 21/50.... training_step: 50100.... batch_loss: 0.041889.... 0.075421 sec/batch\n",
      "epoch: 21/50.... training_step: 50150.... batch_loss: 0.051728.... 0.074996 sec/batch\n",
      "epoch: 21/50.... training_step: 50200.... batch_loss: 0.047363.... 0.074332 sec/batch\n",
      "epoch: 21/50.... training_step: 50250.... batch_loss: 0.050179.... 0.077534 sec/batch\n",
      "epoch: 21/50.... training_step: 50300.... batch_loss: 0.042938.... 0.074326 sec/batch\n",
      "epoch: 21/50.... training_step: 50350.... batch_loss: 0.041078.... 0.074316 sec/batch\n",
      "epoch: 21/50.... training_step: 50400.... batch_loss: 0.052781.... 0.074125 sec/batch\n",
      "epoch: 21/50.... training_step: 50450.... batch_loss: 0.044407.... 0.074623 sec/batch\n",
      "epoch: 21/50.... training_step: 50500.... batch_loss: 0.049673.... 0.074066 sec/batch\n",
      "epoch: 21/50.... training_step: 50550.... batch_loss: 0.045036.... 0.076593 sec/batch\n",
      "epoch: 21/50.... training_step: 50600.... batch_loss: 0.046027.... 0.074175 sec/batch\n",
      "epoch: 21/50.... training_step: 50650.... batch_loss: 0.045678.... 0.074020 sec/batch\n",
      "epoch: 21/50.... training_step: 50700.... batch_loss: 0.041874.... 0.075020 sec/batch\n",
      "epoch: 21/50.... training_step: 50750.... batch_loss: 0.040836.... 0.076956 sec/batch\n",
      "epoch: 21/50.... training_step: 50800.... batch_loss: 0.042319.... 0.075582 sec/batch\n",
      "epoch: 21/50.... training_step: 50850.... batch_loss: 0.044557.... 0.078491 sec/batch\n",
      "epoch: 21/50.... training_step: 50900.... batch_loss: 0.042430.... 0.073433 sec/batch\n",
      "epoch: 21/50.... training_step: 50950.... batch_loss: 0.049214.... 0.074826 sec/batch\n",
      "epoch: 21/50.... training_step: 51000.... batch_loss: 0.043686.... 0.077927 sec/batch\n",
      "epoch: 21/50.... training_step: 51050.... batch_loss: 0.046336.... 0.074095 sec/batch\n",
      "epoch: 21/50.... training_step: 51100.... batch_loss: 0.040109.... 0.075196 sec/batch\n",
      "epoch: 21/50.... training_step: 51150.... batch_loss: 0.047006.... 0.076298 sec/batch\n",
      "epoch: 21/50.... training_step: 51200.... batch_loss: 0.042280.... 0.073912 sec/batch\n",
      "epoch: 21/50.... training_step: 51250.... batch_loss: 0.045200.... 0.073686 sec/batch\n",
      "epoch: 21/50.... training_step: 51300.... batch_loss: 0.050230.... 0.074224 sec/batch\n",
      "epoch: 21/50.... training_step: 51350.... batch_loss: 0.040969.... 0.073479 sec/batch\n",
      "epoch: 21/50.... training_step: 51400.... batch_loss: 0.044590.... 0.075621 sec/batch\n",
      "epoch: 21/50.... training_step: 51450.... batch_loss: 0.041785.... 0.073664 sec/batch\n",
      "epoch: 21/50.... training_step: 51500.... batch_loss: 0.041358.... 0.075823 sec/batch\n",
      "epoch: 21/50.... training_step: 51550.... batch_loss: 0.047283.... 0.078750 sec/batch\n",
      "epoch: 21/50.... training_step: 51600.... batch_loss: 0.044560.... 0.075027 sec/batch\n",
      "epoch: 21/50.... training_step: 51650.... batch_loss: 0.039590.... 0.074040 sec/batch\n",
      "epoch: 21/50.... training_step: 51700.... batch_loss: 0.048140.... 0.075917 sec/batch\n",
      "epoch: 21/50.... training_step: 51750.... batch_loss: 0.035759.... 0.076058 sec/batch\n",
      "epoch: 21/50.... training_step: 51800.... batch_loss: 0.042563.... 0.079778 sec/batch\n",
      "epoch: 22/50.... training_step: 51850.... batch_loss: 0.037969.... 0.072992 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22/50.... training_step: 51900.... batch_loss: 0.047480.... 0.073844 sec/batch\n",
      "epoch: 22/50.... training_step: 51950.... batch_loss: 0.041990.... 0.073774 sec/batch\n",
      "epoch: 22/50.... training_step: 52000.... batch_loss: 0.041524.... 0.073590 sec/batch\n",
      "epoch: 22/50.... training_step: 52050.... batch_loss: 0.039153.... 0.076275 sec/batch\n",
      "epoch: 22/50.... training_step: 52100.... batch_loss: 0.044066.... 0.073445 sec/batch\n",
      "epoch: 22/50.... training_step: 52150.... batch_loss: 0.048326.... 0.076808 sec/batch\n",
      "epoch: 22/50.... training_step: 52200.... batch_loss: 0.047654.... 0.075033 sec/batch\n",
      "epoch: 22/50.... training_step: 52250.... batch_loss: 0.045375.... 0.072916 sec/batch\n",
      "epoch: 22/50.... training_step: 52300.... batch_loss: 0.044810.... 0.074933 sec/batch\n",
      "epoch: 22/50.... training_step: 52350.... batch_loss: 0.046040.... 0.073844 sec/batch\n",
      "epoch: 22/50.... training_step: 52400.... batch_loss: 0.045059.... 0.074946 sec/batch\n",
      "epoch: 22/50.... training_step: 52450.... batch_loss: 0.043504.... 0.073208 sec/batch\n",
      "epoch: 22/50.... training_step: 52500.... batch_loss: 0.045879.... 0.074891 sec/batch\n",
      "epoch: 22/50.... training_step: 52550.... batch_loss: 0.039551.... 0.073823 sec/batch\n",
      "epoch: 22/50.... training_step: 52600.... batch_loss: 0.044033.... 0.073841 sec/batch\n",
      "epoch: 22/50.... training_step: 52650.... batch_loss: 0.039467.... 0.076180 sec/batch\n",
      "epoch: 22/50.... training_step: 52700.... batch_loss: 0.051198.... 0.075839 sec/batch\n",
      "epoch: 22/50.... training_step: 52750.... batch_loss: 0.044291.... 0.073195 sec/batch\n",
      "epoch: 22/50.... training_step: 52800.... batch_loss: 0.045443.... 0.076409 sec/batch\n",
      "epoch: 22/50.... training_step: 52850.... batch_loss: 0.040937.... 0.073731 sec/batch\n",
      "epoch: 22/50.... training_step: 52900.... batch_loss: 0.044545.... 0.074466 sec/batch\n",
      "epoch: 22/50.... training_step: 52950.... batch_loss: 0.041125.... 0.073284 sec/batch\n",
      "epoch: 22/50.... training_step: 53000.... batch_loss: 0.040267.... 0.073467 sec/batch\n",
      "epoch: 22/50.... training_step: 53050.... batch_loss: 0.043728.... 0.077192 sec/batch\n",
      "epoch: 22/50.... training_step: 53100.... batch_loss: 0.048921.... 0.074423 sec/batch\n",
      "epoch: 22/50.... training_step: 53150.... batch_loss: 0.044587.... 0.073103 sec/batch\n",
      "epoch: 22/50.... training_step: 53200.... batch_loss: 0.046373.... 0.077211 sec/batch\n",
      "epoch: 22/50.... training_step: 53250.... batch_loss: 0.043913.... 0.072308 sec/batch\n",
      "epoch: 22/50.... training_step: 53300.... batch_loss: 0.046007.... 0.076072 sec/batch\n",
      "epoch: 22/50.... training_step: 53350.... batch_loss: 0.045860.... 0.074248 sec/batch\n",
      "epoch: 22/50.... training_step: 53400.... batch_loss: 0.047654.... 0.077968 sec/batch\n",
      "epoch: 22/50.... training_step: 53450.... batch_loss: 0.042972.... 0.074211 sec/batch\n",
      "epoch: 22/50.... training_step: 53500.... batch_loss: 0.046945.... 0.076222 sec/batch\n",
      "epoch: 22/50.... training_step: 53550.... batch_loss: 0.049192.... 0.077082 sec/batch\n",
      "epoch: 22/50.... training_step: 53600.... batch_loss: 0.044105.... 0.079193 sec/batch\n",
      "epoch: 22/50.... training_step: 53650.... batch_loss: 0.045357.... 0.076633 sec/batch\n",
      "epoch: 22/50.... training_step: 53700.... batch_loss: 0.045217.... 0.075450 sec/batch\n",
      "epoch: 22/50.... training_step: 53750.... batch_loss: 0.038617.... 0.074214 sec/batch\n",
      "epoch: 22/50.... training_step: 53800.... batch_loss: 0.044905.... 0.077298 sec/batch\n",
      "epoch: 22/50.... training_step: 53850.... batch_loss: 0.042400.... 0.078175 sec/batch\n",
      "epoch: 22/50.... training_step: 53900.... batch_loss: 0.042663.... 0.076146 sec/batch\n",
      "epoch: 22/50.... training_step: 53950.... batch_loss: 0.043095.... 0.079571 sec/batch\n",
      "epoch: 22/50.... training_step: 54000.... batch_loss: 0.041738.... 0.072654 sec/batch\n",
      "epoch: 22/50.... training_step: 54050.... batch_loss: 0.043639.... 0.076624 sec/batch\n",
      "epoch: 22/50.... training_step: 54100.... batch_loss: 0.038610.... 0.073482 sec/batch\n",
      "epoch: 22/50.... training_step: 54150.... batch_loss: 0.040686.... 0.074585 sec/batch\n",
      "epoch: 23/50.... training_step: 54200.... batch_loss: 0.042986.... 0.075635 sec/batch\n",
      "epoch: 23/50.... training_step: 54250.... batch_loss: 0.043444.... 0.075472 sec/batch\n",
      "epoch: 23/50.... training_step: 54300.... batch_loss: 0.047033.... 0.073114 sec/batch\n",
      "epoch: 23/50.... training_step: 54350.... batch_loss: 0.048768.... 0.074273 sec/batch\n",
      "epoch: 23/50.... training_step: 54400.... batch_loss: 0.043939.... 0.078548 sec/batch\n",
      "epoch: 23/50.... training_step: 54450.... batch_loss: 0.038845.... 0.075324 sec/batch\n",
      "epoch: 23/50.... training_step: 54500.... batch_loss: 0.043071.... 0.076005 sec/batch\n",
      "epoch: 23/50.... training_step: 54550.... batch_loss: 0.041047.... 0.073948 sec/batch\n",
      "epoch: 23/50.... training_step: 54600.... batch_loss: 0.045303.... 0.074796 sec/batch\n",
      "epoch: 23/50.... training_step: 54650.... batch_loss: 0.046799.... 0.075023 sec/batch\n",
      "epoch: 23/50.... training_step: 54700.... batch_loss: 0.046548.... 0.074100 sec/batch\n",
      "epoch: 23/50.... training_step: 54750.... batch_loss: 0.045844.... 0.074779 sec/batch\n",
      "epoch: 23/50.... training_step: 54800.... batch_loss: 0.046399.... 0.077218 sec/batch\n",
      "epoch: 23/50.... training_step: 54850.... batch_loss: 0.045586.... 0.074074 sec/batch\n",
      "epoch: 23/50.... training_step: 54900.... batch_loss: 0.046889.... 0.074357 sec/batch\n",
      "epoch: 23/50.... training_step: 54950.... batch_loss: 0.047389.... 0.075819 sec/batch\n",
      "epoch: 23/50.... training_step: 55000.... batch_loss: 0.044707.... 0.076149 sec/batch\n",
      "epoch: 23/50.... training_step: 55050.... batch_loss: 0.035603.... 0.073579 sec/batch\n",
      "epoch: 23/50.... training_step: 55100.... batch_loss: 0.043011.... 0.075483 sec/batch\n",
      "epoch: 23/50.... training_step: 55150.... batch_loss: 0.042074.... 0.073660 sec/batch\n",
      "epoch: 23/50.... training_step: 55200.... batch_loss: 0.042244.... 0.073803 sec/batch\n",
      "epoch: 23/50.... training_step: 55250.... batch_loss: 0.040766.... 0.073135 sec/batch\n",
      "epoch: 23/50.... training_step: 55300.... batch_loss: 0.041019.... 0.074362 sec/batch\n",
      "epoch: 23/50.... training_step: 55350.... batch_loss: 0.040001.... 0.073409 sec/batch\n",
      "epoch: 23/50.... training_step: 55400.... batch_loss: 0.043492.... 0.072861 sec/batch\n",
      "epoch: 23/50.... training_step: 55450.... batch_loss: 0.042002.... 0.073407 sec/batch\n",
      "epoch: 23/50.... training_step: 55500.... batch_loss: 0.045673.... 0.076246 sec/batch\n",
      "epoch: 23/50.... training_step: 55550.... batch_loss: 0.041266.... 0.076217 sec/batch\n",
      "epoch: 23/50.... training_step: 55600.... batch_loss: 0.048470.... 0.074229 sec/batch\n",
      "epoch: 23/50.... training_step: 55650.... batch_loss: 0.042592.... 0.074645 sec/batch\n",
      "epoch: 23/50.... training_step: 55700.... batch_loss: 0.042227.... 0.073605 sec/batch\n",
      "epoch: 23/50.... training_step: 55750.... batch_loss: 0.044742.... 0.077794 sec/batch\n",
      "epoch: 23/50.... training_step: 55800.... batch_loss: 0.039550.... 0.074259 sec/batch\n",
      "epoch: 23/50.... training_step: 55850.... batch_loss: 0.044529.... 0.075185 sec/batch\n",
      "epoch: 23/50.... training_step: 55900.... batch_loss: 0.040768.... 0.073119 sec/batch\n",
      "epoch: 23/50.... training_step: 55950.... batch_loss: 0.037950.... 0.074720 sec/batch\n",
      "epoch: 23/50.... training_step: 56000.... batch_loss: 0.041617.... 0.073457 sec/batch\n",
      "epoch: 23/50.... training_step: 56050.... batch_loss: 0.038709.... 0.073671 sec/batch\n",
      "epoch: 23/50.... training_step: 56100.... batch_loss: 0.042802.... 0.075577 sec/batch\n",
      "epoch: 23/50.... training_step: 56150.... batch_loss: 0.039089.... 0.077364 sec/batch\n",
      "epoch: 23/50.... training_step: 56200.... batch_loss: 0.047194.... 0.074752 sec/batch\n",
      "epoch: 23/50.... training_step: 56250.... batch_loss: 0.043284.... 0.073889 sec/batch\n",
      "epoch: 23/50.... training_step: 56300.... batch_loss: 0.042948.... 0.074749 sec/batch\n",
      "epoch: 23/50.... training_step: 56350.... batch_loss: 0.045541.... 0.076102 sec/batch\n",
      "epoch: 23/50.... training_step: 56400.... batch_loss: 0.039142.... 0.076161 sec/batch\n",
      "epoch: 23/50.... training_step: 56450.... batch_loss: 0.044494.... 0.074470 sec/batch\n",
      "epoch: 23/50.... training_step: 56500.... batch_loss: 0.045534.... 0.076668 sec/batch\n",
      "epoch: 24/50.... training_step: 56550.... batch_loss: 0.043221.... 0.074915 sec/batch\n",
      "epoch: 24/50.... training_step: 56600.... batch_loss: 0.040974.... 0.074071 sec/batch\n",
      "epoch: 24/50.... training_step: 56650.... batch_loss: 0.042110.... 0.073719 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24/50.... training_step: 56700.... batch_loss: 0.042969.... 0.074025 sec/batch\n",
      "epoch: 24/50.... training_step: 56750.... batch_loss: 0.042805.... 0.073907 sec/batch\n",
      "epoch: 24/50.... training_step: 56800.... batch_loss: 0.040396.... 0.078091 sec/batch\n",
      "epoch: 24/50.... training_step: 56850.... batch_loss: 0.046313.... 0.076982 sec/batch\n",
      "epoch: 24/50.... training_step: 56900.... batch_loss: 0.050355.... 0.076418 sec/batch\n",
      "epoch: 24/50.... training_step: 56950.... batch_loss: 0.040919.... 0.076372 sec/batch\n",
      "epoch: 24/50.... training_step: 57000.... batch_loss: 0.039567.... 0.077765 sec/batch\n",
      "epoch: 24/50.... training_step: 57050.... batch_loss: 0.039335.... 0.076827 sec/batch\n",
      "epoch: 24/50.... training_step: 57100.... batch_loss: 0.041852.... 0.077820 sec/batch\n",
      "epoch: 24/50.... training_step: 57150.... batch_loss: 0.038412.... 0.075908 sec/batch\n",
      "epoch: 24/50.... training_step: 57200.... batch_loss: 0.044297.... 0.074646 sec/batch\n",
      "epoch: 24/50.... training_step: 57250.... batch_loss: 0.042380.... 0.075974 sec/batch\n",
      "epoch: 24/50.... training_step: 57300.... batch_loss: 0.038291.... 0.074363 sec/batch\n",
      "epoch: 24/50.... training_step: 57350.... batch_loss: 0.036942.... 0.074274 sec/batch\n",
      "epoch: 24/50.... training_step: 57400.... batch_loss: 0.041240.... 0.077280 sec/batch\n",
      "epoch: 24/50.... training_step: 57450.... batch_loss: 0.042246.... 0.076182 sec/batch\n",
      "epoch: 24/50.... training_step: 57500.... batch_loss: 0.040906.... 0.074195 sec/batch\n",
      "epoch: 24/50.... training_step: 57550.... batch_loss: 0.043310.... 0.077174 sec/batch\n",
      "epoch: 24/50.... training_step: 57600.... batch_loss: 0.041919.... 0.075133 sec/batch\n",
      "epoch: 24/50.... training_step: 57650.... batch_loss: 0.038850.... 0.076923 sec/batch\n",
      "epoch: 24/50.... training_step: 57700.... batch_loss: 0.041279.... 0.077113 sec/batch\n",
      "epoch: 24/50.... training_step: 57750.... batch_loss: 0.047355.... 0.078268 sec/batch\n",
      "epoch: 24/50.... training_step: 57800.... batch_loss: 0.038966.... 0.075378 sec/batch\n",
      "epoch: 24/50.... training_step: 57850.... batch_loss: 0.042556.... 0.074140 sec/batch\n",
      "epoch: 24/50.... training_step: 57900.... batch_loss: 0.045095.... 0.075984 sec/batch\n",
      "epoch: 24/50.... training_step: 57950.... batch_loss: 0.043175.... 0.072649 sec/batch\n",
      "epoch: 24/50.... training_step: 58000.... batch_loss: 0.044740.... 0.075381 sec/batch\n",
      "epoch: 24/50.... training_step: 58050.... batch_loss: 0.037791.... 0.074430 sec/batch\n",
      "epoch: 24/50.... training_step: 58100.... batch_loss: 0.038202.... 0.077411 sec/batch\n",
      "epoch: 24/50.... training_step: 58150.... batch_loss: 0.042712.... 0.076243 sec/batch\n",
      "epoch: 24/50.... training_step: 58200.... batch_loss: 0.041988.... 0.073611 sec/batch\n",
      "epoch: 24/50.... training_step: 58250.... batch_loss: 0.039733.... 0.072998 sec/batch\n",
      "epoch: 24/50.... training_step: 58300.... batch_loss: 0.041973.... 0.074893 sec/batch\n",
      "epoch: 24/50.... training_step: 58350.... batch_loss: 0.046672.... 0.075104 sec/batch\n",
      "epoch: 24/50.... training_step: 58400.... batch_loss: 0.040176.... 0.078052 sec/batch\n",
      "epoch: 24/50.... training_step: 58450.... batch_loss: 0.044096.... 0.073726 sec/batch\n",
      "epoch: 24/50.... training_step: 58500.... batch_loss: 0.046695.... 0.076196 sec/batch\n",
      "epoch: 24/50.... training_step: 58550.... batch_loss: 0.037622.... 0.080198 sec/batch\n",
      "epoch: 24/50.... training_step: 58600.... batch_loss: 0.043614.... 0.075513 sec/batch\n",
      "epoch: 24/50.... training_step: 58650.... batch_loss: 0.039931.... 0.075432 sec/batch\n",
      "epoch: 24/50.... training_step: 58700.... batch_loss: 0.043595.... 0.075566 sec/batch\n",
      "epoch: 24/50.... training_step: 58750.... batch_loss: 0.039981.... 0.078212 sec/batch\n",
      "epoch: 24/50.... training_step: 58800.... batch_loss: 0.045077.... 0.073754 sec/batch\n",
      "epoch: 24/50.... training_step: 58850.... batch_loss: 0.041159.... 0.077598 sec/batch\n",
      "epoch: 24/50.... training_step: 58900.... batch_loss: 0.047875.... 0.073589 sec/batch\n",
      "epoch: 25/50.... training_step: 58950.... batch_loss: 0.038892.... 0.078437 sec/batch\n",
      "epoch: 25/50.... training_step: 59000.... batch_loss: 0.040297.... 0.074564 sec/batch\n",
      "epoch: 25/50.... training_step: 59050.... batch_loss: 0.041471.... 0.073000 sec/batch\n",
      "epoch: 25/50.... training_step: 59100.... batch_loss: 0.041709.... 0.073031 sec/batch\n",
      "epoch: 25/50.... training_step: 59150.... batch_loss: 0.040012.... 0.076025 sec/batch\n",
      "epoch: 25/50.... training_step: 59200.... batch_loss: 0.043817.... 0.075606 sec/batch\n",
      "epoch: 25/50.... training_step: 59250.... batch_loss: 0.042821.... 0.073607 sec/batch\n",
      "epoch: 25/50.... training_step: 59300.... batch_loss: 0.039812.... 0.076396 sec/batch\n",
      "epoch: 25/50.... training_step: 59350.... batch_loss: 0.036596.... 0.076417 sec/batch\n",
      "epoch: 25/50.... training_step: 59400.... batch_loss: 0.038252.... 0.077069 sec/batch\n",
      "epoch: 25/50.... training_step: 59450.... batch_loss: 0.038545.... 0.076421 sec/batch\n",
      "epoch: 25/50.... training_step: 59500.... batch_loss: 0.041368.... 0.073794 sec/batch\n",
      "epoch: 25/50.... training_step: 59550.... batch_loss: 0.037723.... 0.074901 sec/batch\n",
      "epoch: 25/50.... training_step: 59600.... batch_loss: 0.045958.... 0.074698 sec/batch\n",
      "epoch: 25/50.... training_step: 59650.... batch_loss: 0.042790.... 0.073916 sec/batch\n",
      "epoch: 25/50.... training_step: 59700.... batch_loss: 0.041780.... 0.074318 sec/batch\n",
      "epoch: 25/50.... training_step: 59750.... batch_loss: 0.035432.... 0.074499 sec/batch\n",
      "epoch: 25/50.... training_step: 59800.... batch_loss: 0.039112.... 0.078215 sec/batch\n",
      "epoch: 25/50.... training_step: 59850.... batch_loss: 0.040065.... 0.075631 sec/batch\n",
      "epoch: 25/50.... training_step: 59900.... batch_loss: 0.041125.... 0.073802 sec/batch\n",
      "epoch: 25/50.... training_step: 59950.... batch_loss: 0.040460.... 0.075715 sec/batch\n",
      "epoch: 25/50.... training_step: 60000.... batch_loss: 0.039232.... 0.074927 sec/batch\n",
      "epoch: 25/50.... training_step: 60050.... batch_loss: 0.041787.... 0.075200 sec/batch\n",
      "epoch: 25/50.... training_step: 60100.... batch_loss: 0.040650.... 0.075628 sec/batch\n",
      "epoch: 25/50.... training_step: 60150.... batch_loss: 0.042728.... 0.075494 sec/batch\n",
      "epoch: 25/50.... training_step: 60200.... batch_loss: 0.038696.... 0.077867 sec/batch\n",
      "epoch: 25/50.... training_step: 60250.... batch_loss: 0.040033.... 0.074348 sec/batch\n",
      "epoch: 25/50.... training_step: 60300.... batch_loss: 0.037848.... 0.078431 sec/batch\n",
      "epoch: 25/50.... training_step: 60350.... batch_loss: 0.047701.... 0.073148 sec/batch\n",
      "epoch: 25/50.... training_step: 60400.... batch_loss: 0.040506.... 0.075699 sec/batch\n",
      "epoch: 25/50.... training_step: 60450.... batch_loss: 0.043745.... 0.075750 sec/batch\n",
      "epoch: 25/50.... training_step: 60500.... batch_loss: 0.045047.... 0.076365 sec/batch\n",
      "epoch: 25/50.... training_step: 60550.... batch_loss: 0.039268.... 0.076913 sec/batch\n",
      "epoch: 25/50.... training_step: 60600.... batch_loss: 0.039760.... 0.074962 sec/batch\n",
      "epoch: 25/50.... training_step: 60650.... batch_loss: 0.046961.... 0.076868 sec/batch\n",
      "epoch: 25/50.... training_step: 60700.... batch_loss: 0.038646.... 0.074002 sec/batch\n",
      "epoch: 25/50.... training_step: 60750.... batch_loss: 0.043381.... 0.075441 sec/batch\n",
      "epoch: 25/50.... training_step: 60800.... batch_loss: 0.041391.... 0.074906 sec/batch\n",
      "epoch: 25/50.... training_step: 60850.... batch_loss: 0.039504.... 0.073637 sec/batch\n",
      "epoch: 25/50.... training_step: 60900.... batch_loss: 0.039529.... 0.077091 sec/batch\n",
      "epoch: 25/50.... training_step: 60950.... batch_loss: 0.041368.... 0.073613 sec/batch\n",
      "epoch: 25/50.... training_step: 61000.... batch_loss: 0.045683.... 0.076490 sec/batch\n",
      "epoch: 25/50.... training_step: 61050.... batch_loss: 0.041405.... 0.074361 sec/batch\n",
      "epoch: 25/50.... training_step: 61100.... batch_loss: 0.040313.... 0.074131 sec/batch\n",
      "epoch: 25/50.... training_step: 61150.... batch_loss: 0.040941.... 0.075894 sec/batch\n",
      "epoch: 25/50.... training_step: 61200.... batch_loss: 0.041498.... 0.078077 sec/batch\n",
      "epoch: 25/50.... training_step: 61250.... batch_loss: 0.043489.... 0.075930 sec/batch\n",
      "epoch: 26/50.... training_step: 61300.... batch_loss: 0.041912.... 0.074232 sec/batch\n",
      "epoch: 26/50.... training_step: 61350.... batch_loss: 0.039793.... 0.075415 sec/batch\n",
      "epoch: 26/50.... training_step: 61400.... batch_loss: 0.042892.... 0.074934 sec/batch\n",
      "epoch: 26/50.... training_step: 61450.... batch_loss: 0.041055.... 0.076093 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26/50.... training_step: 61500.... batch_loss: 0.041352.... 0.078123 sec/batch\n",
      "epoch: 26/50.... training_step: 61550.... batch_loss: 0.040002.... 0.073663 sec/batch\n",
      "epoch: 26/50.... training_step: 61600.... batch_loss: 0.037974.... 0.073525 sec/batch\n",
      "epoch: 26/50.... training_step: 61650.... batch_loss: 0.039154.... 0.077580 sec/batch\n",
      "epoch: 26/50.... training_step: 61700.... batch_loss: 0.038914.... 0.075408 sec/batch\n",
      "epoch: 26/50.... training_step: 61750.... batch_loss: 0.043520.... 0.074280 sec/batch\n",
      "epoch: 26/50.... training_step: 61800.... batch_loss: 0.040601.... 0.074919 sec/batch\n",
      "epoch: 26/50.... training_step: 61850.... batch_loss: 0.043932.... 0.075939 sec/batch\n",
      "epoch: 26/50.... training_step: 61900.... batch_loss: 0.042781.... 0.073416 sec/batch\n",
      "epoch: 26/50.... training_step: 61950.... batch_loss: 0.034202.... 0.077289 sec/batch\n",
      "epoch: 26/50.... training_step: 62000.... batch_loss: 0.038615.... 0.074847 sec/batch\n",
      "epoch: 26/50.... training_step: 62050.... batch_loss: 0.045262.... 0.075796 sec/batch\n",
      "epoch: 26/50.... training_step: 62100.... batch_loss: 0.039938.... 0.072744 sec/batch\n",
      "epoch: 26/50.... training_step: 62150.... batch_loss: 0.042284.... 0.073947 sec/batch\n",
      "epoch: 26/50.... training_step: 62200.... batch_loss: 0.041529.... 0.077179 sec/batch\n",
      "epoch: 26/50.... training_step: 62250.... batch_loss: 0.040725.... 0.073363 sec/batch\n",
      "epoch: 26/50.... training_step: 62300.... batch_loss: 0.042672.... 0.076472 sec/batch\n",
      "epoch: 26/50.... training_step: 62350.... batch_loss: 0.040079.... 0.075445 sec/batch\n",
      "epoch: 26/50.... training_step: 62400.... batch_loss: 0.033899.... 0.074305 sec/batch\n",
      "epoch: 26/50.... training_step: 62450.... batch_loss: 0.038620.... 0.075038 sec/batch\n",
      "epoch: 26/50.... training_step: 62500.... batch_loss: 0.044380.... 0.073898 sec/batch\n",
      "epoch: 26/50.... training_step: 62550.... batch_loss: 0.039242.... 0.074138 sec/batch\n",
      "epoch: 26/50.... training_step: 62600.... batch_loss: 0.042347.... 0.073426 sec/batch\n",
      "epoch: 26/50.... training_step: 62650.... batch_loss: 0.037726.... 0.074447 sec/batch\n",
      "epoch: 26/50.... training_step: 62700.... batch_loss: 0.043917.... 0.073720 sec/batch\n",
      "epoch: 26/50.... training_step: 62750.... batch_loss: 0.044114.... 0.072974 sec/batch\n",
      "epoch: 26/50.... training_step: 62800.... batch_loss: 0.044006.... 0.073448 sec/batch\n",
      "epoch: 26/50.... training_step: 62850.... batch_loss: 0.039846.... 0.075487 sec/batch\n",
      "epoch: 26/50.... training_step: 62900.... batch_loss: 0.043932.... 0.073315 sec/batch\n",
      "epoch: 26/50.... training_step: 62950.... batch_loss: 0.040696.... 0.072490 sec/batch\n",
      "epoch: 26/50.... training_step: 63000.... batch_loss: 0.040335.... 0.072997 sec/batch\n",
      "epoch: 26/50.... training_step: 63050.... batch_loss: 0.040312.... 0.074290 sec/batch\n",
      "epoch: 26/50.... training_step: 63100.... batch_loss: 0.038004.... 0.077380 sec/batch\n",
      "epoch: 26/50.... training_step: 63150.... batch_loss: 0.040221.... 0.074004 sec/batch\n",
      "epoch: 26/50.... training_step: 63200.... batch_loss: 0.040809.... 0.075027 sec/batch\n",
      "epoch: 26/50.... training_step: 63250.... batch_loss: 0.040716.... 0.073880 sec/batch\n",
      "epoch: 26/50.... training_step: 63300.... batch_loss: 0.042978.... 0.075493 sec/batch\n",
      "epoch: 26/50.... training_step: 63350.... batch_loss: 0.044209.... 0.073080 sec/batch\n",
      "epoch: 26/50.... training_step: 63400.... batch_loss: 0.039410.... 0.076382 sec/batch\n",
      "epoch: 26/50.... training_step: 63450.... batch_loss: 0.043795.... 0.076880 sec/batch\n",
      "epoch: 26/50.... training_step: 63500.... batch_loss: 0.039998.... 0.074796 sec/batch\n",
      "epoch: 26/50.... training_step: 63550.... batch_loss: 0.043152.... 0.073684 sec/batch\n",
      "epoch: 26/50.... training_step: 63600.... batch_loss: 0.040738.... 0.075707 sec/batch\n",
      "epoch: 27/50.... training_step: 63650.... batch_loss: 0.039277.... 0.074751 sec/batch\n",
      "epoch: 27/50.... training_step: 63700.... batch_loss: 0.036629.... 0.073970 sec/batch\n",
      "epoch: 27/50.... training_step: 63750.... batch_loss: 0.038457.... 0.073487 sec/batch\n",
      "epoch: 27/50.... training_step: 63800.... batch_loss: 0.038875.... 0.074798 sec/batch\n",
      "epoch: 27/50.... training_step: 63850.... batch_loss: 0.040059.... 0.076257 sec/batch\n",
      "epoch: 27/50.... training_step: 63900.... batch_loss: 0.044382.... 0.075870 sec/batch\n",
      "epoch: 27/50.... training_step: 63950.... batch_loss: 0.045522.... 0.074532 sec/batch\n",
      "epoch: 27/50.... training_step: 64000.... batch_loss: 0.041399.... 0.072469 sec/batch\n",
      "epoch: 27/50.... training_step: 64050.... batch_loss: 0.033941.... 0.074318 sec/batch\n",
      "epoch: 27/50.... training_step: 64100.... batch_loss: 0.039822.... 0.075144 sec/batch\n",
      "epoch: 27/50.... training_step: 64150.... batch_loss: 0.044649.... 0.073730 sec/batch\n",
      "epoch: 27/50.... training_step: 64200.... batch_loss: 0.036023.... 0.076365 sec/batch\n",
      "epoch: 27/50.... training_step: 64250.... batch_loss: 0.040815.... 0.076564 sec/batch\n",
      "epoch: 27/50.... training_step: 64300.... batch_loss: 0.035788.... 0.074130 sec/batch\n",
      "epoch: 27/50.... training_step: 64350.... batch_loss: 0.037986.... 0.073544 sec/batch\n",
      "epoch: 27/50.... training_step: 64400.... batch_loss: 0.040353.... 0.073088 sec/batch\n",
      "epoch: 27/50.... training_step: 64450.... batch_loss: 0.037462.... 0.080039 sec/batch\n",
      "epoch: 27/50.... training_step: 64500.... batch_loss: 0.035026.... 0.073461 sec/batch\n",
      "epoch: 27/50.... training_step: 64550.... batch_loss: 0.038401.... 0.075045 sec/batch\n",
      "epoch: 27/50.... training_step: 64600.... batch_loss: 0.036845.... 0.073732 sec/batch\n",
      "epoch: 27/50.... training_step: 64650.... batch_loss: 0.043164.... 0.074600 sec/batch\n",
      "epoch: 27/50.... training_step: 64700.... batch_loss: 0.042134.... 0.076872 sec/batch\n",
      "epoch: 27/50.... training_step: 64750.... batch_loss: 0.034551.... 0.075223 sec/batch\n",
      "epoch: 27/50.... training_step: 64800.... batch_loss: 0.034206.... 0.077295 sec/batch\n",
      "epoch: 27/50.... training_step: 64850.... batch_loss: 0.038959.... 0.075355 sec/batch\n",
      "epoch: 27/50.... training_step: 64900.... batch_loss: 0.039160.... 0.075976 sec/batch\n",
      "epoch: 27/50.... training_step: 64950.... batch_loss: 0.040089.... 0.073754 sec/batch\n",
      "epoch: 27/50.... training_step: 65000.... batch_loss: 0.037530.... 0.072708 sec/batch\n",
      "epoch: 27/50.... training_step: 65050.... batch_loss: 0.040866.... 0.076433 sec/batch\n",
      "epoch: 27/50.... training_step: 65100.... batch_loss: 0.041577.... 0.073015 sec/batch\n",
      "epoch: 27/50.... training_step: 65150.... batch_loss: 0.039008.... 0.077040 sec/batch\n",
      "epoch: 27/50.... training_step: 65200.... batch_loss: 0.034310.... 0.077057 sec/batch\n",
      "epoch: 27/50.... training_step: 65250.... batch_loss: 0.040370.... 0.076815 sec/batch\n",
      "epoch: 27/50.... training_step: 65300.... batch_loss: 0.039397.... 0.076412 sec/batch\n",
      "epoch: 27/50.... training_step: 65350.... batch_loss: 0.043107.... 0.075467 sec/batch\n",
      "epoch: 27/50.... training_step: 65400.... batch_loss: 0.036562.... 0.073861 sec/batch\n",
      "epoch: 27/50.... training_step: 65450.... batch_loss: 0.042136.... 0.076236 sec/batch\n",
      "epoch: 27/50.... training_step: 65500.... batch_loss: 0.046536.... 0.074629 sec/batch\n",
      "epoch: 27/50.... training_step: 65550.... batch_loss: 0.043663.... 0.075037 sec/batch\n",
      "epoch: 27/50.... training_step: 65600.... batch_loss: 0.038858.... 0.073610 sec/batch\n",
      "epoch: 27/50.... training_step: 65650.... batch_loss: 0.040188.... 0.077178 sec/batch\n",
      "epoch: 27/50.... training_step: 65700.... batch_loss: 0.039295.... 0.074025 sec/batch\n",
      "epoch: 27/50.... training_step: 65750.... batch_loss: 0.044754.... 0.074560 sec/batch\n",
      "epoch: 27/50.... training_step: 65800.... batch_loss: 0.038912.... 0.073170 sec/batch\n",
      "epoch: 27/50.... training_step: 65850.... batch_loss: 0.038598.... 0.075066 sec/batch\n",
      "epoch: 27/50.... training_step: 65900.... batch_loss: 0.036736.... 0.077767 sec/batch\n",
      "epoch: 27/50.... training_step: 65950.... batch_loss: 0.039239.... 0.079097 sec/batch\n",
      "epoch: 28/50.... training_step: 66000.... batch_loss: 0.036819.... 0.073223 sec/batch\n",
      "epoch: 28/50.... training_step: 66050.... batch_loss: 0.039292.... 0.075757 sec/batch\n",
      "epoch: 28/50.... training_step: 66100.... batch_loss: 0.041297.... 0.075493 sec/batch\n",
      "epoch: 28/50.... training_step: 66150.... batch_loss: 0.036774.... 0.075414 sec/batch\n",
      "epoch: 28/50.... training_step: 66200.... batch_loss: 0.038865.... 0.076260 sec/batch\n",
      "epoch: 28/50.... training_step: 66250.... batch_loss: 0.041396.... 0.074045 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28/50.... training_step: 66300.... batch_loss: 0.038856.... 0.072956 sec/batch\n",
      "epoch: 28/50.... training_step: 66350.... batch_loss: 0.040030.... 0.075291 sec/batch\n",
      "epoch: 28/50.... training_step: 66400.... batch_loss: 0.035273.... 0.073666 sec/batch\n",
      "epoch: 28/50.... training_step: 66450.... batch_loss: 0.038812.... 0.076167 sec/batch\n",
      "epoch: 28/50.... training_step: 66500.... batch_loss: 0.038048.... 0.076236 sec/batch\n",
      "epoch: 28/50.... training_step: 66550.... batch_loss: 0.043109.... 0.073046 sec/batch\n",
      "epoch: 28/50.... training_step: 66600.... batch_loss: 0.037902.... 0.077318 sec/batch\n",
      "epoch: 28/50.... training_step: 66650.... batch_loss: 0.038110.... 0.074874 sec/batch\n",
      "epoch: 28/50.... training_step: 66700.... batch_loss: 0.039547.... 0.074066 sec/batch\n",
      "epoch: 28/50.... training_step: 66750.... batch_loss: 0.041623.... 0.074133 sec/batch\n",
      "epoch: 28/50.... training_step: 66800.... batch_loss: 0.039041.... 0.073927 sec/batch\n",
      "epoch: 28/50.... training_step: 66850.... batch_loss: 0.042867.... 0.073208 sec/batch\n",
      "epoch: 28/50.... training_step: 66900.... batch_loss: 0.043728.... 0.073567 sec/batch\n",
      "epoch: 28/50.... training_step: 66950.... batch_loss: 0.037774.... 0.074632 sec/batch\n",
      "epoch: 28/50.... training_step: 67000.... batch_loss: 0.042202.... 0.073370 sec/batch\n",
      "epoch: 28/50.... training_step: 67050.... batch_loss: 0.037649.... 0.073794 sec/batch\n",
      "epoch: 28/50.... training_step: 67100.... batch_loss: 0.040895.... 0.074274 sec/batch\n",
      "epoch: 28/50.... training_step: 67150.... batch_loss: 0.038853.... 0.074051 sec/batch\n",
      "epoch: 28/50.... training_step: 67200.... batch_loss: 0.037901.... 0.074188 sec/batch\n",
      "epoch: 28/50.... training_step: 67250.... batch_loss: 0.039307.... 0.073889 sec/batch\n",
      "epoch: 28/50.... training_step: 67300.... batch_loss: 0.039865.... 0.075631 sec/batch\n",
      "epoch: 28/50.... training_step: 67350.... batch_loss: 0.042158.... 0.073419 sec/batch\n",
      "epoch: 28/50.... training_step: 67400.... batch_loss: 0.037123.... 0.076076 sec/batch\n",
      "epoch: 28/50.... training_step: 67450.... batch_loss: 0.038379.... 0.073981 sec/batch\n",
      "epoch: 28/50.... training_step: 67500.... batch_loss: 0.038628.... 0.076094 sec/batch\n",
      "epoch: 28/50.... training_step: 67550.... batch_loss: 0.039604.... 0.073101 sec/batch\n",
      "epoch: 28/50.... training_step: 67600.... batch_loss: 0.033899.... 0.075367 sec/batch\n",
      "epoch: 28/50.... training_step: 67650.... batch_loss: 0.037167.... 0.075256 sec/batch\n",
      "epoch: 28/50.... training_step: 67700.... batch_loss: 0.040129.... 0.073364 sec/batch\n",
      "epoch: 28/50.... training_step: 67750.... batch_loss: 0.042291.... 0.074880 sec/batch\n",
      "epoch: 28/50.... training_step: 67800.... batch_loss: 0.037546.... 0.074568 sec/batch\n",
      "epoch: 28/50.... training_step: 67850.... batch_loss: 0.038872.... 0.074143 sec/batch\n",
      "epoch: 28/50.... training_step: 67900.... batch_loss: 0.044986.... 0.073467 sec/batch\n",
      "epoch: 28/50.... training_step: 67950.... batch_loss: 0.039214.... 0.074655 sec/batch\n",
      "epoch: 28/50.... training_step: 68000.... batch_loss: 0.037145.... 0.075344 sec/batch\n",
      "epoch: 28/50.... training_step: 68050.... batch_loss: 0.040340.... 0.073604 sec/batch\n",
      "epoch: 28/50.... training_step: 68100.... batch_loss: 0.048982.... 0.074536 sec/batch\n",
      "epoch: 28/50.... training_step: 68150.... batch_loss: 0.040716.... 0.073937 sec/batch\n",
      "epoch: 28/50.... training_step: 68200.... batch_loss: 0.040627.... 0.074060 sec/batch\n",
      "epoch: 28/50.... training_step: 68250.... batch_loss: 0.042046.... 0.074829 sec/batch\n",
      "epoch: 28/50.... training_step: 68300.... batch_loss: 0.038607.... 0.073802 sec/batch\n",
      "epoch: 29/50.... training_step: 68350.... batch_loss: 0.034713.... 0.075419 sec/batch\n",
      "epoch: 29/50.... training_step: 68400.... batch_loss: 0.039271.... 0.073047 sec/batch\n",
      "epoch: 29/50.... training_step: 68450.... batch_loss: 0.042162.... 0.075568 sec/batch\n",
      "epoch: 29/50.... training_step: 68500.... batch_loss: 0.037389.... 0.073470 sec/batch\n",
      "epoch: 29/50.... training_step: 68550.... batch_loss: 0.039895.... 0.076739 sec/batch\n",
      "epoch: 29/50.... training_step: 68600.... batch_loss: 0.039344.... 0.073605 sec/batch\n",
      "epoch: 29/50.... training_step: 68650.... batch_loss: 0.035517.... 0.079291 sec/batch\n",
      "epoch: 29/50.... training_step: 68700.... batch_loss: 0.038330.... 0.073799 sec/batch\n",
      "epoch: 29/50.... training_step: 68750.... batch_loss: 0.038856.... 0.074249 sec/batch\n",
      "epoch: 29/50.... training_step: 68800.... batch_loss: 0.035031.... 0.076403 sec/batch\n",
      "epoch: 29/50.... training_step: 68850.... batch_loss: 0.043339.... 0.076993 sec/batch\n",
      "epoch: 29/50.... training_step: 68900.... batch_loss: 0.040646.... 0.073868 sec/batch\n",
      "epoch: 29/50.... training_step: 68950.... batch_loss: 0.037523.... 0.073891 sec/batch\n",
      "epoch: 29/50.... training_step: 69000.... batch_loss: 0.036758.... 0.073149 sec/batch\n",
      "epoch: 29/50.... training_step: 69050.... batch_loss: 0.041246.... 0.075671 sec/batch\n",
      "epoch: 29/50.... training_step: 69100.... batch_loss: 0.040771.... 0.076068 sec/batch\n",
      "epoch: 29/50.... training_step: 69150.... batch_loss: 0.037336.... 0.073984 sec/batch\n",
      "epoch: 29/50.... training_step: 69200.... batch_loss: 0.038736.... 0.073334 sec/batch\n",
      "epoch: 29/50.... training_step: 69250.... batch_loss: 0.040249.... 0.078189 sec/batch\n",
      "epoch: 29/50.... training_step: 69300.... batch_loss: 0.039009.... 0.075346 sec/batch\n",
      "epoch: 29/50.... training_step: 69350.... batch_loss: 0.039739.... 0.073594 sec/batch\n",
      "epoch: 29/50.... training_step: 69400.... batch_loss: 0.042234.... 0.074188 sec/batch\n",
      "epoch: 29/50.... training_step: 69450.... batch_loss: 0.039382.... 0.076150 sec/batch\n",
      "epoch: 29/50.... training_step: 69500.... batch_loss: 0.040213.... 0.073629 sec/batch\n",
      "epoch: 29/50.... training_step: 69550.... batch_loss: 0.041151.... 0.073993 sec/batch\n",
      "epoch: 29/50.... training_step: 69600.... batch_loss: 0.040136.... 0.073509 sec/batch\n",
      "epoch: 29/50.... training_step: 69650.... batch_loss: 0.042418.... 0.077621 sec/batch\n",
      "epoch: 29/50.... training_step: 69700.... batch_loss: 0.038352.... 0.074059 sec/batch\n",
      "epoch: 29/50.... training_step: 69750.... batch_loss: 0.036757.... 0.074445 sec/batch\n",
      "epoch: 29/50.... training_step: 69800.... batch_loss: 0.038845.... 0.072971 sec/batch\n",
      "epoch: 29/50.... training_step: 69850.... batch_loss: 0.040349.... 0.077224 sec/batch\n",
      "epoch: 29/50.... training_step: 69900.... batch_loss: 0.039852.... 0.073676 sec/batch\n",
      "epoch: 29/50.... training_step: 69950.... batch_loss: 0.038860.... 0.077450 sec/batch\n",
      "epoch: 29/50.... training_step: 70000.... batch_loss: 0.035467.... 0.077363 sec/batch\n",
      "epoch: 29/50.... training_step: 70050.... batch_loss: 0.039901.... 0.077219 sec/batch\n",
      "epoch: 29/50.... training_step: 70100.... batch_loss: 0.038740.... 0.072707 sec/batch\n",
      "epoch: 29/50.... training_step: 70150.... batch_loss: 0.038919.... 0.075641 sec/batch\n",
      "epoch: 29/50.... training_step: 70200.... batch_loss: 0.036846.... 0.074740 sec/batch\n",
      "epoch: 29/50.... training_step: 70250.... batch_loss: 0.041333.... 0.078547 sec/batch\n",
      "epoch: 29/50.... training_step: 70300.... batch_loss: 0.038170.... 0.073211 sec/batch\n",
      "epoch: 29/50.... training_step: 70350.... batch_loss: 0.036921.... 0.073744 sec/batch\n",
      "epoch: 29/50.... training_step: 70400.... batch_loss: 0.036978.... 0.077118 sec/batch\n",
      "epoch: 29/50.... training_step: 70450.... batch_loss: 0.039001.... 0.077876 sec/batch\n",
      "epoch: 29/50.... training_step: 70500.... batch_loss: 0.035979.... 0.074152 sec/batch\n",
      "epoch: 29/50.... training_step: 70550.... batch_loss: 0.040678.... 0.076027 sec/batch\n",
      "epoch: 29/50.... training_step: 70600.... batch_loss: 0.038676.... 0.075676 sec/batch\n",
      "epoch: 29/50.... training_step: 70650.... batch_loss: 0.039972.... 0.075218 sec/batch\n",
      "epoch: 30/50.... training_step: 70700.... batch_loss: 0.037552.... 0.072523 sec/batch\n",
      "epoch: 30/50.... training_step: 70750.... batch_loss: 0.038093.... 0.074771 sec/batch\n",
      "epoch: 30/50.... training_step: 70800.... batch_loss: 0.038839.... 0.073205 sec/batch\n",
      "epoch: 30/50.... training_step: 70850.... batch_loss: 0.047332.... 0.075315 sec/batch\n",
      "epoch: 30/50.... training_step: 70900.... batch_loss: 0.033020.... 0.074227 sec/batch\n",
      "epoch: 30/50.... training_step: 70950.... batch_loss: 0.036543.... 0.076651 sec/batch\n",
      "epoch: 30/50.... training_step: 71000.... batch_loss: 0.040280.... 0.073423 sec/batch\n",
      "epoch: 30/50.... training_step: 71050.... batch_loss: 0.043326.... 0.077600 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30/50.... training_step: 71100.... batch_loss: 0.040993.... 0.074284 sec/batch\n",
      "epoch: 30/50.... training_step: 71150.... batch_loss: 0.040659.... 0.076677 sec/batch\n",
      "epoch: 30/50.... training_step: 71200.... batch_loss: 0.036233.... 0.073035 sec/batch\n",
      "epoch: 30/50.... training_step: 71250.... batch_loss: 0.038003.... 0.077475 sec/batch\n",
      "epoch: 30/50.... training_step: 71300.... batch_loss: 0.039607.... 0.077609 sec/batch\n",
      "epoch: 30/50.... training_step: 71350.... batch_loss: 0.045132.... 0.076562 sec/batch\n",
      "epoch: 30/50.... training_step: 71400.... batch_loss: 0.032796.... 0.074478 sec/batch\n",
      "epoch: 30/50.... training_step: 71450.... batch_loss: 0.040862.... 0.078012 sec/batch\n",
      "epoch: 30/50.... training_step: 71500.... batch_loss: 0.038543.... 0.073243 sec/batch\n",
      "epoch: 30/50.... training_step: 71550.... batch_loss: 0.037934.... 0.075476 sec/batch\n",
      "epoch: 30/50.... training_step: 71600.... batch_loss: 0.038213.... 0.076115 sec/batch\n",
      "epoch: 30/50.... training_step: 71650.... batch_loss: 0.041777.... 0.077273 sec/batch\n",
      "epoch: 30/50.... training_step: 71700.... batch_loss: 0.035709.... 0.072823 sec/batch\n",
      "epoch: 30/50.... training_step: 71750.... batch_loss: 0.035825.... 0.074665 sec/batch\n",
      "epoch: 30/50.... training_step: 71800.... batch_loss: 0.037700.... 0.075159 sec/batch\n",
      "epoch: 30/50.... training_step: 71850.... batch_loss: 0.039547.... 0.079006 sec/batch\n",
      "epoch: 30/50.... training_step: 71900.... batch_loss: 0.038320.... 0.073498 sec/batch\n",
      "epoch: 30/50.... training_step: 71950.... batch_loss: 0.036996.... 0.076258 sec/batch\n",
      "epoch: 30/50.... training_step: 72000.... batch_loss: 0.039210.... 0.073331 sec/batch\n",
      "epoch: 30/50.... training_step: 72050.... batch_loss: 0.042823.... 0.074013 sec/batch\n",
      "epoch: 30/50.... training_step: 72100.... batch_loss: 0.040154.... 0.073838 sec/batch\n",
      "epoch: 30/50.... training_step: 72150.... batch_loss: 0.037414.... 0.074258 sec/batch\n",
      "epoch: 30/50.... training_step: 72200.... batch_loss: 0.038316.... 0.074057 sec/batch\n",
      "epoch: 30/50.... training_step: 72250.... batch_loss: 0.039076.... 0.075226 sec/batch\n",
      "epoch: 30/50.... training_step: 72300.... batch_loss: 0.038118.... 0.072288 sec/batch\n",
      "epoch: 30/50.... training_step: 72350.... batch_loss: 0.037628.... 0.075649 sec/batch\n",
      "epoch: 30/50.... training_step: 72400.... batch_loss: 0.038463.... 0.072782 sec/batch\n",
      "epoch: 30/50.... training_step: 72450.... batch_loss: 0.041113.... 0.075885 sec/batch\n",
      "epoch: 30/50.... training_step: 72500.... batch_loss: 0.036503.... 0.073806 sec/batch\n",
      "epoch: 30/50.... training_step: 72550.... batch_loss: 0.037476.... 0.076096 sec/batch\n",
      "epoch: 30/50.... training_step: 72600.... batch_loss: 0.033139.... 0.074405 sec/batch\n",
      "epoch: 30/50.... training_step: 72650.... batch_loss: 0.038847.... 0.077980 sec/batch\n",
      "epoch: 30/50.... training_step: 72700.... batch_loss: 0.036863.... 0.074908 sec/batch\n",
      "epoch: 30/50.... training_step: 72750.... batch_loss: 0.038705.... 0.076853 sec/batch\n",
      "epoch: 30/50.... training_step: 72800.... batch_loss: 0.037432.... 0.073015 sec/batch\n",
      "epoch: 30/50.... training_step: 72850.... batch_loss: 0.037673.... 0.076032 sec/batch\n",
      "epoch: 30/50.... training_step: 72900.... batch_loss: 0.037923.... 0.073312 sec/batch\n",
      "epoch: 30/50.... training_step: 72950.... batch_loss: 0.039386.... 0.074126 sec/batch\n",
      "epoch: 30/50.... training_step: 73000.... batch_loss: 0.042054.... 0.074829 sec/batch\n",
      "epoch: 31/50.... training_step: 73050.... batch_loss: 0.034826.... 0.076931 sec/batch\n",
      "epoch: 31/50.... training_step: 73100.... batch_loss: 0.037548.... 0.080462 sec/batch\n",
      "epoch: 31/50.... training_step: 73150.... batch_loss: 0.041082.... 0.073331 sec/batch\n",
      "epoch: 31/50.... training_step: 73200.... batch_loss: 0.044770.... 0.073797 sec/batch\n",
      "epoch: 31/50.... training_step: 73250.... batch_loss: 0.038926.... 0.072791 sec/batch\n",
      "epoch: 31/50.... training_step: 73300.... batch_loss: 0.041201.... 0.075606 sec/batch\n",
      "epoch: 31/50.... training_step: 73350.... batch_loss: 0.034626.... 0.075207 sec/batch\n",
      "epoch: 31/50.... training_step: 73400.... batch_loss: 0.044521.... 0.074543 sec/batch\n",
      "epoch: 31/50.... training_step: 73450.... batch_loss: 0.035360.... 0.074144 sec/batch\n",
      "epoch: 31/50.... training_step: 73500.... batch_loss: 0.037187.... 0.074472 sec/batch\n",
      "epoch: 31/50.... training_step: 73550.... batch_loss: 0.037813.... 0.072906 sec/batch\n",
      "epoch: 31/50.... training_step: 73600.... batch_loss: 0.040137.... 0.075072 sec/batch\n",
      "epoch: 31/50.... training_step: 73650.... batch_loss: 0.038287.... 0.074837 sec/batch\n",
      "epoch: 31/50.... training_step: 73700.... batch_loss: 0.033978.... 0.076692 sec/batch\n",
      "epoch: 31/50.... training_step: 73750.... batch_loss: 0.037654.... 0.073415 sec/batch\n",
      "epoch: 31/50.... training_step: 73800.... batch_loss: 0.036480.... 0.073376 sec/batch\n",
      "epoch: 31/50.... training_step: 73850.... batch_loss: 0.036206.... 0.073749 sec/batch\n",
      "epoch: 31/50.... training_step: 73900.... batch_loss: 0.034914.... 0.072909 sec/batch\n",
      "epoch: 31/50.... training_step: 73950.... batch_loss: 0.036784.... 0.073358 sec/batch\n",
      "epoch: 31/50.... training_step: 74000.... batch_loss: 0.039249.... 0.073566 sec/batch\n",
      "epoch: 31/50.... training_step: 74050.... batch_loss: 0.038347.... 0.073386 sec/batch\n",
      "epoch: 31/50.... training_step: 74100.... batch_loss: 0.044639.... 0.076347 sec/batch\n",
      "epoch: 31/50.... training_step: 74150.... batch_loss: 0.036259.... 0.076018 sec/batch\n",
      "epoch: 31/50.... training_step: 74200.... batch_loss: 0.036496.... 0.074076 sec/batch\n",
      "epoch: 31/50.... training_step: 74250.... batch_loss: 0.036133.... 0.076392 sec/batch\n",
      "epoch: 31/50.... training_step: 74300.... batch_loss: 0.035297.... 0.076206 sec/batch\n",
      "epoch: 31/50.... training_step: 74350.... batch_loss: 0.039434.... 0.073405 sec/batch\n",
      "epoch: 31/50.... training_step: 74400.... batch_loss: 0.034834.... 0.075196 sec/batch\n",
      "epoch: 31/50.... training_step: 74450.... batch_loss: 0.040036.... 0.073871 sec/batch\n",
      "epoch: 31/50.... training_step: 74500.... batch_loss: 0.042562.... 0.074049 sec/batch\n",
      "epoch: 31/50.... training_step: 74550.... batch_loss: 0.036195.... 0.075261 sec/batch\n",
      "epoch: 31/50.... training_step: 74600.... batch_loss: 0.037698.... 0.072418 sec/batch\n",
      "epoch: 31/50.... training_step: 74650.... batch_loss: 0.036526.... 0.075012 sec/batch\n",
      "epoch: 31/50.... training_step: 74700.... batch_loss: 0.037843.... 0.073300 sec/batch\n",
      "epoch: 31/50.... training_step: 74750.... batch_loss: 0.038302.... 0.073468 sec/batch\n",
      "epoch: 31/50.... training_step: 74800.... batch_loss: 0.036493.... 0.073356 sec/batch\n",
      "epoch: 31/50.... training_step: 74850.... batch_loss: 0.039616.... 0.073721 sec/batch\n",
      "epoch: 31/50.... training_step: 74900.... batch_loss: 0.032705.... 0.074855 sec/batch\n",
      "epoch: 31/50.... training_step: 74950.... batch_loss: 0.037711.... 0.074787 sec/batch\n",
      "epoch: 31/50.... training_step: 75000.... batch_loss: 0.036899.... 0.073386 sec/batch\n",
      "epoch: 31/50.... training_step: 75050.... batch_loss: 0.040298.... 0.078677 sec/batch\n",
      "epoch: 31/50.... training_step: 75100.... batch_loss: 0.035667.... 0.072659 sec/batch\n",
      "epoch: 31/50.... training_step: 75150.... batch_loss: 0.034529.... 0.076596 sec/batch\n",
      "epoch: 31/50.... training_step: 75200.... batch_loss: 0.036643.... 0.076199 sec/batch\n",
      "epoch: 31/50.... training_step: 75250.... batch_loss: 0.036159.... 0.074297 sec/batch\n",
      "epoch: 31/50.... training_step: 75300.... batch_loss: 0.040550.... 0.075677 sec/batch\n",
      "epoch: 31/50.... training_step: 75350.... batch_loss: 0.038491.... 0.076375 sec/batch\n",
      "epoch: 32/50.... training_step: 75400.... batch_loss: 0.036919.... 0.073418 sec/batch\n",
      "epoch: 32/50.... training_step: 75450.... batch_loss: 0.036207.... 0.088311 sec/batch\n",
      "epoch: 32/50.... training_step: 75500.... batch_loss: 0.041549.... 0.073497 sec/batch\n",
      "epoch: 32/50.... training_step: 75550.... batch_loss: 0.038172.... 0.074288 sec/batch\n",
      "epoch: 32/50.... training_step: 75600.... batch_loss: 0.038978.... 0.073140 sec/batch\n",
      "epoch: 32/50.... training_step: 75650.... batch_loss: 0.041888.... 0.073292 sec/batch\n",
      "epoch: 32/50.... training_step: 75700.... batch_loss: 0.037639.... 0.077822 sec/batch\n",
      "epoch: 32/50.... training_step: 75750.... batch_loss: 0.041102.... 0.077145 sec/batch\n",
      "epoch: 32/50.... training_step: 75800.... batch_loss: 0.040253.... 0.073467 sec/batch\n",
      "epoch: 32/50.... training_step: 75850.... batch_loss: 0.035189.... 0.075725 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32/50.... training_step: 75900.... batch_loss: 0.041469.... 0.073462 sec/batch\n",
      "epoch: 32/50.... training_step: 75950.... batch_loss: 0.035046.... 0.074352 sec/batch\n",
      "epoch: 32/50.... training_step: 76000.... batch_loss: 0.037985.... 0.074130 sec/batch\n",
      "epoch: 32/50.... training_step: 76050.... batch_loss: 0.036860.... 0.072294 sec/batch\n",
      "epoch: 32/50.... training_step: 76100.... batch_loss: 0.038851.... 0.075861 sec/batch\n",
      "epoch: 32/50.... training_step: 76150.... batch_loss: 0.034797.... 0.074288 sec/batch\n",
      "epoch: 32/50.... training_step: 76200.... batch_loss: 0.036568.... 0.073367 sec/batch\n",
      "epoch: 32/50.... training_step: 76250.... batch_loss: 0.035682.... 0.077486 sec/batch\n",
      "epoch: 32/50.... training_step: 76300.... batch_loss: 0.035870.... 0.076849 sec/batch\n",
      "epoch: 32/50.... training_step: 76350.... batch_loss: 0.037347.... 0.074378 sec/batch\n",
      "epoch: 32/50.... training_step: 76400.... batch_loss: 0.035515.... 0.073412 sec/batch\n",
      "epoch: 32/50.... training_step: 76450.... batch_loss: 0.038044.... 0.074542 sec/batch\n",
      "epoch: 32/50.... training_step: 76500.... batch_loss: 0.038011.... 0.074867 sec/batch\n",
      "epoch: 32/50.... training_step: 76550.... batch_loss: 0.035572.... 0.073961 sec/batch\n",
      "epoch: 32/50.... training_step: 76600.... batch_loss: 0.034231.... 0.073560 sec/batch\n",
      "epoch: 32/50.... training_step: 76650.... batch_loss: 0.037699.... 0.073165 sec/batch\n",
      "epoch: 32/50.... training_step: 76700.... batch_loss: 0.033525.... 0.073974 sec/batch\n",
      "epoch: 32/50.... training_step: 76750.... batch_loss: 0.033632.... 0.075285 sec/batch\n",
      "epoch: 32/50.... training_step: 76800.... batch_loss: 0.038474.... 0.074868 sec/batch\n",
      "epoch: 32/50.... training_step: 76850.... batch_loss: 0.037033.... 0.073469 sec/batch\n",
      "epoch: 32/50.... training_step: 76900.... batch_loss: 0.035978.... 0.075157 sec/batch\n",
      "epoch: 32/50.... training_step: 76950.... batch_loss: 0.036001.... 0.072697 sec/batch\n",
      "epoch: 32/50.... training_step: 77000.... batch_loss: 0.040632.... 0.073703 sec/batch\n",
      "epoch: 32/50.... training_step: 77050.... batch_loss: 0.036179.... 0.073369 sec/batch\n",
      "epoch: 32/50.... training_step: 77100.... batch_loss: 0.037173.... 0.078703 sec/batch\n",
      "epoch: 32/50.... training_step: 77150.... batch_loss: 0.036025.... 0.076864 sec/batch\n",
      "epoch: 32/50.... training_step: 77200.... batch_loss: 0.041368.... 0.074489 sec/batch\n",
      "epoch: 32/50.... training_step: 77250.... batch_loss: 0.035614.... 0.073097 sec/batch\n",
      "epoch: 32/50.... training_step: 77300.... batch_loss: 0.041306.... 0.076299 sec/batch\n",
      "epoch: 32/50.... training_step: 77350.... batch_loss: 0.040502.... 0.074600 sec/batch\n",
      "epoch: 32/50.... training_step: 77400.... batch_loss: 0.036996.... 0.076224 sec/batch\n",
      "epoch: 32/50.... training_step: 77450.... batch_loss: 0.037076.... 0.075844 sec/batch\n",
      "epoch: 32/50.... training_step: 77500.... batch_loss: 0.033044.... 0.083911 sec/batch\n",
      "epoch: 32/50.... training_step: 77550.... batch_loss: 0.037969.... 0.076503 sec/batch\n",
      "epoch: 32/50.... training_step: 77600.... batch_loss: 0.038322.... 0.073578 sec/batch\n",
      "epoch: 32/50.... training_step: 77650.... batch_loss: 0.037501.... 0.073236 sec/batch\n",
      "epoch: 32/50.... training_step: 77700.... batch_loss: 0.035210.... 0.074743 sec/batch\n",
      "epoch: 33/50.... training_step: 77750.... batch_loss: 0.036739.... 0.074628 sec/batch\n",
      "epoch: 33/50.... training_step: 77800.... batch_loss: 0.036721.... 0.075091 sec/batch\n",
      "epoch: 33/50.... training_step: 77850.... batch_loss: 0.035544.... 0.074075 sec/batch\n",
      "epoch: 33/50.... training_step: 77900.... batch_loss: 0.032925.... 0.074588 sec/batch\n",
      "epoch: 33/50.... training_step: 77950.... batch_loss: 0.036879.... 0.075356 sec/batch\n",
      "epoch: 33/50.... training_step: 78000.... batch_loss: 0.034626.... 0.076314 sec/batch\n",
      "epoch: 33/50.... training_step: 78050.... batch_loss: 0.038230.... 0.074562 sec/batch\n",
      "epoch: 33/50.... training_step: 78100.... batch_loss: 0.038550.... 0.074151 sec/batch\n",
      "epoch: 33/50.... training_step: 78150.... batch_loss: 0.034213.... 0.075151 sec/batch\n",
      "epoch: 33/50.... training_step: 78200.... batch_loss: 0.036988.... 0.074128 sec/batch\n",
      "epoch: 33/50.... training_step: 78250.... batch_loss: 0.037311.... 0.073053 sec/batch\n",
      "epoch: 33/50.... training_step: 78300.... batch_loss: 0.032388.... 0.076561 sec/batch\n",
      "epoch: 33/50.... training_step: 78350.... batch_loss: 0.036191.... 0.075248 sec/batch\n",
      "epoch: 33/50.... training_step: 78400.... batch_loss: 0.034757.... 0.074008 sec/batch\n",
      "epoch: 33/50.... training_step: 78450.... batch_loss: 0.033697.... 0.075638 sec/batch\n",
      "epoch: 33/50.... training_step: 78500.... batch_loss: 0.035857.... 0.073976 sec/batch\n",
      "epoch: 33/50.... training_step: 78550.... batch_loss: 0.036436.... 0.075039 sec/batch\n",
      "epoch: 33/50.... training_step: 78600.... batch_loss: 0.035382.... 0.074293 sec/batch\n",
      "epoch: 33/50.... training_step: 78650.... batch_loss: 0.035600.... 0.074330 sec/batch\n",
      "epoch: 33/50.... training_step: 78700.... batch_loss: 0.035043.... 0.082301 sec/batch\n",
      "epoch: 33/50.... training_step: 78750.... batch_loss: 0.040822.... 0.073462 sec/batch\n",
      "epoch: 33/50.... training_step: 78800.... batch_loss: 0.035341.... 0.075078 sec/batch\n",
      "epoch: 33/50.... training_step: 78850.... batch_loss: 0.038210.... 0.073777 sec/batch\n",
      "epoch: 33/50.... training_step: 78900.... batch_loss: 0.037603.... 0.073092 sec/batch\n",
      "epoch: 33/50.... training_step: 78950.... batch_loss: 0.047852.... 0.073882 sec/batch\n",
      "epoch: 33/50.... training_step: 79000.... batch_loss: 0.039043.... 0.073814 sec/batch\n",
      "epoch: 33/50.... training_step: 79050.... batch_loss: 0.032328.... 0.073336 sec/batch\n",
      "epoch: 33/50.... training_step: 79100.... batch_loss: 0.038599.... 0.076123 sec/batch\n",
      "epoch: 33/50.... training_step: 79150.... batch_loss: 0.041842.... 0.073521 sec/batch\n",
      "epoch: 33/50.... training_step: 79200.... batch_loss: 0.037430.... 0.074860 sec/batch\n",
      "epoch: 33/50.... training_step: 79250.... batch_loss: 0.035841.... 0.077553 sec/batch\n",
      "epoch: 33/50.... training_step: 79300.... batch_loss: 0.037859.... 0.075539 sec/batch\n",
      "epoch: 33/50.... training_step: 79350.... batch_loss: 0.042006.... 0.075267 sec/batch\n",
      "epoch: 33/50.... training_step: 79400.... batch_loss: 0.038379.... 0.073451 sec/batch\n",
      "epoch: 33/50.... training_step: 79450.... batch_loss: 0.037125.... 0.075366 sec/batch\n",
      "epoch: 33/50.... training_step: 79500.... batch_loss: 0.032356.... 0.076341 sec/batch\n",
      "epoch: 33/50.... training_step: 79550.... batch_loss: 0.033520.... 0.076337 sec/batch\n",
      "epoch: 33/50.... training_step: 79600.... batch_loss: 0.037198.... 0.077507 sec/batch\n",
      "epoch: 33/50.... training_step: 79650.... batch_loss: 0.036325.... 0.076428 sec/batch\n",
      "epoch: 33/50.... training_step: 79700.... batch_loss: 0.034768.... 0.073141 sec/batch\n",
      "epoch: 33/50.... training_step: 79750.... batch_loss: 0.041627.... 0.074825 sec/batch\n",
      "epoch: 33/50.... training_step: 79800.... batch_loss: 0.038139.... 0.076629 sec/batch\n",
      "epoch: 33/50.... training_step: 79850.... batch_loss: 0.036695.... 0.073748 sec/batch\n",
      "epoch: 33/50.... training_step: 79900.... batch_loss: 0.031597.... 0.076783 sec/batch\n",
      "epoch: 33/50.... training_step: 79950.... batch_loss: 0.042022.... 0.075364 sec/batch\n",
      "epoch: 33/50.... training_step: 80000.... batch_loss: 0.038674.... 0.075433 sec/batch\n",
      "epoch: 33/50.... training_step: 80050.... batch_loss: 0.042546.... 0.076142 sec/batch\n",
      "epoch: 33/50.... training_step: 80100.... batch_loss: 0.035890.... 0.073658 sec/batch\n",
      "epoch: 34/50.... training_step: 80150.... batch_loss: 0.033670.... 0.076121 sec/batch\n",
      "epoch: 34/50.... training_step: 80200.... batch_loss: 0.037030.... 0.073633 sec/batch\n",
      "epoch: 34/50.... training_step: 80250.... batch_loss: 0.040567.... 0.076633 sec/batch\n",
      "epoch: 34/50.... training_step: 80300.... batch_loss: 0.037158.... 0.075015 sec/batch\n",
      "epoch: 34/50.... training_step: 80350.... batch_loss: 0.036668.... 0.072902 sec/batch\n",
      "epoch: 34/50.... training_step: 80400.... batch_loss: 0.035112.... 0.075253 sec/batch\n",
      "epoch: 34/50.... training_step: 80450.... batch_loss: 0.034204.... 0.073400 sec/batch\n",
      "epoch: 34/50.... training_step: 80500.... batch_loss: 0.033916.... 0.072758 sec/batch\n",
      "epoch: 34/50.... training_step: 80550.... batch_loss: 0.042517.... 0.077333 sec/batch\n",
      "epoch: 34/50.... training_step: 80600.... batch_loss: 0.038614.... 0.073223 sec/batch\n",
      "epoch: 34/50.... training_step: 80650.... batch_loss: 0.033825.... 0.073935 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34/50.... training_step: 80700.... batch_loss: 0.036184.... 0.073645 sec/batch\n",
      "epoch: 34/50.... training_step: 80750.... batch_loss: 0.036172.... 0.076340 sec/batch\n",
      "epoch: 34/50.... training_step: 80800.... batch_loss: 0.038740.... 0.074037 sec/batch\n",
      "epoch: 34/50.... training_step: 80850.... batch_loss: 0.034462.... 0.074844 sec/batch\n",
      "epoch: 34/50.... training_step: 80900.... batch_loss: 0.038718.... 0.073836 sec/batch\n",
      "epoch: 34/50.... training_step: 80950.... batch_loss: 0.033605.... 0.074105 sec/batch\n",
      "epoch: 34/50.... training_step: 81000.... batch_loss: 0.034614.... 0.074298 sec/batch\n",
      "epoch: 34/50.... training_step: 81050.... batch_loss: 0.036754.... 0.073842 sec/batch\n",
      "epoch: 34/50.... training_step: 81100.... batch_loss: 0.033510.... 0.078079 sec/batch\n",
      "epoch: 34/50.... training_step: 81150.... batch_loss: 0.032505.... 0.074210 sec/batch\n",
      "epoch: 34/50.... training_step: 81200.... batch_loss: 0.035203.... 0.073123 sec/batch\n",
      "epoch: 34/50.... training_step: 81250.... batch_loss: 0.038549.... 0.075876 sec/batch\n",
      "epoch: 34/50.... training_step: 81300.... batch_loss: 0.036507.... 0.075414 sec/batch\n",
      "epoch: 34/50.... training_step: 81350.... batch_loss: 0.036190.... 0.075615 sec/batch\n",
      "epoch: 34/50.... training_step: 81400.... batch_loss: 0.040308.... 0.073257 sec/batch\n",
      "epoch: 34/50.... training_step: 81450.... batch_loss: 0.034794.... 0.078763 sec/batch\n",
      "epoch: 34/50.... training_step: 81500.... batch_loss: 0.035691.... 0.072895 sec/batch\n",
      "epoch: 34/50.... training_step: 81550.... batch_loss: 0.038580.... 0.078519 sec/batch\n",
      "epoch: 34/50.... training_step: 81600.... batch_loss: 0.040747.... 0.073708 sec/batch\n",
      "epoch: 34/50.... training_step: 81650.... batch_loss: 0.034958.... 0.075552 sec/batch\n",
      "epoch: 34/50.... training_step: 81700.... batch_loss: 0.036091.... 0.075001 sec/batch\n",
      "epoch: 34/50.... training_step: 81750.... batch_loss: 0.031024.... 0.075093 sec/batch\n",
      "epoch: 34/50.... training_step: 81800.... batch_loss: 0.031703.... 0.075565 sec/batch\n",
      "epoch: 34/50.... training_step: 81850.... batch_loss: 0.035537.... 0.074730 sec/batch\n",
      "epoch: 34/50.... training_step: 81900.... batch_loss: 0.038135.... 0.073671 sec/batch\n",
      "epoch: 34/50.... training_step: 81950.... batch_loss: 0.034696.... 0.074526 sec/batch\n",
      "epoch: 34/50.... training_step: 82000.... batch_loss: 0.033986.... 0.074120 sec/batch\n",
      "epoch: 34/50.... training_step: 82050.... batch_loss: 0.041428.... 0.074268 sec/batch\n",
      "epoch: 34/50.... training_step: 82100.... batch_loss: 0.038079.... 0.075202 sec/batch\n",
      "epoch: 34/50.... training_step: 82150.... batch_loss: 0.036837.... 0.075872 sec/batch\n",
      "epoch: 34/50.... training_step: 82200.... batch_loss: 0.039149.... 0.073012 sec/batch\n",
      "epoch: 34/50.... training_step: 82250.... batch_loss: 0.038859.... 0.075964 sec/batch\n",
      "epoch: 34/50.... training_step: 82300.... batch_loss: 0.034746.... 0.077308 sec/batch\n",
      "epoch: 34/50.... training_step: 82350.... batch_loss: 0.037936.... 0.077522 sec/batch\n",
      "epoch: 34/50.... training_step: 82400.... batch_loss: 0.039638.... 0.076081 sec/batch\n",
      "epoch: 34/50.... training_step: 82450.... batch_loss: 0.033180.... 0.075756 sec/batch\n",
      "epoch: 35/50.... training_step: 82500.... batch_loss: 0.038202.... 0.074816 sec/batch\n",
      "epoch: 35/50.... training_step: 82550.... batch_loss: 0.037827.... 0.075999 sec/batch\n",
      "epoch: 35/50.... training_step: 82600.... batch_loss: 0.036193.... 0.073045 sec/batch\n",
      "epoch: 35/50.... training_step: 82650.... batch_loss: 0.036658.... 0.075020 sec/batch\n",
      "epoch: 35/50.... training_step: 82700.... batch_loss: 0.040492.... 0.076709 sec/batch\n",
      "epoch: 35/50.... training_step: 82750.... batch_loss: 0.038473.... 0.075202 sec/batch\n",
      "epoch: 35/50.... training_step: 82800.... batch_loss: 0.039735.... 0.077315 sec/batch\n",
      "epoch: 35/50.... training_step: 82850.... batch_loss: 0.035904.... 0.074866 sec/batch\n",
      "epoch: 35/50.... training_step: 82900.... batch_loss: 0.036722.... 0.073701 sec/batch\n",
      "epoch: 35/50.... training_step: 82950.... batch_loss: 0.034081.... 0.079087 sec/batch\n",
      "epoch: 35/50.... training_step: 83000.... batch_loss: 0.036046.... 0.074177 sec/batch\n",
      "epoch: 35/50.... training_step: 83050.... batch_loss: 0.039093.... 0.074851 sec/batch\n",
      "epoch: 35/50.... training_step: 83100.... batch_loss: 0.035755.... 0.076644 sec/batch\n",
      "epoch: 35/50.... training_step: 83150.... batch_loss: 0.033038.... 0.073999 sec/batch\n",
      "epoch: 35/50.... training_step: 83200.... batch_loss: 0.034656.... 0.073022 sec/batch\n",
      "epoch: 35/50.... training_step: 83250.... batch_loss: 0.036759.... 0.073567 sec/batch\n",
      "epoch: 35/50.... training_step: 83300.... batch_loss: 0.037076.... 0.073655 sec/batch\n",
      "epoch: 35/50.... training_step: 83350.... batch_loss: 0.036921.... 0.074629 sec/batch\n",
      "epoch: 35/50.... training_step: 83400.... batch_loss: 0.035376.... 0.072594 sec/batch\n",
      "epoch: 35/50.... training_step: 83450.... batch_loss: 0.038540.... 0.076812 sec/batch\n",
      "epoch: 35/50.... training_step: 83500.... batch_loss: 0.034715.... 0.075275 sec/batch\n",
      "epoch: 35/50.... training_step: 83550.... batch_loss: 0.034047.... 0.076205 sec/batch\n",
      "epoch: 35/50.... training_step: 83600.... batch_loss: 0.039849.... 0.073794 sec/batch\n",
      "epoch: 35/50.... training_step: 83650.... batch_loss: 0.035736.... 0.078210 sec/batch\n",
      "epoch: 35/50.... training_step: 83700.... batch_loss: 0.035688.... 0.072804 sec/batch\n",
      "epoch: 35/50.... training_step: 83750.... batch_loss: 0.035910.... 0.073982 sec/batch\n",
      "epoch: 35/50.... training_step: 83800.... batch_loss: 0.035484.... 0.074612 sec/batch\n",
      "epoch: 35/50.... training_step: 83850.... batch_loss: 0.034699.... 0.078670 sec/batch\n",
      "epoch: 35/50.... training_step: 83900.... batch_loss: 0.037729.... 0.084229 sec/batch\n",
      "epoch: 35/50.... training_step: 83950.... batch_loss: 0.036048.... 0.075043 sec/batch\n",
      "epoch: 35/50.... training_step: 84000.... batch_loss: 0.037596.... 0.074317 sec/batch\n",
      "epoch: 35/50.... training_step: 84050.... batch_loss: 0.033044.... 0.073957 sec/batch\n",
      "epoch: 35/50.... training_step: 84100.... batch_loss: 0.036580.... 0.073104 sec/batch\n",
      "epoch: 35/50.... training_step: 84150.... batch_loss: 0.037914.... 0.074536 sec/batch\n",
      "epoch: 35/50.... training_step: 84200.... batch_loss: 0.035367.... 0.073222 sec/batch\n",
      "epoch: 35/50.... training_step: 84250.... batch_loss: 0.033876.... 0.075395 sec/batch\n",
      "epoch: 35/50.... training_step: 84300.... batch_loss: 0.036360.... 0.075314 sec/batch\n",
      "epoch: 35/50.... training_step: 84350.... batch_loss: 0.034352.... 0.077387 sec/batch\n",
      "epoch: 35/50.... training_step: 84400.... batch_loss: 0.040704.... 0.075488 sec/batch\n",
      "epoch: 35/50.... training_step: 84450.... batch_loss: 0.033825.... 0.075413 sec/batch\n",
      "epoch: 35/50.... training_step: 84500.... batch_loss: 0.038144.... 0.073664 sec/batch\n",
      "epoch: 35/50.... training_step: 84550.... batch_loss: 0.036080.... 0.076180 sec/batch\n",
      "epoch: 35/50.... training_step: 84600.... batch_loss: 0.031566.... 0.073907 sec/batch\n",
      "epoch: 35/50.... training_step: 84650.... batch_loss: 0.034004.... 0.075264 sec/batch\n",
      "epoch: 35/50.... training_step: 84700.... batch_loss: 0.038036.... 0.072855 sec/batch\n",
      "epoch: 35/50.... training_step: 84750.... batch_loss: 0.035258.... 0.075477 sec/batch\n",
      "epoch: 35/50.... training_step: 84800.... batch_loss: 0.037018.... 0.073861 sec/batch\n",
      "epoch: 36/50.... training_step: 84850.... batch_loss: 0.035844.... 0.074941 sec/batch\n",
      "epoch: 36/50.... training_step: 84900.... batch_loss: 0.036506.... 0.073514 sec/batch\n",
      "epoch: 36/50.... training_step: 84950.... batch_loss: 0.032395.... 0.079310 sec/batch\n",
      "epoch: 36/50.... training_step: 85000.... batch_loss: 0.037743.... 0.076787 sec/batch\n",
      "epoch: 36/50.... training_step: 85050.... batch_loss: 0.035554.... 0.074512 sec/batch\n",
      "epoch: 36/50.... training_step: 85100.... batch_loss: 0.033865.... 0.073758 sec/batch\n",
      "epoch: 36/50.... training_step: 85150.... batch_loss: 0.039486.... 0.076798 sec/batch\n",
      "epoch: 36/50.... training_step: 85200.... batch_loss: 0.033514.... 0.072922 sec/batch\n",
      "epoch: 36/50.... training_step: 85250.... batch_loss: 0.037106.... 0.074424 sec/batch\n",
      "epoch: 36/50.... training_step: 85300.... batch_loss: 0.039367.... 0.073935 sec/batch\n",
      "epoch: 36/50.... training_step: 85350.... batch_loss: 0.031785.... 0.083416 sec/batch\n",
      "epoch: 36/50.... training_step: 85400.... batch_loss: 0.036248.... 0.075851 sec/batch\n",
      "epoch: 36/50.... training_step: 85450.... batch_loss: 0.033801.... 0.075286 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36/50.... training_step: 85500.... batch_loss: 0.035663.... 0.073264 sec/batch\n",
      "epoch: 36/50.... training_step: 85550.... batch_loss: 0.037608.... 0.077729 sec/batch\n",
      "epoch: 36/50.... training_step: 85600.... batch_loss: 0.038852.... 0.075567 sec/batch\n",
      "epoch: 36/50.... training_step: 85650.... batch_loss: 0.035339.... 0.075180 sec/batch\n",
      "epoch: 36/50.... training_step: 85700.... batch_loss: 0.038523.... 0.075126 sec/batch\n",
      "epoch: 36/50.... training_step: 85750.... batch_loss: 0.035338.... 0.073975 sec/batch\n",
      "epoch: 36/50.... training_step: 85800.... batch_loss: 0.036785.... 0.075341 sec/batch\n",
      "epoch: 36/50.... training_step: 85850.... batch_loss: 0.034818.... 0.074316 sec/batch\n",
      "epoch: 36/50.... training_step: 85900.... batch_loss: 0.038334.... 0.073119 sec/batch\n",
      "epoch: 36/50.... training_step: 85950.... batch_loss: 0.036922.... 0.076256 sec/batch\n",
      "epoch: 36/50.... training_step: 86000.... batch_loss: 0.038624.... 0.073849 sec/batch\n",
      "epoch: 36/50.... training_step: 86050.... batch_loss: 0.033443.... 0.073784 sec/batch\n",
      "epoch: 36/50.... training_step: 86100.... batch_loss: 0.038846.... 0.076768 sec/batch\n",
      "epoch: 36/50.... training_step: 86150.... batch_loss: 0.035934.... 0.075620 sec/batch\n",
      "epoch: 36/50.... training_step: 86200.... batch_loss: 0.032034.... 0.072923 sec/batch\n",
      "epoch: 36/50.... training_step: 86250.... batch_loss: 0.042321.... 0.074796 sec/batch\n",
      "epoch: 36/50.... training_step: 86300.... batch_loss: 0.038463.... 0.075124 sec/batch\n",
      "epoch: 36/50.... training_step: 86350.... batch_loss: 0.037782.... 0.073660 sec/batch\n",
      "epoch: 36/50.... training_step: 86400.... batch_loss: 0.036046.... 0.076154 sec/batch\n",
      "epoch: 36/50.... training_step: 86450.... batch_loss: 0.031888.... 0.074073 sec/batch\n",
      "epoch: 36/50.... training_step: 86500.... batch_loss: 0.039501.... 0.073994 sec/batch\n",
      "epoch: 36/50.... training_step: 86550.... batch_loss: 0.036801.... 0.074193 sec/batch\n",
      "epoch: 36/50.... training_step: 86600.... batch_loss: 0.034718.... 0.074413 sec/batch\n",
      "epoch: 36/50.... training_step: 86650.... batch_loss: 0.036276.... 0.072515 sec/batch\n",
      "epoch: 36/50.... training_step: 86700.... batch_loss: 0.033891.... 0.073895 sec/batch\n",
      "epoch: 36/50.... training_step: 86750.... batch_loss: 0.034517.... 0.073107 sec/batch\n",
      "epoch: 36/50.... training_step: 86800.... batch_loss: 0.039992.... 0.076686 sec/batch\n",
      "epoch: 36/50.... training_step: 86850.... batch_loss: 0.034164.... 0.075309 sec/batch\n",
      "epoch: 36/50.... training_step: 86900.... batch_loss: 0.031056.... 0.073870 sec/batch\n",
      "epoch: 36/50.... training_step: 86950.... batch_loss: 0.035913.... 0.077493 sec/batch\n",
      "epoch: 36/50.... training_step: 87000.... batch_loss: 0.035521.... 0.075110 sec/batch\n",
      "epoch: 36/50.... training_step: 87050.... batch_loss: 0.038168.... 0.073317 sec/batch\n",
      "epoch: 36/50.... training_step: 87100.... batch_loss: 0.033422.... 0.073028 sec/batch\n",
      "epoch: 36/50.... training_step: 87150.... batch_loss: 0.032901.... 0.074807 sec/batch\n",
      "epoch: 37/50.... training_step: 87200.... batch_loss: 0.034578.... 0.073327 sec/batch\n",
      "epoch: 37/50.... training_step: 87250.... batch_loss: 0.034503.... 0.073330 sec/batch\n",
      "epoch: 37/50.... training_step: 87300.... batch_loss: 0.038106.... 0.075605 sec/batch\n",
      "epoch: 37/50.... training_step: 87350.... batch_loss: 0.034886.... 0.075751 sec/batch\n",
      "epoch: 37/50.... training_step: 87400.... batch_loss: 0.033087.... 0.074845 sec/batch\n",
      "epoch: 37/50.... training_step: 87450.... batch_loss: 0.034102.... 0.076616 sec/batch\n",
      "epoch: 37/50.... training_step: 87500.... batch_loss: 0.035086.... 0.077963 sec/batch\n",
      "epoch: 37/50.... training_step: 87550.... batch_loss: 0.034462.... 0.076538 sec/batch\n",
      "epoch: 37/50.... training_step: 87600.... batch_loss: 0.036145.... 0.074598 sec/batch\n",
      "epoch: 37/50.... training_step: 87650.... batch_loss: 0.037325.... 0.077188 sec/batch\n",
      "epoch: 37/50.... training_step: 87700.... batch_loss: 0.034967.... 0.078317 sec/batch\n",
      "epoch: 37/50.... training_step: 87750.... batch_loss: 0.033314.... 0.074805 sec/batch\n",
      "epoch: 37/50.... training_step: 87800.... batch_loss: 0.036365.... 0.076665 sec/batch\n",
      "epoch: 37/50.... training_step: 87850.... batch_loss: 0.033564.... 0.073519 sec/batch\n",
      "epoch: 37/50.... training_step: 87900.... batch_loss: 0.038325.... 0.079726 sec/batch\n",
      "epoch: 37/50.... training_step: 87950.... batch_loss: 0.035515.... 0.073198 sec/batch\n",
      "epoch: 37/50.... training_step: 88000.... batch_loss: 0.036061.... 0.073770 sec/batch\n",
      "epoch: 37/50.... training_step: 88050.... batch_loss: 0.037610.... 0.073266 sec/batch\n",
      "epoch: 37/50.... training_step: 88100.... batch_loss: 0.035953.... 0.074097 sec/batch\n",
      "epoch: 37/50.... training_step: 88150.... batch_loss: 0.038753.... 0.078465 sec/batch\n",
      "epoch: 37/50.... training_step: 88200.... batch_loss: 0.033963.... 0.077172 sec/batch\n",
      "epoch: 37/50.... training_step: 88250.... batch_loss: 0.036553.... 0.076000 sec/batch\n",
      "epoch: 37/50.... training_step: 88300.... batch_loss: 0.031118.... 0.076030 sec/batch\n",
      "epoch: 37/50.... training_step: 88350.... batch_loss: 0.035673.... 0.079329 sec/batch\n",
      "epoch: 37/50.... training_step: 88400.... batch_loss: 0.035183.... 0.075564 sec/batch\n",
      "epoch: 37/50.... training_step: 88450.... batch_loss: 0.036263.... 0.075578 sec/batch\n",
      "epoch: 37/50.... training_step: 88500.... batch_loss: 0.034884.... 0.073555 sec/batch\n",
      "epoch: 37/50.... training_step: 88550.... batch_loss: 0.030335.... 0.076533 sec/batch\n",
      "epoch: 37/50.... training_step: 88600.... batch_loss: 0.038633.... 0.074458 sec/batch\n",
      "epoch: 37/50.... training_step: 88650.... batch_loss: 0.036770.... 0.076034 sec/batch\n",
      "epoch: 37/50.... training_step: 88700.... batch_loss: 0.034718.... 0.075924 sec/batch\n",
      "epoch: 37/50.... training_step: 88750.... batch_loss: 0.036729.... 0.075491 sec/batch\n",
      "epoch: 37/50.... training_step: 88800.... batch_loss: 0.033406.... 0.073097 sec/batch\n",
      "epoch: 37/50.... training_step: 88850.... batch_loss: 0.039431.... 0.073715 sec/batch\n",
      "epoch: 37/50.... training_step: 88900.... batch_loss: 0.040114.... 0.075188 sec/batch\n",
      "epoch: 37/50.... training_step: 88950.... batch_loss: 0.036283.... 0.077776 sec/batch\n",
      "epoch: 37/50.... training_step: 89000.... batch_loss: 0.035173.... 0.073815 sec/batch\n",
      "epoch: 37/50.... training_step: 89050.... batch_loss: 0.035618.... 0.073395 sec/batch\n",
      "epoch: 37/50.... training_step: 89100.... batch_loss: 0.035173.... 0.075086 sec/batch\n",
      "epoch: 37/50.... training_step: 89150.... batch_loss: 0.030490.... 0.072729 sec/batch\n",
      "epoch: 37/50.... training_step: 89200.... batch_loss: 0.035641.... 0.073596 sec/batch\n",
      "epoch: 37/50.... training_step: 89250.... batch_loss: 0.033294.... 0.074168 sec/batch\n",
      "epoch: 37/50.... training_step: 89300.... batch_loss: 0.034767.... 0.073594 sec/batch\n",
      "epoch: 37/50.... training_step: 89350.... batch_loss: 0.034823.... 0.079500 sec/batch\n",
      "epoch: 37/50.... training_step: 89400.... batch_loss: 0.034565.... 0.074497 sec/batch\n",
      "epoch: 37/50.... training_step: 89450.... batch_loss: 0.035129.... 0.073684 sec/batch\n",
      "epoch: 37/50.... training_step: 89500.... batch_loss: 0.033283.... 0.076412 sec/batch\n",
      "epoch: 38/50.... training_step: 89550.... batch_loss: 0.035086.... 0.074709 sec/batch\n",
      "epoch: 38/50.... training_step: 89600.... batch_loss: 0.035186.... 0.077465 sec/batch\n",
      "epoch: 38/50.... training_step: 89650.... batch_loss: 0.032688.... 0.076099 sec/batch\n",
      "epoch: 38/50.... training_step: 89700.... batch_loss: 0.035062.... 0.073531 sec/batch\n",
      "epoch: 38/50.... training_step: 89750.... batch_loss: 0.033010.... 0.074972 sec/batch\n",
      "epoch: 38/50.... training_step: 89800.... batch_loss: 0.034335.... 0.075563 sec/batch\n",
      "epoch: 38/50.... training_step: 89850.... batch_loss: 0.033698.... 0.073317 sec/batch\n",
      "epoch: 38/50.... training_step: 89900.... batch_loss: 0.035768.... 0.075622 sec/batch\n",
      "epoch: 38/50.... training_step: 89950.... batch_loss: 0.037118.... 0.073067 sec/batch\n",
      "epoch: 38/50.... training_step: 90000.... batch_loss: 0.036377.... 0.074133 sec/batch\n",
      "epoch: 38/50.... training_step: 90050.... batch_loss: 0.034723.... 0.073572 sec/batch\n",
      "epoch: 38/50.... training_step: 90100.... batch_loss: 0.032678.... 0.073116 sec/batch\n",
      "epoch: 38/50.... training_step: 90150.... batch_loss: 0.037483.... 0.079698 sec/batch\n",
      "epoch: 38/50.... training_step: 90200.... batch_loss: 0.036083.... 0.074290 sec/batch\n",
      "epoch: 38/50.... training_step: 90250.... batch_loss: 0.039825.... 0.074874 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38/50.... training_step: 90300.... batch_loss: 0.038331.... 0.076056 sec/batch\n",
      "epoch: 38/50.... training_step: 90350.... batch_loss: 0.038111.... 0.073110 sec/batch\n",
      "epoch: 38/50.... training_step: 90400.... batch_loss: 0.034419.... 0.073435 sec/batch\n",
      "epoch: 38/50.... training_step: 90450.... batch_loss: 0.033007.... 0.074487 sec/batch\n",
      "epoch: 38/50.... training_step: 90500.... batch_loss: 0.036558.... 0.073136 sec/batch\n",
      "epoch: 38/50.... training_step: 90550.... batch_loss: 0.033719.... 0.073368 sec/batch\n",
      "epoch: 38/50.... training_step: 90600.... batch_loss: 0.034114.... 0.075236 sec/batch\n",
      "epoch: 38/50.... training_step: 90650.... batch_loss: 0.038744.... 0.078050 sec/batch\n",
      "epoch: 38/50.... training_step: 90700.... batch_loss: 0.035664.... 0.075488 sec/batch\n",
      "epoch: 38/50.... training_step: 90750.... batch_loss: 0.037458.... 0.073134 sec/batch\n",
      "epoch: 38/50.... training_step: 90800.... batch_loss: 0.030836.... 0.073328 sec/batch\n",
      "epoch: 38/50.... training_step: 90850.... batch_loss: 0.034372.... 0.073004 sec/batch\n",
      "epoch: 38/50.... training_step: 90900.... batch_loss: 0.030690.... 0.074376 sec/batch\n",
      "epoch: 38/50.... training_step: 90950.... batch_loss: 0.033741.... 0.075865 sec/batch\n",
      "epoch: 38/50.... training_step: 91000.... batch_loss: 0.034764.... 0.074733 sec/batch\n",
      "epoch: 38/50.... training_step: 91050.... batch_loss: 0.034180.... 0.076216 sec/batch\n",
      "epoch: 38/50.... training_step: 91100.... batch_loss: 0.034251.... 0.072767 sec/batch\n",
      "epoch: 38/50.... training_step: 91150.... batch_loss: 0.034471.... 0.078022 sec/batch\n",
      "epoch: 38/50.... training_step: 91200.... batch_loss: 0.036100.... 0.073267 sec/batch\n",
      "epoch: 38/50.... training_step: 91250.... batch_loss: 0.032968.... 0.076460 sec/batch\n",
      "epoch: 38/50.... training_step: 91300.... batch_loss: 0.039854.... 0.073466 sec/batch\n",
      "epoch: 38/50.... training_step: 91350.... batch_loss: 0.035104.... 0.077798 sec/batch\n",
      "epoch: 38/50.... training_step: 91400.... batch_loss: 0.035400.... 0.074509 sec/batch\n",
      "epoch: 38/50.... training_step: 91450.... batch_loss: 0.032703.... 0.073439 sec/batch\n",
      "epoch: 38/50.... training_step: 91500.... batch_loss: 0.034010.... 0.071666 sec/batch\n",
      "epoch: 38/50.... training_step: 91550.... batch_loss: 0.039394.... 0.075006 sec/batch\n",
      "epoch: 38/50.... training_step: 91600.... batch_loss: 0.035567.... 0.073190 sec/batch\n",
      "epoch: 38/50.... training_step: 91650.... batch_loss: 0.036229.... 0.075609 sec/batch\n",
      "epoch: 38/50.... training_step: 91700.... batch_loss: 0.036938.... 0.072876 sec/batch\n",
      "epoch: 38/50.... training_step: 91750.... batch_loss: 0.030938.... 0.074585 sec/batch\n",
      "epoch: 38/50.... training_step: 91800.... batch_loss: 0.036459.... 0.073821 sec/batch\n",
      "epoch: 38/50.... training_step: 91850.... batch_loss: 0.037165.... 0.074179 sec/batch\n",
      "epoch: 39/50.... training_step: 91900.... batch_loss: 0.032095.... 0.073067 sec/batch\n",
      "epoch: 39/50.... training_step: 91950.... batch_loss: 0.036120.... 0.075667 sec/batch\n",
      "epoch: 39/50.... training_step: 92000.... batch_loss: 0.036791.... 0.076170 sec/batch\n",
      "epoch: 39/50.... training_step: 92050.... batch_loss: 0.035638.... 0.073627 sec/batch\n",
      "epoch: 39/50.... training_step: 92100.... batch_loss: 0.035684.... 0.076945 sec/batch\n",
      "epoch: 39/50.... training_step: 92150.... batch_loss: 0.035313.... 0.075531 sec/batch\n",
      "epoch: 39/50.... training_step: 92200.... batch_loss: 0.032640.... 0.073424 sec/batch\n",
      "epoch: 39/50.... training_step: 92250.... batch_loss: 0.034843.... 0.075042 sec/batch\n",
      "epoch: 39/50.... training_step: 92300.... batch_loss: 0.036604.... 0.074937 sec/batch\n",
      "epoch: 39/50.... training_step: 92350.... batch_loss: 0.037313.... 0.073248 sec/batch\n",
      "epoch: 39/50.... training_step: 92400.... batch_loss: 0.037547.... 0.073520 sec/batch\n",
      "epoch: 39/50.... training_step: 92450.... batch_loss: 0.037058.... 0.075028 sec/batch\n",
      "epoch: 39/50.... training_step: 92500.... batch_loss: 0.034665.... 0.075706 sec/batch\n",
      "epoch: 39/50.... training_step: 92550.... batch_loss: 0.034973.... 0.075889 sec/batch\n",
      "epoch: 39/50.... training_step: 92600.... batch_loss: 0.032169.... 0.074179 sec/batch\n",
      "epoch: 39/50.... training_step: 92650.... batch_loss: 0.035382.... 0.077070 sec/batch\n",
      "epoch: 39/50.... training_step: 92700.... batch_loss: 0.041100.... 0.073663 sec/batch\n",
      "epoch: 39/50.... training_step: 92750.... batch_loss: 0.036946.... 0.073165 sec/batch\n",
      "epoch: 39/50.... training_step: 92800.... batch_loss: 0.032949.... 0.077835 sec/batch\n",
      "epoch: 39/50.... training_step: 92850.... batch_loss: 0.032273.... 0.071748 sec/batch\n",
      "epoch: 39/50.... training_step: 92900.... batch_loss: 0.033540.... 0.074705 sec/batch\n",
      "epoch: 39/50.... training_step: 92950.... batch_loss: 0.032125.... 0.076547 sec/batch\n",
      "epoch: 39/50.... training_step: 93000.... batch_loss: 0.035240.... 0.078996 sec/batch\n",
      "epoch: 39/50.... training_step: 93050.... batch_loss: 0.032614.... 0.073491 sec/batch\n",
      "epoch: 39/50.... training_step: 93100.... batch_loss: 0.035468.... 0.074045 sec/batch\n",
      "epoch: 39/50.... training_step: 93150.... batch_loss: 0.034619.... 0.073236 sec/batch\n",
      "epoch: 39/50.... training_step: 93200.... batch_loss: 0.034644.... 0.073750 sec/batch\n",
      "epoch: 39/50.... training_step: 93250.... batch_loss: 0.035176.... 0.075886 sec/batch\n",
      "epoch: 39/50.... training_step: 93300.... batch_loss: 0.034557.... 0.073395 sec/batch\n",
      "epoch: 39/50.... training_step: 93350.... batch_loss: 0.034712.... 0.074030 sec/batch\n",
      "epoch: 39/50.... training_step: 93400.... batch_loss: 0.033778.... 0.073205 sec/batch\n",
      "epoch: 39/50.... training_step: 93450.... batch_loss: 0.034297.... 0.075040 sec/batch\n",
      "epoch: 39/50.... training_step: 93500.... batch_loss: 0.043437.... 0.073797 sec/batch\n",
      "epoch: 39/50.... training_step: 93550.... batch_loss: 0.033439.... 0.074519 sec/batch\n",
      "epoch: 39/50.... training_step: 93600.... batch_loss: 0.036515.... 0.073317 sec/batch\n",
      "epoch: 39/50.... training_step: 93650.... batch_loss: 0.029245.... 0.081280 sec/batch\n",
      "epoch: 39/50.... training_step: 93700.... batch_loss: 0.032120.... 0.074299 sec/batch\n",
      "epoch: 39/50.... training_step: 93750.... batch_loss: 0.034273.... 0.075210 sec/batch\n",
      "epoch: 39/50.... training_step: 93800.... batch_loss: 0.034710.... 0.075810 sec/batch\n",
      "epoch: 39/50.... training_step: 93850.... batch_loss: 0.035308.... 0.076343 sec/batch\n",
      "epoch: 39/50.... training_step: 93900.... batch_loss: 0.034171.... 0.074005 sec/batch\n",
      "epoch: 39/50.... training_step: 93950.... batch_loss: 0.038537.... 0.075316 sec/batch\n",
      "epoch: 39/50.... training_step: 94000.... batch_loss: 0.036407.... 0.072715 sec/batch\n",
      "epoch: 39/50.... training_step: 94050.... batch_loss: 0.035672.... 0.073242 sec/batch\n",
      "epoch: 39/50.... training_step: 94100.... batch_loss: 0.034092.... 0.074030 sec/batch\n",
      "epoch: 39/50.... training_step: 94150.... batch_loss: 0.034312.... 0.076663 sec/batch\n",
      "epoch: 39/50.... training_step: 94200.... batch_loss: 0.031844.... 0.073963 sec/batch\n",
      "epoch: 40/50.... training_step: 94250.... batch_loss: 0.034526.... 0.076147 sec/batch\n",
      "epoch: 40/50.... training_step: 94300.... batch_loss: 0.033250.... 0.075283 sec/batch\n",
      "epoch: 40/50.... training_step: 94350.... batch_loss: 0.038299.... 0.073901 sec/batch\n",
      "epoch: 40/50.... training_step: 94400.... batch_loss: 0.035870.... 0.075442 sec/batch\n",
      "epoch: 40/50.... training_step: 94450.... batch_loss: 0.035324.... 0.075134 sec/batch\n",
      "epoch: 40/50.... training_step: 94500.... batch_loss: 0.038097.... 0.073931 sec/batch\n",
      "epoch: 40/50.... training_step: 94550.... batch_loss: 0.034646.... 0.074611 sec/batch\n",
      "epoch: 40/50.... training_step: 94600.... batch_loss: 0.035857.... 0.073983 sec/batch\n",
      "epoch: 40/50.... training_step: 94650.... batch_loss: 0.035825.... 0.072811 sec/batch\n",
      "epoch: 40/50.... training_step: 94700.... batch_loss: 0.037639.... 0.074267 sec/batch\n",
      "epoch: 40/50.... training_step: 94750.... batch_loss: 0.034392.... 0.073685 sec/batch\n",
      "epoch: 40/50.... training_step: 94800.... batch_loss: 0.034963.... 0.078975 sec/batch\n",
      "epoch: 40/50.... training_step: 94850.... batch_loss: 0.034397.... 0.073347 sec/batch\n",
      "epoch: 40/50.... training_step: 94900.... batch_loss: 0.036120.... 0.074378 sec/batch\n",
      "epoch: 40/50.... training_step: 94950.... batch_loss: 0.034296.... 0.072546 sec/batch\n",
      "epoch: 40/50.... training_step: 95000.... batch_loss: 0.036587.... 0.073119 sec/batch\n",
      "epoch: 40/50.... training_step: 95050.... batch_loss: 0.035616.... 0.072877 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40/50.... training_step: 95100.... batch_loss: 0.034551.... 0.073378 sec/batch\n",
      "epoch: 40/50.... training_step: 95150.... batch_loss: 0.035356.... 0.072033 sec/batch\n",
      "epoch: 40/50.... training_step: 95200.... batch_loss: 0.033269.... 0.072800 sec/batch\n",
      "epoch: 40/50.... training_step: 95250.... batch_loss: 0.032442.... 0.072550 sec/batch\n",
      "epoch: 40/50.... training_step: 95300.... batch_loss: 0.034458.... 0.074020 sec/batch\n",
      "epoch: 40/50.... training_step: 95350.... batch_loss: 0.033402.... 0.073351 sec/batch\n",
      "epoch: 40/50.... training_step: 95400.... batch_loss: 0.038175.... 0.074482 sec/batch\n",
      "epoch: 40/50.... training_step: 95450.... batch_loss: 0.039199.... 0.074046 sec/batch\n",
      "epoch: 40/50.... training_step: 95500.... batch_loss: 0.033626.... 0.073413 sec/batch\n",
      "epoch: 40/50.... training_step: 95550.... batch_loss: 0.036615.... 0.073663 sec/batch\n",
      "epoch: 40/50.... training_step: 95600.... batch_loss: 0.032166.... 0.075767 sec/batch\n",
      "epoch: 40/50.... training_step: 95650.... batch_loss: 0.040885.... 0.075404 sec/batch\n",
      "epoch: 40/50.... training_step: 95700.... batch_loss: 0.035490.... 0.072306 sec/batch\n",
      "epoch: 40/50.... training_step: 95750.... batch_loss: 0.034593.... 0.073615 sec/batch\n",
      "epoch: 40/50.... training_step: 95800.... batch_loss: 0.037448.... 0.072521 sec/batch\n",
      "epoch: 40/50.... training_step: 95850.... batch_loss: 0.034319.... 0.072811 sec/batch\n",
      "epoch: 40/50.... training_step: 95900.... batch_loss: 0.034828.... 0.074049 sec/batch\n",
      "epoch: 40/50.... training_step: 95950.... batch_loss: 0.033760.... 0.073348 sec/batch\n",
      "epoch: 40/50.... training_step: 96000.... batch_loss: 0.036657.... 0.077012 sec/batch\n",
      "epoch: 40/50.... training_step: 96050.... batch_loss: 0.030730.... 0.076234 sec/batch\n",
      "epoch: 40/50.... training_step: 96100.... batch_loss: 0.033043.... 0.074774 sec/batch\n",
      "epoch: 40/50.... training_step: 96150.... batch_loss: 0.033235.... 0.074027 sec/batch\n",
      "epoch: 40/50.... training_step: 96200.... batch_loss: 0.034758.... 0.073034 sec/batch\n",
      "epoch: 40/50.... training_step: 96250.... batch_loss: 0.033748.... 0.074274 sec/batch\n",
      "epoch: 40/50.... training_step: 96300.... batch_loss: 0.031499.... 0.073249 sec/batch\n",
      "epoch: 40/50.... training_step: 96350.... batch_loss: 0.036781.... 0.076311 sec/batch\n",
      "epoch: 40/50.... training_step: 96400.... batch_loss: 0.031792.... 0.073210 sec/batch\n",
      "epoch: 40/50.... training_step: 96450.... batch_loss: 0.034126.... 0.073896 sec/batch\n",
      "epoch: 40/50.... training_step: 96500.... batch_loss: 0.034349.... 0.076260 sec/batch\n",
      "epoch: 40/50.... training_step: 96550.... batch_loss: 0.032361.... 0.076358 sec/batch\n",
      "epoch: 41/50.... training_step: 96600.... batch_loss: 0.035836.... 0.075832 sec/batch\n",
      "epoch: 41/50.... training_step: 96650.... batch_loss: 0.031250.... 0.073990 sec/batch\n",
      "epoch: 41/50.... training_step: 96700.... batch_loss: 0.036630.... 0.073635 sec/batch\n",
      "epoch: 41/50.... training_step: 96750.... batch_loss: 0.035148.... 0.073597 sec/batch\n",
      "epoch: 41/50.... training_step: 96800.... batch_loss: 0.035943.... 0.078470 sec/batch\n",
      "epoch: 41/50.... training_step: 96850.... batch_loss: 0.032637.... 0.073831 sec/batch\n",
      "epoch: 41/50.... training_step: 96900.... batch_loss: 0.031300.... 0.073096 sec/batch\n",
      "epoch: 41/50.... training_step: 96950.... batch_loss: 0.036276.... 0.075822 sec/batch\n",
      "epoch: 41/50.... training_step: 97000.... batch_loss: 0.033614.... 0.074366 sec/batch\n",
      "epoch: 41/50.... training_step: 97050.... batch_loss: 0.032493.... 0.074714 sec/batch\n",
      "epoch: 41/50.... training_step: 97100.... batch_loss: 0.033812.... 0.079814 sec/batch\n",
      "epoch: 41/50.... training_step: 97150.... batch_loss: 0.036275.... 0.073339 sec/batch\n",
      "epoch: 41/50.... training_step: 97200.... batch_loss: 0.043955.... 0.073325 sec/batch\n",
      "epoch: 41/50.... training_step: 97250.... batch_loss: 0.030984.... 0.073761 sec/batch\n",
      "epoch: 41/50.... training_step: 97300.... batch_loss: 0.033886.... 0.074054 sec/batch\n",
      "epoch: 41/50.... training_step: 97350.... batch_loss: 0.032078.... 0.075715 sec/batch\n",
      "epoch: 41/50.... training_step: 97400.... batch_loss: 0.029049.... 0.074484 sec/batch\n",
      "epoch: 41/50.... training_step: 97450.... batch_loss: 0.034127.... 0.073336 sec/batch\n",
      "epoch: 41/50.... training_step: 97500.... batch_loss: 0.034424.... 0.072756 sec/batch\n",
      "epoch: 41/50.... training_step: 97550.... batch_loss: 0.032588.... 0.073659 sec/batch\n",
      "epoch: 41/50.... training_step: 97600.... batch_loss: 0.031230.... 0.075686 sec/batch\n",
      "epoch: 41/50.... training_step: 97650.... batch_loss: 0.034404.... 0.073686 sec/batch\n",
      "epoch: 41/50.... training_step: 97700.... batch_loss: 0.032267.... 0.073764 sec/batch\n",
      "epoch: 41/50.... training_step: 97750.... batch_loss: 0.033301.... 0.074190 sec/batch\n",
      "epoch: 41/50.... training_step: 97800.... batch_loss: 0.033190.... 0.075880 sec/batch\n",
      "epoch: 41/50.... training_step: 97850.... batch_loss: 0.032432.... 0.074424 sec/batch\n",
      "epoch: 41/50.... training_step: 97900.... batch_loss: 0.032569.... 0.073979 sec/batch\n",
      "epoch: 41/50.... training_step: 97950.... batch_loss: 0.035943.... 0.072719 sec/batch\n",
      "epoch: 41/50.... training_step: 98000.... batch_loss: 0.035069.... 0.075093 sec/batch\n",
      "epoch: 41/50.... training_step: 98050.... batch_loss: 0.032155.... 0.076177 sec/batch\n",
      "epoch: 41/50.... training_step: 98100.... batch_loss: 0.033031.... 0.073318 sec/batch\n",
      "epoch: 41/50.... training_step: 98150.... batch_loss: 0.033767.... 0.077357 sec/batch\n",
      "epoch: 41/50.... training_step: 98200.... batch_loss: 0.035636.... 0.077505 sec/batch\n",
      "epoch: 41/50.... training_step: 98250.... batch_loss: 0.034665.... 0.075992 sec/batch\n",
      "epoch: 41/50.... training_step: 98300.... batch_loss: 0.033669.... 0.074313 sec/batch\n",
      "epoch: 41/50.... training_step: 98350.... batch_loss: 0.030663.... 0.074369 sec/batch\n",
      "epoch: 41/50.... training_step: 98400.... batch_loss: 0.034998.... 0.073983 sec/batch\n",
      "epoch: 41/50.... training_step: 98450.... batch_loss: 0.031232.... 0.073414 sec/batch\n",
      "epoch: 41/50.... training_step: 98500.... batch_loss: 0.033990.... 0.073564 sec/batch\n",
      "epoch: 41/50.... training_step: 98550.... batch_loss: 0.032660.... 0.073535 sec/batch\n",
      "epoch: 41/50.... training_step: 98600.... batch_loss: 0.032526.... 0.074063 sec/batch\n",
      "epoch: 41/50.... training_step: 98650.... batch_loss: 0.031745.... 0.074420 sec/batch\n",
      "epoch: 41/50.... training_step: 98700.... batch_loss: 0.038149.... 0.073267 sec/batch\n",
      "epoch: 41/50.... training_step: 98750.... batch_loss: 0.032507.... 0.076911 sec/batch\n",
      "epoch: 41/50.... training_step: 98800.... batch_loss: 0.033417.... 0.074400 sec/batch\n",
      "epoch: 41/50.... training_step: 98850.... batch_loss: 0.033661.... 0.076457 sec/batch\n",
      "epoch: 41/50.... training_step: 98900.... batch_loss: 0.033372.... 0.073045 sec/batch\n",
      "epoch: 41/50.... training_step: 98950.... batch_loss: 0.035891.... 0.075099 sec/batch\n",
      "epoch: 42/50.... training_step: 99000.... batch_loss: 0.033614.... 0.073630 sec/batch\n",
      "epoch: 42/50.... training_step: 99050.... batch_loss: 0.034248.... 0.074980 sec/batch\n",
      "epoch: 42/50.... training_step: 99100.... batch_loss: 0.035680.... 0.072557 sec/batch\n",
      "epoch: 42/50.... training_step: 99150.... batch_loss: 0.033586.... 0.073927 sec/batch\n",
      "epoch: 42/50.... training_step: 99200.... batch_loss: 0.033818.... 0.079491 sec/batch\n",
      "epoch: 42/50.... training_step: 99250.... batch_loss: 0.035611.... 0.073455 sec/batch\n",
      "epoch: 42/50.... training_step: 99300.... batch_loss: 0.032963.... 0.076720 sec/batch\n",
      "epoch: 42/50.... training_step: 99350.... batch_loss: 0.034197.... 0.073223 sec/batch\n",
      "epoch: 42/50.... training_step: 99400.... batch_loss: 0.031007.... 0.073825 sec/batch\n",
      "epoch: 42/50.... training_step: 99450.... batch_loss: 0.033900.... 0.075928 sec/batch\n",
      "epoch: 42/50.... training_step: 99500.... batch_loss: 0.035061.... 0.075020 sec/batch\n",
      "epoch: 42/50.... training_step: 99550.... batch_loss: 0.036939.... 0.074973 sec/batch\n",
      "epoch: 42/50.... training_step: 99600.... batch_loss: 0.033130.... 0.073676 sec/batch\n",
      "epoch: 42/50.... training_step: 99650.... batch_loss: 0.031519.... 0.076147 sec/batch\n",
      "epoch: 42/50.... training_step: 99700.... batch_loss: 0.034636.... 0.073430 sec/batch\n",
      "epoch: 42/50.... training_step: 99750.... batch_loss: 0.032500.... 0.075778 sec/batch\n",
      "epoch: 42/50.... training_step: 99800.... batch_loss: 0.034761.... 0.072600 sec/batch\n",
      "epoch: 42/50.... training_step: 99850.... batch_loss: 0.036564.... 0.076294 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42/50.... training_step: 99900.... batch_loss: 0.030858.... 0.076844 sec/batch\n",
      "epoch: 42/50.... training_step: 99950.... batch_loss: 0.031823.... 0.074184 sec/batch\n",
      "epoch: 42/50.... training_step: 100000.... batch_loss: 0.034319.... 0.074231 sec/batch\n",
      "epoch: 42/50.... training_step: 100050.... batch_loss: 0.034435.... 0.073301 sec/batch\n",
      "epoch: 42/50.... training_step: 100100.... batch_loss: 0.032176.... 0.073374 sec/batch\n",
      "epoch: 42/50.... training_step: 100150.... batch_loss: 0.031230.... 0.074684 sec/batch\n",
      "epoch: 42/50.... training_step: 100200.... batch_loss: 0.034777.... 0.073848 sec/batch\n",
      "epoch: 42/50.... training_step: 100250.... batch_loss: 0.034075.... 0.074507 sec/batch\n",
      "epoch: 42/50.... training_step: 100300.... batch_loss: 0.036852.... 0.074157 sec/batch\n",
      "epoch: 42/50.... training_step: 100350.... batch_loss: 0.035040.... 0.075316 sec/batch\n",
      "epoch: 42/50.... training_step: 100400.... batch_loss: 0.031183.... 0.076047 sec/batch\n",
      "epoch: 42/50.... training_step: 100450.... batch_loss: 0.029850.... 0.076418 sec/batch\n",
      "epoch: 42/50.... training_step: 100500.... batch_loss: 0.033089.... 0.077075 sec/batch\n",
      "epoch: 42/50.... training_step: 100550.... batch_loss: 0.034641.... 0.074763 sec/batch\n",
      "epoch: 42/50.... training_step: 100600.... batch_loss: 0.037749.... 0.072459 sec/batch\n",
      "epoch: 42/50.... training_step: 100650.... batch_loss: 0.033085.... 0.077322 sec/batch\n",
      "epoch: 42/50.... training_step: 100700.... batch_loss: 0.036810.... 0.074467 sec/batch\n",
      "epoch: 42/50.... training_step: 100750.... batch_loss: 0.032195.... 0.076573 sec/batch\n",
      "epoch: 42/50.... training_step: 100800.... batch_loss: 0.033322.... 0.073501 sec/batch\n",
      "epoch: 42/50.... training_step: 100850.... batch_loss: 0.034746.... 0.074794 sec/batch\n",
      "epoch: 42/50.... training_step: 100900.... batch_loss: 0.032989.... 0.074356 sec/batch\n",
      "epoch: 42/50.... training_step: 100950.... batch_loss: 0.033862.... 0.075211 sec/batch\n",
      "epoch: 42/50.... training_step: 101000.... batch_loss: 0.035186.... 0.074440 sec/batch\n",
      "epoch: 42/50.... training_step: 101050.... batch_loss: 0.033483.... 0.078710 sec/batch\n",
      "epoch: 42/50.... training_step: 101100.... batch_loss: 0.035740.... 0.073523 sec/batch\n",
      "epoch: 42/50.... training_step: 101150.... batch_loss: 0.033192.... 0.074903 sec/batch\n",
      "epoch: 42/50.... training_step: 101200.... batch_loss: 0.037438.... 0.072800 sec/batch\n",
      "epoch: 42/50.... training_step: 101250.... batch_loss: 0.034858.... 0.078245 sec/batch\n",
      "epoch: 42/50.... training_step: 101300.... batch_loss: 0.032677.... 0.077940 sec/batch\n",
      "epoch: 43/50.... training_step: 101350.... batch_loss: 0.032938.... 0.075175 sec/batch\n",
      "epoch: 43/50.... training_step: 101400.... batch_loss: 0.033172.... 0.074749 sec/batch\n",
      "epoch: 43/50.... training_step: 101450.... batch_loss: 0.027666.... 0.075533 sec/batch\n",
      "epoch: 43/50.... training_step: 101500.... batch_loss: 0.030816.... 0.073706 sec/batch\n",
      "epoch: 43/50.... training_step: 101550.... batch_loss: 0.033273.... 0.074802 sec/batch\n",
      "epoch: 43/50.... training_step: 101600.... batch_loss: 0.032518.... 0.073653 sec/batch\n",
      "epoch: 43/50.... training_step: 101650.... batch_loss: 0.034810.... 0.074326 sec/batch\n",
      "epoch: 43/50.... training_step: 101700.... batch_loss: 0.031054.... 0.073177 sec/batch\n",
      "epoch: 43/50.... training_step: 101750.... batch_loss: 0.033479.... 0.074730 sec/batch\n",
      "epoch: 43/50.... training_step: 101800.... batch_loss: 0.030902.... 0.075195 sec/batch\n",
      "epoch: 43/50.... training_step: 101850.... batch_loss: 0.032956.... 0.075268 sec/batch\n",
      "epoch: 43/50.... training_step: 101900.... batch_loss: 0.035141.... 0.072990 sec/batch\n",
      "epoch: 43/50.... training_step: 101950.... batch_loss: 0.030844.... 0.074445 sec/batch\n",
      "epoch: 43/50.... training_step: 102000.... batch_loss: 0.033688.... 0.076280 sec/batch\n",
      "epoch: 43/50.... training_step: 102050.... batch_loss: 0.030918.... 0.074673 sec/batch\n",
      "epoch: 43/50.... training_step: 102100.... batch_loss: 0.035274.... 0.074159 sec/batch\n",
      "epoch: 43/50.... training_step: 102150.... batch_loss: 0.031646.... 0.073563 sec/batch\n",
      "epoch: 43/50.... training_step: 102200.... batch_loss: 0.036457.... 0.076341 sec/batch\n",
      "epoch: 43/50.... training_step: 102250.... batch_loss: 0.035790.... 0.076763 sec/batch\n",
      "epoch: 43/50.... training_step: 102300.... batch_loss: 0.038262.... 0.073108 sec/batch\n",
      "epoch: 43/50.... training_step: 102350.... batch_loss: 0.033834.... 0.074864 sec/batch\n",
      "epoch: 43/50.... training_step: 102400.... batch_loss: 0.032402.... 0.073201 sec/batch\n",
      "epoch: 43/50.... training_step: 102450.... batch_loss: 0.030389.... 0.071867 sec/batch\n",
      "epoch: 43/50.... training_step: 102500.... batch_loss: 0.034782.... 0.072327 sec/batch\n",
      "epoch: 43/50.... training_step: 102550.... batch_loss: 0.034288.... 0.072849 sec/batch\n",
      "epoch: 43/50.... training_step: 102600.... batch_loss: 0.033919.... 0.074576 sec/batch\n",
      "epoch: 43/50.... training_step: 102650.... batch_loss: 0.040832.... 0.073848 sec/batch\n",
      "epoch: 43/50.... training_step: 102700.... batch_loss: 0.033890.... 0.073961 sec/batch\n",
      "epoch: 43/50.... training_step: 102750.... batch_loss: 0.033173.... 0.073601 sec/batch\n",
      "epoch: 43/50.... training_step: 102800.... batch_loss: 0.032062.... 0.074136 sec/batch\n",
      "epoch: 43/50.... training_step: 102850.... batch_loss: 0.033083.... 0.075216 sec/batch\n",
      "epoch: 43/50.... training_step: 102900.... batch_loss: 0.031538.... 0.073224 sec/batch\n",
      "epoch: 43/50.... training_step: 102950.... batch_loss: 0.035455.... 0.072620 sec/batch\n",
      "epoch: 43/50.... training_step: 103000.... batch_loss: 0.030363.... 0.074131 sec/batch\n",
      "epoch: 43/50.... training_step: 103050.... batch_loss: 0.035037.... 0.075800 sec/batch\n",
      "epoch: 43/50.... training_step: 103100.... batch_loss: 0.033478.... 0.077565 sec/batch\n",
      "epoch: 43/50.... training_step: 103150.... batch_loss: 0.033833.... 0.073648 sec/batch\n",
      "epoch: 43/50.... training_step: 103200.... batch_loss: 0.032880.... 0.074134 sec/batch\n",
      "epoch: 43/50.... training_step: 103250.... batch_loss: 0.032765.... 0.073672 sec/batch\n",
      "epoch: 43/50.... training_step: 103300.... batch_loss: 0.036828.... 0.074666 sec/batch\n",
      "epoch: 43/50.... training_step: 103350.... batch_loss: 0.034999.... 0.073959 sec/batch\n",
      "epoch: 43/50.... training_step: 103400.... batch_loss: 0.033621.... 0.073128 sec/batch\n",
      "epoch: 43/50.... training_step: 103450.... batch_loss: 0.034078.... 0.073893 sec/batch\n",
      "epoch: 43/50.... training_step: 103500.... batch_loss: 0.034545.... 0.075786 sec/batch\n",
      "epoch: 43/50.... training_step: 103550.... batch_loss: 0.032971.... 0.075579 sec/batch\n",
      "epoch: 43/50.... training_step: 103600.... batch_loss: 0.029321.... 0.075550 sec/batch\n",
      "epoch: 43/50.... training_step: 103650.... batch_loss: 0.037666.... 0.076318 sec/batch\n",
      "epoch: 44/50.... training_step: 103700.... batch_loss: 0.033803.... 0.072036 sec/batch\n",
      "epoch: 44/50.... training_step: 103750.... batch_loss: 0.033725.... 0.074875 sec/batch\n",
      "epoch: 44/50.... training_step: 103800.... batch_loss: 0.030349.... 0.078179 sec/batch\n",
      "epoch: 44/50.... training_step: 103850.... batch_loss: 0.037844.... 0.073373 sec/batch\n",
      "epoch: 44/50.... training_step: 103900.... batch_loss: 0.032797.... 0.075575 sec/batch\n",
      "epoch: 44/50.... training_step: 103950.... batch_loss: 0.032269.... 0.076858 sec/batch\n",
      "epoch: 44/50.... training_step: 104000.... batch_loss: 0.033338.... 0.076888 sec/batch\n",
      "epoch: 44/50.... training_step: 104050.... batch_loss: 0.031147.... 0.073847 sec/batch\n",
      "epoch: 44/50.... training_step: 104100.... batch_loss: 0.031634.... 0.075119 sec/batch\n",
      "epoch: 44/50.... training_step: 104150.... batch_loss: 0.031814.... 0.075969 sec/batch\n",
      "epoch: 44/50.... training_step: 104200.... batch_loss: 0.032256.... 0.073839 sec/batch\n",
      "epoch: 44/50.... training_step: 104250.... batch_loss: 0.034945.... 0.075512 sec/batch\n",
      "epoch: 44/50.... training_step: 104300.... batch_loss: 0.034327.... 0.075692 sec/batch\n",
      "epoch: 44/50.... training_step: 104350.... batch_loss: 0.033800.... 0.075728 sec/batch\n",
      "epoch: 44/50.... training_step: 104400.... batch_loss: 0.029957.... 0.072908 sec/batch\n",
      "epoch: 44/50.... training_step: 104450.... batch_loss: 0.036817.... 0.076160 sec/batch\n",
      "epoch: 44/50.... training_step: 104500.... batch_loss: 0.033640.... 0.073972 sec/batch\n",
      "epoch: 44/50.... training_step: 104550.... batch_loss: 0.036208.... 0.075307 sec/batch\n",
      "epoch: 44/50.... training_step: 104600.... batch_loss: 0.031323.... 0.074223 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44/50.... training_step: 104650.... batch_loss: 0.031787.... 0.073763 sec/batch\n",
      "epoch: 44/50.... training_step: 104700.... batch_loss: 0.036475.... 0.073618 sec/batch\n",
      "epoch: 44/50.... training_step: 104750.... batch_loss: 0.036205.... 0.077019 sec/batch\n",
      "epoch: 44/50.... training_step: 104800.... batch_loss: 0.033372.... 0.076663 sec/batch\n",
      "epoch: 44/50.... training_step: 104850.... batch_loss: 0.032681.... 0.075496 sec/batch\n",
      "epoch: 44/50.... training_step: 104900.... batch_loss: 0.031614.... 0.076243 sec/batch\n",
      "epoch: 44/50.... training_step: 104950.... batch_loss: 0.035477.... 0.075949 sec/batch\n",
      "epoch: 44/50.... training_step: 105000.... batch_loss: 0.031934.... 0.072458 sec/batch\n",
      "epoch: 44/50.... training_step: 105050.... batch_loss: 0.033461.... 0.076384 sec/batch\n",
      "epoch: 44/50.... training_step: 105100.... batch_loss: 0.033401.... 0.073419 sec/batch\n",
      "epoch: 44/50.... training_step: 105150.... batch_loss: 0.033113.... 0.072737 sec/batch\n",
      "epoch: 44/50.... training_step: 105200.... batch_loss: 0.030250.... 0.074721 sec/batch\n",
      "epoch: 44/50.... training_step: 105250.... batch_loss: 0.032548.... 0.074522 sec/batch\n",
      "epoch: 44/50.... training_step: 105300.... batch_loss: 0.035886.... 0.074796 sec/batch\n",
      "epoch: 44/50.... training_step: 105350.... batch_loss: 0.031398.... 0.076649 sec/batch\n",
      "epoch: 44/50.... training_step: 105400.... batch_loss: 0.033295.... 0.075100 sec/batch\n",
      "epoch: 44/50.... training_step: 105450.... batch_loss: 0.033160.... 0.074077 sec/batch\n",
      "epoch: 44/50.... training_step: 105500.... batch_loss: 0.032353.... 0.082365 sec/batch\n",
      "epoch: 44/50.... training_step: 105550.... batch_loss: 0.034350.... 0.072408 sec/batch\n",
      "epoch: 44/50.... training_step: 105600.... batch_loss: 0.034582.... 0.073789 sec/batch\n",
      "epoch: 44/50.... training_step: 105650.... batch_loss: 0.030198.... 0.072913 sec/batch\n",
      "epoch: 44/50.... training_step: 105700.... batch_loss: 0.033601.... 0.073634 sec/batch\n",
      "epoch: 44/50.... training_step: 105750.... batch_loss: 0.033977.... 0.074266 sec/batch\n",
      "epoch: 44/50.... training_step: 105800.... batch_loss: 0.031664.... 0.076741 sec/batch\n",
      "epoch: 44/50.... training_step: 105850.... batch_loss: 0.036859.... 0.075479 sec/batch\n",
      "epoch: 44/50.... training_step: 105900.... batch_loss: 0.034414.... 0.073224 sec/batch\n",
      "epoch: 44/50.... training_step: 105950.... batch_loss: 0.031543.... 0.075368 sec/batch\n",
      "epoch: 44/50.... training_step: 106000.... batch_loss: 0.031592.... 0.072991 sec/batch\n",
      "epoch: 45/50.... training_step: 106050.... batch_loss: 0.032995.... 0.074247 sec/batch\n",
      "epoch: 45/50.... training_step: 106100.... batch_loss: 0.032041.... 0.075922 sec/batch\n",
      "epoch: 45/50.... training_step: 106150.... batch_loss: 0.031646.... 0.074664 sec/batch\n",
      "epoch: 45/50.... training_step: 106200.... batch_loss: 0.027859.... 0.073373 sec/batch\n",
      "epoch: 45/50.... training_step: 106250.... batch_loss: 0.035744.... 0.074098 sec/batch\n",
      "epoch: 45/50.... training_step: 106300.... batch_loss: 0.033044.... 0.074863 sec/batch\n",
      "epoch: 45/50.... training_step: 106350.... batch_loss: 0.035078.... 0.073502 sec/batch\n",
      "epoch: 45/50.... training_step: 106400.... batch_loss: 0.038779.... 0.073591 sec/batch\n",
      "epoch: 45/50.... training_step: 106450.... batch_loss: 0.031634.... 0.073192 sec/batch\n",
      "epoch: 45/50.... training_step: 106500.... batch_loss: 0.031204.... 0.071972 sec/batch\n",
      "epoch: 45/50.... training_step: 106550.... batch_loss: 0.034630.... 0.076854 sec/batch\n",
      "epoch: 45/50.... training_step: 106600.... batch_loss: 0.035028.... 0.073638 sec/batch\n",
      "epoch: 45/50.... training_step: 106650.... batch_loss: 0.031588.... 0.073363 sec/batch\n",
      "epoch: 45/50.... training_step: 106700.... batch_loss: 0.033188.... 0.075824 sec/batch\n",
      "epoch: 45/50.... training_step: 106750.... batch_loss: 0.031962.... 0.072698 sec/batch\n",
      "epoch: 45/50.... training_step: 106800.... batch_loss: 0.032636.... 0.076520 sec/batch\n",
      "epoch: 45/50.... training_step: 106850.... batch_loss: 0.033111.... 0.072829 sec/batch\n",
      "epoch: 45/50.... training_step: 106900.... batch_loss: 0.034710.... 0.073312 sec/batch\n",
      "epoch: 45/50.... training_step: 106950.... batch_loss: 0.034124.... 0.074116 sec/batch\n",
      "epoch: 45/50.... training_step: 107000.... batch_loss: 0.034008.... 0.074934 sec/batch\n",
      "epoch: 45/50.... training_step: 107050.... batch_loss: 0.030074.... 0.075083 sec/batch\n",
      "epoch: 45/50.... training_step: 107100.... batch_loss: 0.033075.... 0.073816 sec/batch\n",
      "epoch: 45/50.... training_step: 107150.... batch_loss: 0.035619.... 0.074780 sec/batch\n",
      "epoch: 45/50.... training_step: 107200.... batch_loss: 0.031490.... 0.072754 sec/batch\n",
      "epoch: 45/50.... training_step: 107250.... batch_loss: 0.029635.... 0.073820 sec/batch\n",
      "epoch: 45/50.... training_step: 107300.... batch_loss: 0.033321.... 0.074824 sec/batch\n",
      "epoch: 45/50.... training_step: 107350.... batch_loss: 0.033962.... 0.076802 sec/batch\n",
      "epoch: 45/50.... training_step: 107400.... batch_loss: 0.034685.... 0.073859 sec/batch\n",
      "epoch: 45/50.... training_step: 107450.... batch_loss: 0.030514.... 0.075019 sec/batch\n",
      "epoch: 45/50.... training_step: 107500.... batch_loss: 0.030793.... 0.074993 sec/batch\n",
      "epoch: 45/50.... training_step: 107550.... batch_loss: 0.033142.... 0.072650 sec/batch\n",
      "epoch: 45/50.... training_step: 107600.... batch_loss: 0.031865.... 0.072899 sec/batch\n",
      "epoch: 45/50.... training_step: 107650.... batch_loss: 0.033996.... 0.072131 sec/batch\n",
      "epoch: 45/50.... training_step: 107700.... batch_loss: 0.031575.... 0.077731 sec/batch\n",
      "epoch: 45/50.... training_step: 107750.... batch_loss: 0.033594.... 0.074399 sec/batch\n",
      "epoch: 45/50.... training_step: 107800.... batch_loss: 0.033144.... 0.074634 sec/batch\n",
      "epoch: 45/50.... training_step: 107850.... batch_loss: 0.034739.... 0.072523 sec/batch\n",
      "epoch: 45/50.... training_step: 107900.... batch_loss: 0.034573.... 0.072083 sec/batch\n",
      "epoch: 45/50.... training_step: 107950.... batch_loss: 0.036661.... 0.072781 sec/batch\n",
      "epoch: 45/50.... training_step: 108000.... batch_loss: 0.036196.... 0.073625 sec/batch\n",
      "epoch: 45/50.... training_step: 108050.... batch_loss: 0.034344.... 0.076777 sec/batch\n",
      "epoch: 45/50.... training_step: 108100.... batch_loss: 0.039124.... 0.073668 sec/batch\n",
      "epoch: 45/50.... training_step: 108150.... batch_loss: 0.034735.... 0.074273 sec/batch\n",
      "epoch: 45/50.... training_step: 108200.... batch_loss: 0.037397.... 0.074461 sec/batch\n",
      "epoch: 45/50.... training_step: 108250.... batch_loss: 0.030531.... 0.073975 sec/batch\n",
      "epoch: 45/50.... training_step: 108300.... batch_loss: 0.032930.... 0.076759 sec/batch\n",
      "epoch: 45/50.... training_step: 108350.... batch_loss: 0.033630.... 0.073251 sec/batch\n",
      "epoch: 46/50.... training_step: 108400.... batch_loss: 0.032108.... 0.072191 sec/batch\n",
      "epoch: 46/50.... training_step: 108450.... batch_loss: 0.029530.... 0.082290 sec/batch\n",
      "epoch: 46/50.... training_step: 108500.... batch_loss: 0.033207.... 0.073485 sec/batch\n",
      "epoch: 46/50.... training_step: 108550.... batch_loss: 0.032768.... 0.073690 sec/batch\n",
      "epoch: 46/50.... training_step: 108600.... batch_loss: 0.031347.... 0.074065 sec/batch\n",
      "epoch: 46/50.... training_step: 108650.... batch_loss: 0.033702.... 0.076864 sec/batch\n",
      "epoch: 46/50.... training_step: 108700.... batch_loss: 0.031618.... 0.073273 sec/batch\n",
      "epoch: 46/50.... training_step: 108750.... batch_loss: 0.033382.... 0.072750 sec/batch\n",
      "epoch: 46/50.... training_step: 108800.... batch_loss: 0.032371.... 0.073037 sec/batch\n",
      "epoch: 46/50.... training_step: 108850.... batch_loss: 0.036682.... 0.073614 sec/batch\n",
      "epoch: 46/50.... training_step: 108900.... batch_loss: 0.033234.... 0.075767 sec/batch\n",
      "epoch: 46/50.... training_step: 108950.... batch_loss: 0.031178.... 0.074733 sec/batch\n",
      "epoch: 46/50.... training_step: 109000.... batch_loss: 0.031852.... 0.073557 sec/batch\n",
      "epoch: 46/50.... training_step: 109050.... batch_loss: 0.034586.... 0.073922 sec/batch\n",
      "epoch: 46/50.... training_step: 109100.... batch_loss: 0.033681.... 0.075583 sec/batch\n",
      "epoch: 46/50.... training_step: 109150.... batch_loss: 0.032893.... 0.072451 sec/batch\n",
      "epoch: 46/50.... training_step: 109200.... batch_loss: 0.032216.... 0.073113 sec/batch\n",
      "epoch: 46/50.... training_step: 109250.... batch_loss: 0.032245.... 0.076577 sec/batch\n",
      "epoch: 46/50.... training_step: 109300.... batch_loss: 0.034500.... 0.075204 sec/batch\n",
      "epoch: 46/50.... training_step: 109350.... batch_loss: 0.034561.... 0.073901 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46/50.... training_step: 109400.... batch_loss: 0.033143.... 0.073863 sec/batch\n",
      "epoch: 46/50.... training_step: 109450.... batch_loss: 0.033097.... 0.076716 sec/batch\n",
      "epoch: 46/50.... training_step: 109500.... batch_loss: 0.034233.... 0.073910 sec/batch\n",
      "epoch: 46/50.... training_step: 109550.... batch_loss: 0.034678.... 0.073384 sec/batch\n",
      "epoch: 46/50.... training_step: 109600.... batch_loss: 0.030959.... 0.074505 sec/batch\n",
      "epoch: 46/50.... training_step: 109650.... batch_loss: 0.031263.... 0.074042 sec/batch\n",
      "epoch: 46/50.... training_step: 109700.... batch_loss: 0.031879.... 0.073388 sec/batch\n",
      "epoch: 46/50.... training_step: 109750.... batch_loss: 0.035813.... 0.072475 sec/batch\n",
      "epoch: 46/50.... training_step: 109800.... batch_loss: 0.031644.... 0.074573 sec/batch\n",
      "epoch: 46/50.... training_step: 109850.... batch_loss: 0.030565.... 0.075308 sec/batch\n",
      "epoch: 46/50.... training_step: 109900.... batch_loss: 0.032229.... 0.074650 sec/batch\n",
      "epoch: 46/50.... training_step: 109950.... batch_loss: 0.035963.... 0.073762 sec/batch\n",
      "epoch: 46/50.... training_step: 110000.... batch_loss: 0.032327.... 0.072798 sec/batch\n",
      "epoch: 46/50.... training_step: 110050.... batch_loss: 0.035647.... 0.074459 sec/batch\n",
      "epoch: 46/50.... training_step: 110100.... batch_loss: 0.030127.... 0.072892 sec/batch\n",
      "epoch: 46/50.... training_step: 110150.... batch_loss: 0.030438.... 0.073457 sec/batch\n",
      "epoch: 46/50.... training_step: 110200.... batch_loss: 0.031769.... 0.077509 sec/batch\n",
      "epoch: 46/50.... training_step: 110250.... batch_loss: 0.032532.... 0.075845 sec/batch\n",
      "epoch: 46/50.... training_step: 110300.... batch_loss: 0.030836.... 0.074399 sec/batch\n",
      "epoch: 46/50.... training_step: 110350.... batch_loss: 0.034018.... 0.077152 sec/batch\n",
      "epoch: 46/50.... training_step: 110400.... batch_loss: 0.032830.... 0.073021 sec/batch\n",
      "epoch: 46/50.... training_step: 110450.... batch_loss: 0.032129.... 0.072701 sec/batch\n",
      "epoch: 46/50.... training_step: 110500.... batch_loss: 0.030601.... 0.072686 sec/batch\n",
      "epoch: 46/50.... training_step: 110550.... batch_loss: 0.031737.... 0.072998 sec/batch\n",
      "epoch: 46/50.... training_step: 110600.... batch_loss: 0.035385.... 0.074180 sec/batch\n",
      "epoch: 46/50.... training_step: 110650.... batch_loss: 0.031919.... 0.076668 sec/batch\n",
      "epoch: 46/50.... training_step: 110700.... batch_loss: 0.036234.... 0.074439 sec/batch\n",
      "epoch: 47/50.... training_step: 110750.... batch_loss: 0.034094.... 0.073666 sec/batch\n",
      "epoch: 47/50.... training_step: 110800.... batch_loss: 0.033511.... 0.073253 sec/batch\n",
      "epoch: 47/50.... training_step: 110850.... batch_loss: 0.035348.... 0.075719 sec/batch\n",
      "epoch: 47/50.... training_step: 110900.... batch_loss: 0.030661.... 0.072880 sec/batch\n",
      "epoch: 47/50.... training_step: 110950.... batch_loss: 0.032809.... 0.075262 sec/batch\n",
      "epoch: 47/50.... training_step: 111000.... batch_loss: 0.034902.... 0.073437 sec/batch\n",
      "epoch: 47/50.... training_step: 111050.... batch_loss: 0.033343.... 0.073232 sec/batch\n",
      "epoch: 47/50.... training_step: 111100.... batch_loss: 0.033783.... 0.073415 sec/batch\n",
      "epoch: 47/50.... training_step: 111150.... batch_loss: 0.036441.... 0.074024 sec/batch\n",
      "epoch: 47/50.... training_step: 111200.... batch_loss: 0.032438.... 0.072669 sec/batch\n",
      "epoch: 47/50.... training_step: 111250.... batch_loss: 0.031843.... 0.074661 sec/batch\n",
      "epoch: 47/50.... training_step: 111300.... batch_loss: 0.030797.... 0.073675 sec/batch\n",
      "epoch: 47/50.... training_step: 111350.... batch_loss: 0.037242.... 0.072799 sec/batch\n",
      "epoch: 47/50.... training_step: 111400.... batch_loss: 0.033824.... 0.074323 sec/batch\n",
      "epoch: 47/50.... training_step: 111450.... batch_loss: 0.032324.... 0.073783 sec/batch\n",
      "epoch: 47/50.... training_step: 111500.... batch_loss: 0.029138.... 0.073808 sec/batch\n",
      "epoch: 47/50.... training_step: 111550.... batch_loss: 0.034212.... 0.073377 sec/batch\n",
      "epoch: 47/50.... training_step: 111600.... batch_loss: 0.034668.... 0.076484 sec/batch\n",
      "epoch: 47/50.... training_step: 111650.... batch_loss: 0.034527.... 0.075263 sec/batch\n",
      "epoch: 47/50.... training_step: 111700.... batch_loss: 0.031734.... 0.076035 sec/batch\n",
      "epoch: 47/50.... training_step: 111750.... batch_loss: 0.028853.... 0.073970 sec/batch\n",
      "epoch: 47/50.... training_step: 111800.... batch_loss: 0.033030.... 0.078493 sec/batch\n",
      "epoch: 47/50.... training_step: 111850.... batch_loss: 0.032830.... 0.075899 sec/batch\n",
      "epoch: 47/50.... training_step: 111900.... batch_loss: 0.034278.... 0.074470 sec/batch\n",
      "epoch: 47/50.... training_step: 111950.... batch_loss: 0.033484.... 0.075376 sec/batch\n",
      "epoch: 47/50.... training_step: 112000.... batch_loss: 0.033574.... 0.074435 sec/batch\n",
      "epoch: 47/50.... training_step: 112050.... batch_loss: 0.031130.... 0.076835 sec/batch\n",
      "epoch: 47/50.... training_step: 112100.... batch_loss: 0.033128.... 0.072883 sec/batch\n",
      "epoch: 47/50.... training_step: 112150.... batch_loss: 0.032132.... 0.075182 sec/batch\n",
      "epoch: 47/50.... training_step: 112200.... batch_loss: 0.030906.... 0.073755 sec/batch\n",
      "epoch: 47/50.... training_step: 112250.... batch_loss: 0.034577.... 0.074495 sec/batch\n",
      "epoch: 47/50.... training_step: 112300.... batch_loss: 0.031281.... 0.075168 sec/batch\n",
      "epoch: 47/50.... training_step: 112350.... batch_loss: 0.028145.... 0.075374 sec/batch\n",
      "epoch: 47/50.... training_step: 112400.... batch_loss: 0.034082.... 0.071898 sec/batch\n",
      "epoch: 47/50.... training_step: 112450.... batch_loss: 0.030572.... 0.072480 sec/batch\n",
      "epoch: 47/50.... training_step: 112500.... batch_loss: 0.033438.... 0.072606 sec/batch\n",
      "epoch: 47/50.... training_step: 112550.... batch_loss: 0.033488.... 0.075227 sec/batch\n",
      "epoch: 47/50.... training_step: 112600.... batch_loss: 0.035588.... 0.075225 sec/batch\n",
      "epoch: 47/50.... training_step: 112650.... batch_loss: 0.029576.... 0.074073 sec/batch\n",
      "epoch: 47/50.... training_step: 112700.... batch_loss: 0.030543.... 0.075207 sec/batch\n",
      "epoch: 47/50.... training_step: 112750.... batch_loss: 0.028726.... 0.076322 sec/batch\n",
      "epoch: 47/50.... training_step: 112800.... batch_loss: 0.035140.... 0.076313 sec/batch\n",
      "epoch: 47/50.... training_step: 112850.... batch_loss: 0.031030.... 0.076480 sec/batch\n",
      "epoch: 47/50.... training_step: 112900.... batch_loss: 0.032279.... 0.074708 sec/batch\n",
      "epoch: 47/50.... training_step: 112950.... batch_loss: 0.031534.... 0.073007 sec/batch\n",
      "epoch: 47/50.... training_step: 113000.... batch_loss: 0.032804.... 0.075198 sec/batch\n",
      "epoch: 47/50.... training_step: 113050.... batch_loss: 0.033099.... 0.075037 sec/batch\n",
      "epoch: 48/50.... training_step: 113100.... batch_loss: 0.031935.... 0.077681 sec/batch\n",
      "epoch: 48/50.... training_step: 113150.... batch_loss: 0.032176.... 0.073217 sec/batch\n",
      "epoch: 48/50.... training_step: 113200.... batch_loss: 0.034838.... 0.072781 sec/batch\n",
      "epoch: 48/50.... training_step: 113250.... batch_loss: 0.032937.... 0.076371 sec/batch\n",
      "epoch: 48/50.... training_step: 113300.... batch_loss: 0.032774.... 0.072626 sec/batch\n",
      "epoch: 48/50.... training_step: 113350.... batch_loss: 0.031093.... 0.076068 sec/batch\n",
      "epoch: 48/50.... training_step: 113400.... batch_loss: 0.030895.... 0.073411 sec/batch\n",
      "epoch: 48/50.... training_step: 113450.... batch_loss: 0.032611.... 0.077238 sec/batch\n",
      "epoch: 48/50.... training_step: 113500.... batch_loss: 0.032080.... 0.072350 sec/batch\n",
      "epoch: 48/50.... training_step: 113550.... batch_loss: 0.033403.... 0.074340 sec/batch\n",
      "epoch: 48/50.... training_step: 113600.... batch_loss: 0.034771.... 0.075218 sec/batch\n",
      "epoch: 48/50.... training_step: 113650.... batch_loss: 0.036062.... 0.072015 sec/batch\n",
      "epoch: 48/50.... training_step: 113700.... batch_loss: 0.035912.... 0.073261 sec/batch\n",
      "epoch: 48/50.... training_step: 113750.... batch_loss: 0.033672.... 0.075988 sec/batch\n",
      "epoch: 48/50.... training_step: 113800.... batch_loss: 0.032271.... 0.072984 sec/batch\n",
      "epoch: 48/50.... training_step: 113850.... batch_loss: 0.034480.... 0.077947 sec/batch\n",
      "epoch: 48/50.... training_step: 113900.... batch_loss: 0.033763.... 0.073940 sec/batch\n",
      "epoch: 48/50.... training_step: 113950.... batch_loss: 0.035504.... 0.072974 sec/batch\n",
      "epoch: 48/50.... training_step: 114000.... batch_loss: 0.030550.... 0.076453 sec/batch\n",
      "epoch: 48/50.... training_step: 114050.... batch_loss: 0.033168.... 0.078906 sec/batch\n",
      "epoch: 48/50.... training_step: 114100.... batch_loss: 0.029864.... 0.076365 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 48/50.... training_step: 114150.... batch_loss: 0.028990.... 0.074293 sec/batch\n",
      "epoch: 48/50.... training_step: 114200.... batch_loss: 0.034036.... 0.074651 sec/batch\n",
      "epoch: 48/50.... training_step: 114250.... batch_loss: 0.038342.... 0.076594 sec/batch\n",
      "epoch: 48/50.... training_step: 114300.... batch_loss: 0.031681.... 0.073259 sec/batch\n",
      "epoch: 48/50.... training_step: 114350.... batch_loss: 0.030604.... 0.076394 sec/batch\n",
      "epoch: 48/50.... training_step: 114400.... batch_loss: 0.031666.... 0.072976 sec/batch\n",
      "epoch: 48/50.... training_step: 114450.... batch_loss: 0.031449.... 0.074180 sec/batch\n",
      "epoch: 48/50.... training_step: 114500.... batch_loss: 0.036129.... 0.073719 sec/batch\n",
      "epoch: 48/50.... training_step: 114550.... batch_loss: 0.033495.... 0.077097 sec/batch\n",
      "epoch: 48/50.... training_step: 114600.... batch_loss: 0.035558.... 0.074656 sec/batch\n",
      "epoch: 48/50.... training_step: 114650.... batch_loss: 0.031580.... 0.074678 sec/batch\n",
      "epoch: 48/50.... training_step: 114700.... batch_loss: 0.031824.... 0.072841 sec/batch\n",
      "epoch: 48/50.... training_step: 114750.... batch_loss: 0.033341.... 0.073747 sec/batch\n",
      "epoch: 48/50.... training_step: 114800.... batch_loss: 0.033131.... 0.072582 sec/batch\n",
      "epoch: 48/50.... training_step: 114850.... batch_loss: 0.032352.... 0.072309 sec/batch\n",
      "epoch: 48/50.... training_step: 114900.... batch_loss: 0.035676.... 0.073896 sec/batch\n",
      "epoch: 48/50.... training_step: 114950.... batch_loss: 0.032311.... 0.074923 sec/batch\n",
      "epoch: 48/50.... training_step: 115000.... batch_loss: 0.030622.... 0.073188 sec/batch\n",
      "epoch: 48/50.... training_step: 115050.... batch_loss: 0.031293.... 0.073598 sec/batch\n",
      "epoch: 48/50.... training_step: 115100.... batch_loss: 0.029716.... 0.073384 sec/batch\n",
      "epoch: 48/50.... training_step: 115150.... batch_loss: 0.034273.... 0.076950 sec/batch\n",
      "epoch: 48/50.... training_step: 115200.... batch_loss: 0.035173.... 0.073291 sec/batch\n",
      "epoch: 48/50.... training_step: 115250.... batch_loss: 0.033907.... 0.076130 sec/batch\n",
      "epoch: 48/50.... training_step: 115300.... batch_loss: 0.031957.... 0.072470 sec/batch\n",
      "epoch: 48/50.... training_step: 115350.... batch_loss: 0.031240.... 0.073219 sec/batch\n",
      "epoch: 48/50.... training_step: 115400.... batch_loss: 0.031641.... 0.076956 sec/batch\n",
      "epoch: 49/50.... training_step: 115450.... batch_loss: 0.030866.... 0.074126 sec/batch\n",
      "epoch: 49/50.... training_step: 115500.... batch_loss: 0.030117.... 0.076319 sec/batch\n",
      "epoch: 49/50.... training_step: 115550.... batch_loss: 0.030447.... 0.072728 sec/batch\n",
      "epoch: 49/50.... training_step: 115600.... batch_loss: 0.033095.... 0.073560 sec/batch\n",
      "epoch: 49/50.... training_step: 115650.... batch_loss: 0.034223.... 0.072603 sec/batch\n",
      "epoch: 49/50.... training_step: 115700.... batch_loss: 0.033691.... 0.072470 sec/batch\n",
      "epoch: 49/50.... training_step: 115750.... batch_loss: 0.036344.... 0.072453 sec/batch\n",
      "epoch: 49/50.... training_step: 115800.... batch_loss: 0.031767.... 0.073795 sec/batch\n",
      "epoch: 49/50.... training_step: 115850.... batch_loss: 0.033438.... 0.073348 sec/batch\n",
      "epoch: 49/50.... training_step: 115900.... batch_loss: 0.031398.... 0.075599 sec/batch\n",
      "epoch: 49/50.... training_step: 115950.... batch_loss: 0.030841.... 0.074082 sec/batch\n",
      "epoch: 49/50.... training_step: 116000.... batch_loss: 0.031907.... 0.074280 sec/batch\n",
      "epoch: 49/50.... training_step: 116050.... batch_loss: 0.028535.... 0.076309 sec/batch\n",
      "epoch: 49/50.... training_step: 116100.... batch_loss: 0.032647.... 0.076333 sec/batch\n",
      "epoch: 49/50.... training_step: 116150.... batch_loss: 0.031159.... 0.074233 sec/batch\n",
      "epoch: 49/50.... training_step: 116200.... batch_loss: 0.029787.... 0.075834 sec/batch\n",
      "epoch: 49/50.... training_step: 116250.... batch_loss: 0.030668.... 0.074793 sec/batch\n",
      "epoch: 49/50.... training_step: 116300.... batch_loss: 0.034964.... 0.075967 sec/batch\n",
      "epoch: 49/50.... training_step: 116350.... batch_loss: 0.031438.... 0.074270 sec/batch\n",
      "epoch: 49/50.... training_step: 116400.... batch_loss: 0.030325.... 0.077041 sec/batch\n",
      "epoch: 49/50.... training_step: 116450.... batch_loss: 0.027908.... 0.074075 sec/batch\n",
      "epoch: 49/50.... training_step: 116500.... batch_loss: 0.033313.... 0.073466 sec/batch\n",
      "epoch: 49/50.... training_step: 116550.... batch_loss: 0.030930.... 0.074907 sec/batch\n",
      "epoch: 49/50.... training_step: 116600.... batch_loss: 0.035806.... 0.073013 sec/batch\n",
      "epoch: 49/50.... training_step: 116650.... batch_loss: 0.029591.... 0.072777 sec/batch\n",
      "epoch: 49/50.... training_step: 116700.... batch_loss: 0.032097.... 0.073732 sec/batch\n",
      "epoch: 49/50.... training_step: 116750.... batch_loss: 0.033045.... 0.072638 sec/batch\n",
      "epoch: 49/50.... training_step: 116800.... batch_loss: 0.031699.... 0.075921 sec/batch\n",
      "epoch: 49/50.... training_step: 116850.... batch_loss: 0.034120.... 0.074115 sec/batch\n",
      "epoch: 49/50.... training_step: 116900.... batch_loss: 0.031498.... 0.074670 sec/batch\n",
      "epoch: 49/50.... training_step: 116950.... batch_loss: 0.033168.... 0.075647 sec/batch\n",
      "epoch: 49/50.... training_step: 117000.... batch_loss: 0.034457.... 0.075320 sec/batch\n",
      "epoch: 49/50.... training_step: 117050.... batch_loss: 0.032578.... 0.073620 sec/batch\n",
      "epoch: 49/50.... training_step: 117100.... batch_loss: 0.029540.... 0.074823 sec/batch\n",
      "epoch: 49/50.... training_step: 117150.... batch_loss: 0.030882.... 0.075802 sec/batch\n",
      "epoch: 49/50.... training_step: 117200.... batch_loss: 0.034315.... 0.073564 sec/batch\n",
      "epoch: 49/50.... training_step: 117250.... batch_loss: 0.034650.... 0.075649 sec/batch\n",
      "epoch: 49/50.... training_step: 117300.... batch_loss: 0.033551.... 0.074046 sec/batch\n",
      "epoch: 49/50.... training_step: 117350.... batch_loss: 0.032598.... 0.073737 sec/batch\n",
      "epoch: 49/50.... training_step: 117400.... batch_loss: 0.033538.... 0.075786 sec/batch\n",
      "epoch: 49/50.... training_step: 117450.... batch_loss: 0.031300.... 0.075775 sec/batch\n",
      "epoch: 49/50.... training_step: 117500.... batch_loss: 0.032866.... 0.074274 sec/batch\n",
      "epoch: 49/50.... training_step: 117550.... batch_loss: 0.030939.... 0.073898 sec/batch\n",
      "epoch: 49/50.... training_step: 117600.... batch_loss: 0.033885.... 0.072539 sec/batch\n",
      "epoch: 49/50.... training_step: 117650.... batch_loss: 0.031953.... 0.074598 sec/batch\n",
      "epoch: 49/50.... training_step: 117700.... batch_loss: 0.034964.... 0.072908 sec/batch\n",
      "epoch: 49/50.... training_step: 117750.... batch_loss: 0.032724.... 0.073265 sec/batch\n",
      "epoch: 49/50.... training_step: 117800.... batch_loss: 0.031718.... 0.072733 sec/batch\n"
     ]
    }
   ],
   "source": [
    "#training code\n",
    "\n",
    "arvix = build_arvix(num_classes, lstmsize, batch_size, character_length,\n",
    "                keep_prob, num_layers, learning_rate, sampling = False, grad_clip = 5)\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "print_every_n = 50\n",
    "\n",
    "save_every_steps = 100\n",
    "\n",
    "    \n",
    "saver = tf.train.Saver(max_to_keep = 100)\n",
    "    \n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    all_trainable_vars = tf.reduce_sum([tf.reduce_prod(v.shape) for v in tf.trainable_variables()])\n",
    "        \n",
    "    print(sess.run(all_trainable_vars))\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        \n",
    "        \n",
    "        feed_state = sess.run([arvix.initial_state])\n",
    "            \n",
    "        for x, y in create_batches(encoded, batch_size, character_length):\n",
    "            \n",
    "            counter += 1\n",
    "        \n",
    "            start = time.time()\n",
    "        \n",
    "            feed = {arvix.inputs: x, arvix.targets : y, arvix.keep_prob: keep_prob, arvix.initial_state: feed_state}\n",
    "        \n",
    "            batch_loss, feed_state, _ = sess.run([arvix.loss, arvix.final_state, arvix.optimizer], feed_dict = feed)\n",
    "        \n",
    "        \n",
    "            if (counter % print_every_n == 0):\n",
    "            \n",
    "                end = time.time()\n",
    "            \n",
    "                print('epoch: {}/{}....' .format(epoch, epochs), \n",
    "                      'training_step: {}....'. format(counter), \n",
    "                      'batch_loss: {:4f}....' .format(batch_loss), \n",
    "                      '{:4f} sec/batch' .format(end-start))\n",
    "                \n",
    "            \n",
    "            #if (counter % print_every_n == 0):\n",
    "            \n",
    "                #saver.save(sess, \"checkpoints/i{}_l{}_h{}.ckpt\" .format (counter, lstmsize, 1))\n",
    "                \n",
    "            \n",
    "            \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}_h{}.ckpt\" .format (counter, lstmsize, 1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returning predictions \n",
    "\n",
    "int_to_vocab = dict(enumerate(character_set))\n",
    "\n",
    "def pick_top_n(preds, vocab_size, top_n=3):\n",
    "    \n",
    "    p = np.squeeze(preds)\n",
    "    \n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    \n",
    "    p = p / np.sum(p)\n",
    "    \n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling Code\n",
    "\n",
    "def sample(checkpoint, n_samples, lstmsize, vocab_size, prime = 'The'):\n",
    "    \n",
    "    samples = [c for c in prime]\n",
    "    \n",
    "    model_sample = build_arvix(len(character_set), lstmsize = lstmsize, sampling = True)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        saver.restore(sess, checkpoint)\n",
    "        \n",
    "        feed_state = sess.run(model_sample.initial_state)\n",
    "        \n",
    "        \n",
    "        for c in prime:\n",
    "            \n",
    "            x = np.zeros((1, 1))\n",
    "            \n",
    "            x[0, 0] = dict_int[c]\n",
    "            \n",
    "            \n",
    "            feed = {model_sample.inputs : x,\n",
    "                   model_sample.keep_prob : 1,\n",
    "                   model_sample.initial_state : feed_state}\n",
    "            \n",
    "            \n",
    "            preds, feed_state = sess.run([model_sample.predictions, model_sample.final_state], \n",
    "                                         feed_dict=feed)\n",
    "            \n",
    "            c = pick_top_n(preds, len(character_set))\n",
    "            \n",
    "            samples.append(int_to_vocab[c])\n",
    "            \n",
    "            \n",
    "        for i in range(n_samples):\n",
    "                \n",
    "            x[0, 0] =  c\n",
    "                \n",
    "            feed = {model_sample.inputs: x, model_sample.keep_prob: 1., model_sample.initial_state: feed_state}\n",
    "            \n",
    "            preds, feed_state = sess.run([model_sample.predictions, model_sample.final_state], feed_dict=feed)\n",
    "            \n",
    "            c = pick_top_n(preds, len(character_set))\n",
    "            \n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i117800_l780_h1.ckpt\n",
      "ConvolutionalIDvolutional Neural Networks (CNNs). This work seeks to explore a pats of a new formalization to depth an autoencoder with lateral shortcut connections from the encoder to decoder at each level of the hierarchy. The lateral shortcut connections allow the higher levels of the hierarchy to focus on abstrect invariant features. We then experiments leveraging recent methods for dealing with overfitting in neural networks as well as other tricks from the neural networkss with layers representing iterations.\n",
      "Many \n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 500, lstmsize, len(character_set), prime=\"Convolutional\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
